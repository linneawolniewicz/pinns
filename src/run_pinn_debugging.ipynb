{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7e6e69-8377-4874-aee5-22f99dbf95b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "# from pinn import PINN\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfm = tf.math\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8fd5e8-fb2d-4ec5-a104-5f606294015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"data\"))\n",
    "OUTPUTS_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"outputs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4ede82-5306-4443-be72-353fbda27c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(DATA_PATH + '/f_boundary.pkl', 'rb') as file:\n",
    "    f_boundary = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/p.pkl', 'rb') as file:\n",
    "    p = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/T.pkl', 'rb') as file:\n",
    "    T = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/r_120au.pkl', 'rb') as file:\n",
    "    r = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/P_predict.pkl', 'rb') as file:\n",
    "    P_predict = pkl.load(file)\n",
    "    \n",
    "# Get upper and lower bounds\n",
    "lb = np.log(np.array([p[0], r[0]], dtype='float32'))\n",
    "ub = np.log(np.array([p[-1], r[-1]], dtype='float32'))\n",
    "f_bound = np.array([-34.54346331847909, 6.466899920699378], dtype='float32')\n",
    "size = len(f_boundary[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f263d59-0b7b-4e47-9c3f-2dbd223187b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: (10, 1), p: (10, 1), T: (10, 1), f_boundary: (10, 1), P_predict: (100, 2)\n",
      "lb: [[-3.1390402]\n",
      " [17.909855 ]], ub:[[ 6.9086924]\n",
      " [23.613638 ]]\n"
     ]
    }
   ],
   "source": [
    "lb = np.array([[-3.1390402], [17.909855]], dtype='float32')\n",
    "\n",
    "# Check inputs\n",
    "print(f'r: {r.shape}, p: {p.shape}, T: {T.shape}, f_boundary: {f_boundary.shape}, P_predict: {P_predict.shape}')\n",
    "print(f'lb: {lb}, ub:{ub}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7dab597-dc24-4b1c-a440-00e78daca9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, f_boundary, f_bound, size, n_samples=20000):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.size = size\n",
    "        self.f_bound = f_bound\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and f_boundary.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                t1.watch(p)\n",
    "                t1.watch(r)\n",
    "                t1.watch(p_boundary)\n",
    "                t1.watch(r_boundary)\n",
    "                \n",
    "                # print(f'------------------------------------')\n",
    "                # print(f'p {p} r {r} p_boundary {p_boundary} r_boundary {r_boundary}')\n",
    "                \n",
    "                # PINN loss\n",
    "                p_scaled = (tfm.log(p) - self.lower_bound[0])/tfm.abs(self.upper_bound[0] - self.lower_bound[0])\n",
    "                r_scaled = (tfm.log(r) - self.lower_bound[1])/tfm.abs(self.upper_bound[1] - self.lower_bound[1])\n",
    "                \n",
    "                P = tf.concat((p_scaled, r_scaled), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "                \n",
    "                # print(f'p_scaled {p_scaled} r_scaled {r_scaled} f {f}')\n",
    "\n",
    "                # Boundary loss\n",
    "                p_boundary_scaled = (tfm.log(p_boundary) - self.lower_bound[0])/tfm.abs(self.upper_bound[0] - self.lower_bound[0])\n",
    "                r_boundary_scaled = (tfm.log(r_boundary) - self.lower_bound[1])/tfm.abs(self.upper_bound[1] - self.lower_bound[1])\n",
    "                \n",
    "                P_boundary = tf.concat((p_boundary_scaled, r_boundary_scaled), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "                \n",
    "                # print(f'p_boundary_scaled {p_boundary_scaled} r_boundary_scaled {r_boundary_scaled} f_pred_boundary {f_pred_boundary}')\n",
    "                \n",
    "                boundary_loss = tfm.reduce_mean(tfm.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            g_p = t1.gradient(f, p)\n",
    "            g_r = t1.gradient(f, r)\n",
    "            \n",
    "            g_p_boundary = t1.gradient(f_pred_boundary, p_boundary)\n",
    "            g_r_boundary = t1.gradient(f_pred_boundary, r_boundary)\n",
    "            \n",
    "            # Calculate f_p and f_r using chain rule\n",
    "            diff = tfm.abs(self.f_bound[1] - self.f_bound[0])\n",
    "            f_g = diff*tfm.exp(diff*f + self.f_bound[0])\n",
    "            f_g_boundary = diff*tfm.exp(diff*f_pred_boundary + self.f_bound[0])\n",
    "            \n",
    "            f_p = f_g*g_p\n",
    "            f_r = f_g*g_r\n",
    "            \n",
    "            f_p_boundary = f_g_boundary*g_p_boundary\n",
    "            f_r_boundary = f_g_boundary*g_r_boundary\n",
    "            \n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r) + self.pinn_loss(p_boundary, r_boundary, f_p_boundary, f_r_boundary)\n",
    "            total_loss = (1-alpha)*pinn_loss + alpha*boundary_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        client, trial: Sherpa client and trial\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_decay: If -1, learning rate will not be decayed. Otherwise, lr = lr_decay*lr if loss hasn't \n",
    "        decreased\n",
    "        \n",
    "        alpha_decay: If -1, alpha will not be changed. Otherwise, alpha = alpha_decay*alpha if loss \n",
    "        hasn't decreased\n",
    "        \n",
    "        alpha_limit = Minimum alpha value to decay to\n",
    "        \n",
    "        patience: Number of epochs to check whether loss has decreased before updating lr or alpha\n",
    "        \n",
    "        filename: Name for the checkpoint file\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, client=None, trial=None, alpha=0.5, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, size=256, \n",
    "            save=False, load_epoch=-1, lr_decay=-1, alpha_decay=-1, alpha_limit = 0.5, patience=3, filename=''):\n",
    "        \n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(load_epoch)\n",
    "            self.load_weights(name)\n",
    "        \n",
    "        # Initialize\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs,))\n",
    "        total_boundary_loss = np.zeros((epochs,))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, sample data and pass to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "#                 # Sample p and r according to a uniform distribution between upper and lower bounds\n",
    "#                 dist = tfd.Uniform(0, 1)\n",
    "\n",
    "#                 p = (dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[0] - self.lower_bound[0])) + self.lower_bound[0]\n",
    "#                 r = (dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[1] - self.lower_bound[1])) + self.lower_bound[1]\n",
    "                \n",
    "#                 p = tfm.exp(p)\n",
    "#                 r = tfm.exp(r)\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = tf.Variable(self.p[p_idx], dtype=tf.float32)\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_boundary = np.zeros((boundary_batchsize, 1))\n",
    "                upper_boundary[:] = tfm.exp(self.upper_bound[1])\n",
    "                r_boundary = tf.Variable(upper_boundary, dtype=tf.float32)\n",
    "                \n",
    "                # Testing sampling p and r same as boundary p and r\n",
    "                p = tf.Variable(self.p[p_idx], dtype=tf.float32)\n",
    "                r = tf.Variable(upper_boundary, dtype=tf.float32)\n",
    "                ########################################\n",
    "                \n",
    "                # Train and get loss\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Sum losses\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Epoch {epoch}. Current alpha: {alpha:.6f}, lr: {lr:.10f}. Training losses: pinn: {total_pinn_loss[epoch]}, ' +\n",
    "                  f'boundary: {total_boundary_loss[epoch]:.6f}, weighted total: {((alpha*total_boundary_loss[epoch])+((1-alpha)*total_pinn_loss[epoch])):.10f}')\n",
    "            \n",
    "            predictions[:, :, epoch] = self.predict(P_predict, batchsize)\n",
    "            \n",
    "            # Decay lr if loss hasn't decreased since current epoch - patience\n",
    "            if (epoch > patience):\n",
    "                hasntDecreased = False\n",
    "                if (total_pinn_loss[epoch] + total_boundary_loss[epoch]) > (total_pinn_loss[epoch-patience] + total_boundary_loss[epoch-patience]):\n",
    "                    hasntDecreased = True\n",
    "                        \n",
    "                if (lr_decay != -1) & hasntDecreased:\n",
    "                    lr = lr_decay*lr\n",
    "\n",
    "            # Decrease alpha each epoch\n",
    "            if (alpha_decay != -1) & (alpha >= alpha_limit):\n",
    "                alpha = alpha_decay*alpha\n",
    "\n",
    "            # If the epoch is a multiple of 100, save to a checkpoint\n",
    "            if (epoch%100 == 0) & (save == True):\n",
    "                name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(epoch)\n",
    "                self.save_weights(name, overwrite=True, save_format=None, options=None)\n",
    "                \n",
    "            # Send metrics\n",
    "            if client:\n",
    "                if (np.isnan(total_pinn_loss[epoch]) and np.isnan(total_boundary_loss[epoch])):\n",
    "                    obj = np.inf\n",
    "                else:\n",
    "                    obj = total_pinn_loss[epoch] + total_boundary_loss[epoch]\n",
    "                client.send_metrics(\n",
    "                         trial=trial,\n",
    "                         iteration=epoch,\n",
    "                         objective=obj)\n",
    "        \n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, batchsize):\n",
    "        P_size = P.shape[0]\n",
    "        steps_per_epoch = np.ceil(P_size / batchsize).astype(int)\n",
    "        predictions = np.zeros((P_size, 1))\n",
    "        \n",
    "        # For each step predict on data between start and end indices\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # Calculate end_idx\n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P_size - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Predict\n",
    "            predictions[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r):\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tfm.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tfm.divide(p, tfm.sqrt(tfm.square(p) + tfm.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tfm.reduce_mean(tfm.abs(f_r + (tfm.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "442d0c7b-4143-45df-95fc-a534727e3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 500\n",
    "alpha = 1\n",
    "alpha_decay = 0.9930128211390397\n",
    "alpha_limit = 0\n",
    "lr_decay = 0.95\n",
    "patience = 10\n",
    "batchsize = 10 #1032\n",
    "boundary_batchsize = 10 #512\n",
    "activation = 'selu'\n",
    "save = False\n",
    "load_epoch = -1\n",
    "filename = 'test120au'\n",
    "n_samples = 10 #20000\n",
    "lr = 1.143693722231739e-05\n",
    "num_layers = 3\n",
    "num_hidden_units = 97\n",
    "\n",
    "# Create model\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(inputs)\n",
    "for _ in range(num_layers-1):\n",
    "    x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64a9cf11-b7a4-4914-866b-6fa66a51ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Current alpha: 1.000000, lr: 0.0000114369. Training losses: pinn: 1.056560631695902e-05, boundary: 0.017588, weighted total: 0.0175878610\n",
      "WARNING:tensorflow:5 out of the last 6001 calls to <function PINN.tf_call at 0x2adaa1d71900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1. Current alpha: 0.993013, lr: 0.0000114369. Training losses: pinn: 1.441373024135828e-05, boundary: 0.018604, weighted total: 0.0184736332\n",
      "Epoch 2. Current alpha: 0.986074, lr: 0.0000114369. Training losses: pinn: 1.0792406101245433e-05, boundary: 0.017465, weighted total: 0.0172220648\n",
      "Epoch 3. Current alpha: 0.979185, lr: 0.0000114369. Training losses: pinn: 1.4715564248035662e-05, boundary: 0.018471, weighted total: 0.0180871209\n",
      "Epoch 4. Current alpha: 0.972343, lr: 0.0000114369. Training losses: pinn: 1.1018619261449203e-05, boundary: 0.017385, weighted total: 0.0169043315\n",
      "Epoch 5. Current alpha: 0.965549, lr: 0.0000114369. Training losses: pinn: 1.1127540346933529e-05, boundary: 0.017171, weighted total: 0.0165800028\n",
      "Epoch 6. Current alpha: 0.958802, lr: 0.0000114369. Training losses: pinn: 8.327531759277917e-06, boundary: 0.018340, weighted total: 0.0175843880\n",
      "Epoch 7. Current alpha: 0.952103, lr: 0.0000114369. Training losses: pinn: 1.1363112207618542e-05, boundary: 0.017049, weighted total: 0.0162330867\n",
      "Epoch 8. Current alpha: 0.945451, lr: 0.0000114369. Training losses: pinn: 8.503830031258985e-06, boundary: 0.018226, weighted total: 0.0172323713\n",
      "Epoch 9. Current alpha: 0.938845, lr: 0.0000114369. Training losses: pinn: 1.1599983736232389e-05, boundary: 0.016940, weighted total: 0.0159049075\n",
      "Epoch 10. Current alpha: 0.932285, lr: 0.0000114369. Training losses: pinn: 1.166825859399978e-05, boundary: 0.017274, weighted total: 0.0161051002\n",
      "Epoch 11. Current alpha: 0.925771, lr: 0.0000114369. Training losses: pinn: 8.732416972634383e-06, boundary: 0.017555, weighted total: 0.0162521541\n",
      "Epoch 12. Current alpha: 0.919302, lr: 0.0000114369. Training losses: pinn: 1.1914506103494205e-05, boundary: 0.017148, weighted total: 0.0157648148\n",
      "Epoch 13. Current alpha: 0.912879, lr: 0.0000114369. Training losses: pinn: 8.916666956793051e-06, boundary: 0.017440, weighted total: 0.0159216337\n",
      "Epoch 14. Current alpha: 0.906500, lr: 0.0000114369. Training losses: pinn: 1.2163474821136333e-05, boundary: 0.017026, weighted total: 0.0154354891\n",
      "Epoch 15. Current alpha: 0.900166, lr: 0.0000114369. Training losses: pinn: 9.102930562221445e-06, boundary: 0.017326, weighted total: 0.0155973367\n",
      "Epoch 16. Current alpha: 0.893877, lr: 0.0000108651. Training losses: pinn: 1.242249709321186e-05, boundary: 0.016905, weighted total: 0.0151121055\n",
      "Epoch 17. Current alpha: 0.887631, lr: 0.0000108651. Training losses: pinn: 9.430384125153068e-06, boundary: 0.017132, weighted total: 0.0152080175\n",
      "Epoch 18. Current alpha: 0.881429, lr: 0.0000103218. Training losses: pinn: 1.2667483133554924e-05, boundary: 0.016789, weighted total: 0.0148002300\n",
      "Epoch 19. Current alpha: 0.875270, lr: 0.0000103218. Training losses: pinn: 9.751242942002136e-06, boundary: 0.016949, weighted total: 0.0148360362\n",
      "Epoch 20. Current alpha: 0.869155, lr: 0.0000098057. Training losses: pinn: 1.2910973055113573e-05, boundary: 0.016680, weighted total: 0.0144987832\n",
      "Epoch 21. Current alpha: 0.863082, lr: 0.0000098057. Training losses: pinn: 1.0069024938275106e-05, boundary: 0.016775, weighted total: 0.0144793777\n",
      "Epoch 22. Current alpha: 0.857051, lr: 0.0000098057. Training losses: pinn: 1.3142588613845874e-05, boundary: 0.016575, weighted total: 0.0142078023\n",
      "Epoch 23. Current alpha: 0.851063, lr: 0.0000098057. Training losses: pinn: 1.0249620572722051e-05, boundary: 0.016676, weighted total: 0.0141941815\n",
      "Epoch 24. Current alpha: 0.845116, lr: 0.0000098057. Training losses: pinn: 1.3382576980802696e-05, boundary: 0.016471, weighted total: 0.0139220880\n",
      "Epoch 25. Current alpha: 0.839211, lr: 0.0000098057. Training losses: pinn: 1.0435436706757173e-05, boundary: 0.016577, weighted total: 0.0139133952\n",
      "Epoch 26. Current alpha: 0.833348, lr: 0.0000098057. Training losses: pinn: 1.3621244761452544e-05, boundary: 0.016367, weighted total: 0.0136414188\n",
      "Epoch 27. Current alpha: 0.827525, lr: 0.0000098057. Training losses: pinn: 1.062300452758791e-05, boundary: 0.016479, weighted total: 0.0136382898\n",
      "Epoch 28. Current alpha: 0.821743, lr: 0.0000098057. Training losses: pinn: 1.387077099934686e-05, boundary: 0.016262, weighted total: 0.0133654617\n",
      "Epoch 29. Current alpha: 0.816001, lr: 0.0000098057. Training losses: pinn: 1.0817568181664683e-05, boundary: 0.016486, weighted total: 0.0134548181\n",
      "Epoch 30. Current alpha: 0.810300, lr: 0.0000098057. Training losses: pinn: 1.3886720807931852e-05, boundary: 0.016228, weighted total: 0.0131521247\n",
      "Epoch 31. Current alpha: 0.804638, lr: 0.0000098057. Training losses: pinn: 1.082889320969116e-05, boundary: 0.016423, weighted total: 0.0132167767\n",
      "Epoch 32. Current alpha: 0.799016, lr: 0.0000098057. Training losses: pinn: 1.3901733836974017e-05, boundary: 0.016194, weighted total: 0.0129422239\n",
      "Epoch 33. Current alpha: 0.793433, lr: 0.0000098057. Training losses: pinn: 1.0842160008905921e-05, boundary: 0.016361, weighted total: 0.0129836630\n",
      "Epoch 34. Current alpha: 0.787889, lr: 0.0000098057. Training losses: pinn: 1.3922694961365778e-05, boundary: 0.016163, weighted total: 0.0127375454\n",
      "Epoch 35. Current alpha: 0.782384, lr: 0.0000098057. Training losses: pinn: 1.0857657798624132e-05, boundary: 0.016300, weighted total: 0.0127553532\n",
      "Epoch 36. Current alpha: 0.776917, lr: 0.0000098057. Training losses: pinn: 1.3942662917543203e-05, boundary: 0.016132, weighted total: 0.0125360839\n",
      "Epoch 37. Current alpha: 0.771489, lr: 0.0000098057. Training losses: pinn: 1.087326199922245e-05, boundary: 0.016239, weighted total: 0.0125308639\n",
      "Epoch 38. Current alpha: 0.766098, lr: 0.0000098057. Training losses: pinn: 1.3960753676656168e-05, boundary: 0.016098, weighted total: 0.0123356569\n",
      "Epoch 39. Current alpha: 0.760745, lr: 0.0000098057. Training losses: pinn: 1.0887254575209226e-05, boundary: 0.016178, weighted total: 0.0123101483\n",
      "Epoch 40. Current alpha: 0.755430, lr: 0.0000098057. Training losses: pinn: 1.398172025801614e-05, boundary: 0.016066, weighted total: 0.0121401538\n",
      "Epoch 41. Current alpha: 0.750152, lr: 0.0000098057. Training losses: pinn: 1.0903722795774229e-05, boundary: 0.016117, weighted total: 0.0120928876\n",
      "Epoch 42. Current alpha: 0.744910, lr: 0.0000098057. Training losses: pinn: 1.4002331226947717e-05, boundary: 0.016034, weighted total: 0.0119475762\n",
      "Epoch 43. Current alpha: 0.739705, lr: 0.0000098057. Training losses: pinn: 1.0921058674284723e-05, boundary: 0.016051, weighted total: 0.0118756312\n",
      "Epoch 44. Current alpha: 0.734537, lr: 0.0000098057. Training losses: pinn: 1.4028142686584033e-05, boundary: 0.016003, weighted total: 0.0117583354\n",
      "Epoch 45. Current alpha: 0.729405, lr: 0.0000098057. Training losses: pinn: 1.0941190339508466e-05, boundary: 0.015988, weighted total: 0.0116647523\n",
      "Epoch 46. Current alpha: 0.724308, lr: 0.0000098057. Training losses: pinn: 1.3376916285778861e-05, boundary: 0.015868, weighted total: 0.0114973015\n",
      "Epoch 47. Current alpha: 0.719247, lr: 0.0000098057. Training losses: pinn: 1.0433233001094777e-05, boundary: 0.016050, weighted total: 0.0115465302\n",
      "Epoch 48. Current alpha: 0.714222, lr: 0.0000098057. Training losses: pinn: 1.3397939255810343e-05, boundary: 0.015835, weighted total: 0.0113136717\n",
      "Epoch 49. Current alpha: 0.709231, lr: 0.0000098057. Training losses: pinn: 1.0449626643094234e-05, boundary: 0.015983, weighted total: 0.0113386401\n",
      "Epoch 50. Current alpha: 0.704276, lr: 0.0000098057. Training losses: pinn: 1.3418029993772507e-05, boundary: 0.015801, weighted total: 0.0111323051\n",
      "Epoch 51. Current alpha: 0.699355, lr: 0.0000098057. Training losses: pinn: 1.046513716573827e-05, boundary: 0.015916, weighted total: 0.0111340438\n",
      "Epoch 52. Current alpha: 0.694468, lr: 0.0000098057. Training losses: pinn: 1.343774601991754e-05, boundary: 0.015767, weighted total: 0.0109538079\n",
      "Epoch 53. Current alpha: 0.689616, lr: 0.0000098057. Training losses: pinn: 1.0480785022082273e-05, boundary: 0.015849, weighted total: 0.0109331833\n",
      "Epoch 54. Current alpha: 0.684798, lr: 0.0000098057. Training losses: pinn: 1.3455062799039297e-05, boundary: 0.015734, weighted total: 0.0107786460\n",
      "Epoch 55. Current alpha: 0.680013, lr: 0.0000098057. Training losses: pinn: 1.0494424714124762e-05, boundary: 0.015783, weighted total: 0.0107358533\n",
      "Epoch 56. Current alpha: 0.675261, lr: 0.0000098057. Training losses: pinn: 1.347329634882044e-05, boundary: 0.015701, weighted total: 0.0106063814\n",
      "Epoch 57. Current alpha: 0.670543, lr: 0.0000098057. Training losses: pinn: 1.0508209925319534e-05, boundary: 0.015716, weighted total: 0.0105419777\n",
      "Epoch 58. Current alpha: 0.665858, lr: 0.0000098057. Training losses: pinn: 1.3491242498275824e-05, boundary: 0.015667, weighted total: 0.0104367307\n",
      "Epoch 59. Current alpha: 0.661206, lr: 0.0000098057. Training losses: pinn: 1.0522444426896982e-05, boundary: 0.015650, weighted total: 0.0103515172\n",
      "Epoch 60. Current alpha: 0.656586, lr: 0.0000098057. Training losses: pinn: 1.3509062227967661e-05, boundary: 0.015634, weighted total: 0.0102698062\n",
      "Epoch 61. Current alpha: 0.651998, lr: 0.0000098057. Training losses: pinn: 1.053641699400032e-05, boundary: 0.015584, weighted total: 0.0101644261\n",
      "Epoch 62. Current alpha: 0.647442, lr: 0.0000098057. Training losses: pinn: 1.3536242477130145e-05, boundary: 0.015604, weighted total: 0.0101075454\n",
      "Epoch 63. Current alpha: 0.642918, lr: 0.0000098057. Training losses: pinn: 1.0557343557593413e-05, boundary: 0.015518, weighted total: 0.0099804607\n",
      "Epoch 64. Current alpha: 0.638426, lr: 0.0000098057. Training losses: pinn: 1.3554121323977597e-05, boundary: 0.015571, weighted total: 0.0099456532\n",
      "Epoch 65. Current alpha: 0.633966, lr: 0.0000098057. Training losses: pinn: 1.057153167494107e-05, boundary: 0.015452, weighted total: 0.0097996066\n",
      "Epoch 66. Current alpha: 0.629536, lr: 0.0000098057. Training losses: pinn: 1.3572450370702427e-05, boundary: 0.015538, weighted total: 0.0097865113\n",
      "Epoch 67. Current alpha: 0.625137, lr: 0.0000098057. Training losses: pinn: 1.0585704330878798e-05, boundary: 0.015385, weighted total: 0.0096217948\n",
      "Epoch 68. Current alpha: 0.620769, lr: 0.0000098057. Training losses: pinn: 1.359055022476241e-05, boundary: 0.015504, weighted total: 0.0096298193\n",
      "Epoch 69. Current alpha: 0.616432, lr: 0.0000098057. Training losses: pinn: 1.059994065144565e-05, boundary: 0.015319, weighted total: 0.0094472464\n",
      "Epoch 70. Current alpha: 0.612125, lr: 0.0000098057. Training losses: pinn: 1.3618428965855855e-05, boundary: 0.015474, weighted total: 0.0094775748\n",
      "Epoch 71. Current alpha: 0.607848, lr: 0.0000098057. Training losses: pinn: 1.0621490218909457e-05, boundary: 0.015253, weighted total: 0.0092756071\n",
      "Epoch 72. Current alpha: 0.603601, lr: 0.0000098057. Training losses: pinn: 1.3635728464578278e-05, boundary: 0.015440, weighted total: 0.0093252293\n",
      "Epoch 73. Current alpha: 0.599383, lr: 0.0000098057. Training losses: pinn: 1.0635061698849313e-05, boundary: 0.015187, weighted total: 0.0091068843\n",
      "Epoch 74. Current alpha: 0.595195, lr: 0.0000098057. Training losses: pinn: 1.365417665510904e-05, boundary: 0.015407, weighted total: 0.0091758862\n",
      "Epoch 75. Current alpha: 0.591036, lr: 0.0000098057. Training losses: pinn: 1.0649498108250555e-05, boundary: 0.015129, weighted total: 0.0089460153\n",
      "Epoch 76. Current alpha: 0.586907, lr: 0.0000098057. Training losses: pinn: 1.3021615814068355e-05, boundary: 0.015277, weighted total: 0.0089713196\n",
      "Epoch 77. Current alpha: 0.582806, lr: 0.0000098057. Training losses: pinn: 1.0156280040973797e-05, boundary: 0.015187, weighted total: 0.0088551559\n",
      "Epoch 78. Current alpha: 0.578734, lr: 0.0000098057. Training losses: pinn: 1.3038200449955184e-05, boundary: 0.015243, weighted total: 0.0088270111\n",
      "Epoch 79. Current alpha: 0.574690, lr: 0.0000098057. Training losses: pinn: 1.0169182132813148e-05, boundary: 0.015120, weighted total: 0.0086939197\n",
      "Epoch 80. Current alpha: 0.570675, lr: 0.0000098057. Training losses: pinn: 1.3055743693257682e-05, boundary: 0.015220, weighted total: 0.0086912405\n",
      "Epoch 81. Current alpha: 0.566687, lr: 0.0000098057. Training losses: pinn: 1.0096250662172679e-05, boundary: 0.015102, weighted total: 0.0085623493\n",
      "Epoch 82. Current alpha: 0.562728, lr: 0.0000098057. Training losses: pinn: 1.2962051187059842e-05, boundary: 0.015200, weighted total: 0.0085589246\n",
      "Epoch 83. Current alpha: 0.558796, lr: 0.0000098057. Training losses: pinn: 1.0023961294791661e-05, boundary: 0.015083, weighted total: 0.0084326931\n",
      "Epoch 84. Current alpha: 0.554891, lr: 0.0000098057. Training losses: pinn: 1.2869216334365774e-05, boundary: 0.015179, weighted total: 0.0084285938\n",
      "Epoch 85. Current alpha: 0.551014, lr: 0.0000098057. Training losses: pinn: 9.951452739187516e-06, boundary: 0.015064, weighted total: 0.0083046733\n",
      "Epoch 86. Current alpha: 0.547164, lr: 0.0000098057. Training losses: pinn: 1.2775929462804925e-05, boundary: 0.015159, weighted total: 0.0083000386\n",
      "Epoch 87. Current alpha: 0.543341, lr: 0.0000098057. Training losses: pinn: 9.879465324047487e-06, boundary: 0.015053, weighted total: 0.0081835778\n",
      "Epoch 88. Current alpha: 0.539545, lr: 0.0000098057. Training losses: pinn: 1.2901537047582678e-05, boundary: 0.015056, weighted total: 0.0081291873\n",
      "Epoch 89. Current alpha: 0.535775, lr: 0.0000098057. Training losses: pinn: 9.976394721888937e-06, boundary: 0.015099, weighted total: 0.0080941329\n",
      "Epoch 90. Current alpha: 0.532031, lr: 0.0000098057. Training losses: pinn: 1.2808292922272813e-05, boundary: 0.015035, weighted total: 0.0080051478\n",
      "Epoch 91. Current alpha: 0.528314, lr: 0.0000098057. Training losses: pinn: 9.904190847009886e-06, boundary: 0.015079, weighted total: 0.0079713002\n",
      "Epoch 92. Current alpha: 0.524622, lr: 0.0000098057. Training losses: pinn: 1.2715628145087976e-05, boundary: 0.015014, weighted total: 0.0078829428\n",
      "Epoch 93. Current alpha: 0.520957, lr: 0.0000098057. Training losses: pinn: 9.832499927142635e-06, boundary: 0.015060, weighted total: 0.0078501866\n",
      "Epoch 94. Current alpha: 0.517317, lr: 0.0000098057. Training losses: pinn: 1.2623527254618239e-05, boundary: 0.014994, weighted total: 0.0077625747\n",
      "Epoch 95. Current alpha: 0.513702, lr: 0.0000098057. Training losses: pinn: 9.761550245457329e-06, boundary: 0.015041, weighted total: 0.0077311360\n",
      "Epoch 96. Current alpha: 0.510113, lr: 0.0000098057. Training losses: pinn: 1.254038761544507e-05, boundary: 0.014975, weighted total: 0.0076452332\n",
      "Epoch 97. Current alpha: 0.506548, lr: 0.0000098057. Training losses: pinn: 9.697307177702896e-06, boundary: 0.015021, weighted total: 0.0076138339\n",
      "Epoch 98. Current alpha: 0.503009, lr: 0.0000098057. Training losses: pinn: 1.2449731002561748e-05, boundary: 0.014955, weighted total: 0.0075284815\n",
      "Epoch 99. Current alpha: 0.499495, lr: 0.0000098057. Training losses: pinn: 9.62703961704392e-06, boundary: 0.015002, weighted total: 0.0074981944\n",
      "Epoch 100. Current alpha: 0.496004, lr: 0.0000098057. Training losses: pinn: 1.2359463653410785e-05, boundary: 0.014934, weighted total: 0.0074135638\n",
      "Epoch 101. Current alpha: 0.492539, lr: 0.0000098057. Training losses: pinn: 9.55637551669497e-06, boundary: 0.014983, weighted total: 0.0073843657\n",
      "Epoch 102. Current alpha: 0.489097, lr: 0.0000098057. Training losses: pinn: 1.2276943380129524e-05, boundary: 0.014915, weighted total: 0.0073013958\n",
      "Epoch 103. Current alpha: 0.485680, lr: 0.0000098057. Training losses: pinn: 9.492616300121881e-06, boundary: 0.014963, weighted total: 0.0072722553\n",
      "Epoch 104. Current alpha: 0.482286, lr: 0.0000098057. Training losses: pinn: 1.2186618732812349e-05, boundary: 0.014895, weighted total: 0.0071898557\n",
      "Epoch 105. Current alpha: 0.478917, lr: 0.0000098057. Training losses: pinn: 9.422894436283968e-06, boundary: 0.014944, weighted total: 0.0071617021\n",
      "Epoch 106. Current alpha: 0.475570, lr: 0.0000098057. Training losses: pinn: 1.2097031685698312e-05, boundary: 0.014874, weighted total: 0.0070800515\n",
      "Epoch 107. Current alpha: 0.472247, lr: 0.0000098057. Training losses: pinn: 9.354271242045797e-06, boundary: 0.014924, weighted total: 0.0070527535\n",
      "Epoch 108. Current alpha: 0.468948, lr: 0.0000098057. Training losses: pinn: 1.201687064167345e-05, boundary: 0.014856, weighted total: 0.0069729089\n",
      "Epoch 109. Current alpha: 0.465671, lr: 0.0000098057. Training losses: pinn: 9.292813047068194e-06, boundary: 0.014904, weighted total: 0.0069452698\n",
      "Epoch 110. Current alpha: 0.462417, lr: 0.0000098057. Training losses: pinn: 1.1929905667784624e-05, boundary: 0.014835, weighted total: 0.0068662295\n",
      "Epoch 111. Current alpha: 0.459186, lr: 0.0000098057. Training losses: pinn: 9.224962013831828e-06, boundary: 0.014893, weighted total: 0.0068436317\n",
      "Epoch 112. Current alpha: 0.455978, lr: 0.0000098057. Training losses: pinn: 1.2047166819684207e-05, boundary: 0.014732, weighted total: 0.0067238300\n",
      "Epoch 113. Current alpha: 0.452792, lr: 0.0000098057. Training losses: pinn: 9.315613169746939e-06, boundary: 0.014938, weighted total: 0.0067690198\n",
      "Epoch 114. Current alpha: 0.449628, lr: 0.0000098057. Training losses: pinn: 1.1959145922446623e-05, boundary: 0.014711, weighted total: 0.0066209556\n",
      "Epoch 115. Current alpha: 0.446487, lr: 0.0000098057. Training losses: pinn: 9.247521120414604e-06, boundary: 0.014918, weighted total: 0.0066659672\n",
      "Epoch 116. Current alpha: 0.443367, lr: 0.0000098057. Training losses: pinn: 1.1872025424963795e-05, boundary: 0.014690, weighted total: 0.0065196446\n",
      "Epoch 117. Current alpha: 0.440269, lr: 0.0000098057. Training losses: pinn: 9.18088699108921e-06, boundary: 0.014899, weighted total: 0.0065647360\n",
      "Epoch 118. Current alpha: 0.437193, lr: 0.0000098057. Training losses: pinn: 1.1794183592428453e-05, boundary: 0.014671, weighted total: 0.0064208528\n",
      "Epoch 119. Current alpha: 0.434138, lr: 0.0000098057. Training losses: pinn: 9.120602953771595e-06, boundary: 0.014880, weighted total: 0.0064653097\n",
      "Epoch 120. Current alpha: 0.431105, lr: 0.0000098057. Training losses: pinn: 1.1708170859492384e-05, boundary: 0.014650, weighted total: 0.0063225206\n",
      "Epoch 121. Current alpha: 0.428092, lr: 0.0000098057. Training losses: pinn: 9.053414942172822e-06, boundary: 0.014861, weighted total: 0.0063669816\n",
      "Epoch 122. Current alpha: 0.425101, lr: 0.0000098057. Training losses: pinn: 1.16303071990842e-05, boundary: 0.014632, weighted total: 0.0062267348\n",
      "Epoch 123. Current alpha: 0.422131, lr: 0.0000098057. Training losses: pinn: 8.993993105832487e-06, boundary: 0.014842, weighted total: 0.0062706405\n",
      "Epoch 124. Current alpha: 0.419182, lr: 0.0000098057. Training losses: pinn: 1.15453558464651e-05, boundary: 0.014611, weighted total: 0.0061312956\n",
      "Epoch 125. Current alpha: 0.416253, lr: 0.0000098057. Training losses: pinn: 8.928442184696905e-06, boundary: 0.014822, weighted total: 0.0061750575\n",
      "Epoch 126. Current alpha: 0.413344, lr: 0.0000098057. Training losses: pinn: 1.1461232134024613e-05, boundary: 0.014590, weighted total: 0.0060373827\n",
      "Epoch 127. Current alpha: 0.410456, lr: 0.0000098057. Training losses: pinn: 8.86307952896459e-06, boundary: 0.014804, weighted total: 0.0060815899\n",
      "Epoch 128. Current alpha: 0.407588, lr: 0.0000098057. Training losses: pinn: 1.1385548532416578e-05, boundary: 0.014572, weighted total: 0.0059459350\n",
      "Epoch 129. Current alpha: 0.404740, lr: 0.0000098057. Training losses: pinn: 8.805061952443793e-06, boundary: 0.014785, weighted total: 0.0059893310\n",
      "Epoch 130. Current alpha: 0.401912, lr: 0.0000098057. Training losses: pinn: 1.1304816325719003e-05, boundary: 0.014551, weighted total: 0.0058550558\n",
      "Epoch 131. Current alpha: 0.399104, lr: 0.0000098057. Training losses: pinn: 8.74177749210503e-06, boundary: 0.014766, weighted total: 0.0058985012\n",
      "Epoch 132. Current alpha: 0.396315, lr: 0.0000098057. Training losses: pinn: 1.122927187680034e-05, boundary: 0.014533, weighted total: 0.0057663605\n",
      "Epoch 133. Current alpha: 0.393546, lr: 0.0000098057. Training losses: pinn: 8.684482963872142e-06, boundary: 0.014747, weighted total: 0.0058090546\n",
      "Epoch 134. Current alpha: 0.390797, lr: 0.0000098057. Training losses: pinn: 1.114965107262833e-05, boundary: 0.014513, weighted total: 0.0056782537\n",
      "Epoch 135. Current alpha: 0.388066, lr: 0.0000098057. Training losses: pinn: 8.622552741144318e-06, boundary: 0.014736, weighted total: 0.0057237532\n",
      "Epoch 136. Current alpha: 0.385354, lr: 0.0000098057. Training losses: pinn: 1.1269268725300208e-05, boundary: 0.014396, weighted total: 0.0055543925\n",
      "Epoch 137. Current alpha: 0.382662, lr: 0.0000098057. Training losses: pinn: 8.714218893146608e-06, boundary: 0.014798, weighted total: 0.0056680716\n",
      "Epoch 138. Current alpha: 0.379988, lr: 0.0000098057. Training losses: pinn: 1.118804266297957e-05, boundary: 0.014375, weighted total: 0.0054693762\n",
      "Epoch 139. Current alpha: 0.377333, lr: 0.0000098057. Training losses: pinn: 8.651447387819644e-06, boundary: 0.014779, weighted total: 0.0055820757\n",
      "Epoch 140. Current alpha: 0.374697, lr: 0.0000098057. Training losses: pinn: 1.1107278623967431e-05, boundary: 0.014355, weighted total: 0.0053857296\n",
      "Epoch 141. Current alpha: 0.372079, lr: 0.0000098057. Training losses: pinn: 8.589006029069424e-06, boundary: 0.014760, weighted total: 0.0054973417\n",
      "Epoch 142. Current alpha: 0.369479, lr: 0.0000098057. Training losses: pinn: 1.1027088476112112e-05, boundary: 0.014335, weighted total: 0.0053033630\n",
      "Epoch 143. Current alpha: 0.366897, lr: 0.0000098057. Training losses: pinn: 8.527039426553529e-06, boundary: 0.014741, weighted total: 0.0054138641\n",
      "Epoch 144. Current alpha: 0.364334, lr: 0.0000098057. Training losses: pinn: 1.094748131436063e-05, boundary: 0.014314, weighted total: 0.0052221717\n",
      "Epoch 145. Current alpha: 0.361788, lr: 0.0000098057. Training losses: pinn: 8.465483006148133e-06, boundary: 0.014722, weighted total: 0.0053316231\n",
      "Epoch 146. Current alpha: 0.359260, lr: 0.0000098057. Training losses: pinn: 1.086855809262488e-05, boundary: 0.014294, weighted total: 0.0051423564\n",
      "Epoch 147. Current alpha: 0.356750, lr: 0.0000098057. Training losses: pinn: 8.401882951147854e-06, boundary: 0.014701, weighted total: 0.0052499282\n",
      "Epoch 148. Current alpha: 0.354257, lr: 0.0000098057. Training losses: pinn: 1.0786763596115634e-05, boundary: 0.014274, weighted total: 0.0050636435\n",
      "Epoch 149. Current alpha: 0.351782, lr: 0.0000098057. Training losses: pinn: 8.34135335026076e-06, boundary: 0.014682, weighted total: 0.0051702380\n",
      "Epoch 150. Current alpha: 0.349324, lr: 0.0000098057. Training losses: pinn: 1.0708962690841872e-05, boundary: 0.014254, weighted total: 0.0049861411\n",
      "Epoch 151. Current alpha: 0.346883, lr: 0.0000098057. Training losses: pinn: 8.281198461190797e-06, boundary: 0.014663, weighted total: 0.0050916870\n",
      "Epoch 152. Current alpha: 0.344459, lr: 0.0000098057. Training losses: pinn: 1.0631601071509067e-05, boundary: 0.014234, weighted total: 0.0049098809\n",
      "Epoch 153. Current alpha: 0.342053, lr: 0.0000098057. Training losses: pinn: 8.218600669351872e-06, boundary: 0.014642, weighted total: 0.0050138840\n",
      "Epoch 154. Current alpha: 0.339663, lr: 0.0000098057. Training losses: pinn: 1.0551416380621959e-05, boundary: 0.014214, weighted total: 0.0048349236\n",
      "Epoch 155. Current alpha: 0.337289, lr: 0.0000098057. Training losses: pinn: 8.15945168142207e-06, boundary: 0.014623, weighted total: 0.0049377159\n",
      "Epoch 156. Current alpha: 0.334933, lr: 0.0000098057. Training losses: pinn: 1.0475270755705424e-05, boundary: 0.014194, weighted total: 0.0047609495\n",
      "Epoch 157. Current alpha: 0.332592, lr: 0.0000098057. Training losses: pinn: 8.100634659058414e-06, boundary: 0.014604, weighted total: 0.0048627020\n",
      "Epoch 158. Current alpha: 0.330269, lr: 0.0000098057. Training losses: pinn: 1.0399827260698657e-05, boundary: 0.014174, weighted total: 0.0046880505\n",
      "Epoch 159. Current alpha: 0.327961, lr: 0.0000098057. Training losses: pinn: 8.039982276386581e-06, boundary: 0.014584, weighted total: 0.0047882685\n",
      "Epoch 160. Current alpha: 0.325669, lr: 0.0000098057. Training losses: pinn: 1.0327546078769956e-05, boundary: 0.014156, weighted total: 0.0046170693\n",
      "Epoch 161. Current alpha: 0.323394, lr: 0.0000098057. Training losses: pinn: 7.986991477082483e-06, boundary: 0.014572, weighted total: 0.0047179813\n",
      "Epoch 162. Current alpha: 0.321134, lr: 0.0000098057. Training losses: pinn: 1.0433555871713907e-05, boundary: 0.014055, weighted total: 0.0045205332\n",
      "Epoch 163. Current alpha: 0.318890, lr: 0.0000098057. Training losses: pinn: 8.06910884421086e-06, boundary: 0.014621, weighted total: 0.0046678339\n",
      "Epoch 164. Current alpha: 0.316662, lr: 0.0000098057. Training losses: pinn: 1.0359147381677758e-05, boundary: 0.014034, weighted total: 0.0044512618\n",
      "Epoch 165. Current alpha: 0.314450, lr: 0.0000098057. Training losses: pinn: 8.010849342099391e-06, boundary: 0.014602, weighted total: 0.0045969353\n",
      "Epoch 166. Current alpha: 0.312253, lr: 0.0000098057. Training losses: pinn: 1.0290008503943682e-05, boundary: 0.014016, weighted total: 0.0043835977\n",
      "Epoch 167. Current alpha: 0.310071, lr: 0.0000098057. Training losses: pinn: 7.957484740472864e-06, boundary: 0.014583, weighted total: 0.0045271313\n",
      "Epoch 168. Current alpha: 0.307904, lr: 0.0000098057. Training losses: pinn: 1.0215732800133992e-05, boundary: 0.013996, weighted total: 0.0043164749\n",
      "Epoch 169. Current alpha: 0.305753, lr: 0.0000098057. Training losses: pinn: 7.897860996308737e-06, boundary: 0.014562, weighted total: 0.0044578871\n",
      "Epoch 170. Current alpha: 0.303617, lr: 0.0000098057. Training losses: pinn: 1.0261226634611376e-05, boundary: 0.013976, weighted total: 0.0042505982\n",
      "Epoch 171. Current alpha: 0.301495, lr: 0.0000098057. Training losses: pinn: 7.84191615821328e-06, boundary: 0.014545, weighted total: 0.0043905774\n",
      "Epoch 172. Current alpha: 0.299389, lr: 0.0000098057. Training losses: pinn: 1.018728107737843e-05, boundary: 0.013956, weighted total: 0.0041855160\n",
      "Epoch 173. Current alpha: 0.297297, lr: 0.0000098057. Training losses: pinn: 7.784634362906218e-06, boundary: 0.014526, weighted total: 0.0043238550\n",
      "Epoch 174. Current alpha: 0.295219, lr: 0.0000098057. Training losses: pinn: 1.0113745702255983e-05, boundary: 0.013936, weighted total: 0.0041214504\n",
      "Epoch 175. Current alpha: 0.293157, lr: 0.0000098057. Training losses: pinn: 7.727756383246742e-06, boundary: 0.014507, weighted total: 0.0042584159\n",
      "Epoch 176. Current alpha: 0.291108, lr: 0.0000098057. Training losses: pinn: 1.0046360330306925e-05, boundary: 0.013918, weighted total: 0.0040588782\n",
      "Epoch 177. Current alpha: 0.289074, lr: 0.0000098057. Training losses: pinn: 7.676727364014369e-06, boundary: 0.014489, weighted total: 0.0041939708\n",
      "Epoch 178. Current alpha: 0.287054, lr: 0.0000098057. Training losses: pinn: 9.975763532565907e-06, boundary: 0.013898, weighted total: 0.0039966014\n",
      "Epoch 179. Current alpha: 0.285049, lr: 0.0000098057. Training losses: pinn: 7.620514224981889e-06, boundary: 0.014472, weighted total: 0.0041305951\n",
      "Epoch 180. Current alpha: 0.283057, lr: 0.0000098057. Training losses: pinn: 9.90612716122996e-06, boundary: 0.013879, weighted total: 0.0039355723\n",
      "Epoch 181. Current alpha: 0.281079, lr: 0.0000098057. Training losses: pinn: 7.567587545054266e-06, boundary: 0.014455, weighted total: 0.0040683131\n",
      "Epoch 182. Current alpha: 0.279115, lr: 0.0000098057. Training losses: pinn: 9.836012395680882e-06, boundary: 0.013859, weighted total: 0.0038754271\n",
      "Epoch 183. Current alpha: 0.277165, lr: 0.0000098057. Training losses: pinn: 7.5143566391489e-06, boundary: 0.014436, weighted total: 0.0040067018\n",
      "Epoch 184. Current alpha: 0.275229, lr: 0.0000098057. Training losses: pinn: 9.768054951564409e-06, boundary: 0.013840, weighted total: 0.0038162146\n",
      "Epoch 185. Current alpha: 0.273305, lr: 0.0000098057. Training losses: pinn: 7.461579116352368e-06, boundary: 0.014426, weighted total: 0.0039479989\n",
      "Epoch 186. Current alpha: 0.271396, lr: 0.0000098057. Training losses: pinn: 9.87261864793254e-06, boundary: 0.013725, weighted total: 0.0037321559\n",
      "Epoch 187. Current alpha: 0.269500, lr: 0.0000098057. Training losses: pinn: 7.541510967712384e-06, boundary: 0.014489, weighted total: 0.0039103061\n",
      "Epoch 188. Current alpha: 0.267617, lr: 0.0000098057. Training losses: pinn: 9.801493433769792e-06, boundary: 0.013705, weighted total: 0.0036747304\n",
      "Epoch 189. Current alpha: 0.265747, lr: 0.0000098057. Training losses: pinn: 7.486380582122365e-06, boundary: 0.014471, weighted total: 0.0038510733\n",
      "Epoch 190. Current alpha: 0.263890, lr: 0.0000098057. Training losses: pinn: 9.732302714837715e-06, boundary: 0.013685, weighted total: 0.0036184975\n",
      "Epoch 191. Current alpha: 0.262046, lr: 0.0000098057. Training losses: pinn: 7.432810434693238e-06, boundary: 0.014453, weighted total: 0.0037927581\n",
      "Epoch 192. Current alpha: 0.260215, lr: 0.0000098057. Training losses: pinn: 9.662108823249582e-06, boundary: 0.013664, weighted total: 0.0035628445\n",
      "Epoch 193. Current alpha: 0.258397, lr: 0.0000098057. Training losses: pinn: 7.3782989602477755e-06, boundary: 0.014434, weighted total: 0.0037352627\n",
      "Epoch 194. Current alpha: 0.256591, lr: 0.0000098057. Training losses: pinn: 9.592450624040794e-06, boundary: 0.013644, weighted total: 0.0035080170\n",
      "Epoch 195. Current alpha: 0.254799, lr: 0.0000098057. Training losses: pinn: 7.324356374738272e-06, boundary: 0.014416, weighted total: 0.0036786966\n",
      "Epoch 196. Current alpha: 0.253018, lr: 0.0000098057. Training losses: pinn: 9.524444976705126e-06, boundary: 0.013624, weighted total: 0.0034543311\n",
      "Epoch 197. Current alpha: 0.251250, lr: 0.0000098057. Training losses: pinn: 7.271692993526813e-06, boundary: 0.014398, weighted total: 0.0036229694\n",
      "Epoch 198. Current alpha: 0.249495, lr: 0.0000098057. Training losses: pinn: 9.455457984586246e-06, boundary: 0.013604, weighted total: 0.0034011802\n",
      "Epoch 199. Current alpha: 0.247752, lr: 0.0000098057. Training losses: pinn: 7.218329301394988e-06, boundary: 0.014380, weighted total: 0.0035680106\n",
      "Epoch 200. Current alpha: 0.246020, lr: 0.0000098057. Training losses: pinn: 9.386968486069236e-06, boundary: 0.013583, weighted total: 0.0033488562\n",
      "Epoch 201. Current alpha: 0.244301, lr: 0.0000098057. Training losses: pinn: 7.1663416747469455e-06, boundary: 0.014362, weighted total: 0.0035140305\n",
      "Epoch 202. Current alpha: 0.242594, lr: 0.0000098057. Training losses: pinn: 9.322021469415631e-06, boundary: 0.013564, weighted total: 0.0032975795\n",
      "Epoch 203. Current alpha: 0.240899, lr: 0.0000098057. Training losses: pinn: 7.115808784874389e-06, boundary: 0.014344, weighted total: 0.0034607962\n",
      "Epoch 204. Current alpha: 0.239216, lr: 0.0000098057. Training losses: pinn: 9.257456440536771e-06, boundary: 0.013544, weighted total: 0.0032471003\n",
      "Epoch 205. Current alpha: 0.237545, lr: 0.0000098057. Training losses: pinn: 7.0656560637871735e-06, boundary: 0.014326, weighted total: 0.0034083764\n",
      "Epoch 206. Current alpha: 0.235885, lr: 0.0000098057. Training losses: pinn: 9.191559911414515e-06, boundary: 0.013524, weighted total: 0.0031970796\n",
      "Epoch 207. Current alpha: 0.234237, lr: 0.0000098057. Training losses: pinn: 7.013491995166987e-06, boundary: 0.014308, weighted total: 0.0033568492\n",
      "Epoch 208. Current alpha: 0.232600, lr: 0.0000098057. Training losses: pinn: 9.130129910772666e-06, boundary: 0.013507, weighted total: 0.0031486308\n",
      "Epoch 209. Current alpha: 0.230975, lr: 0.0000098057. Training losses: pinn: 6.967386525502661e-06, boundary: 0.014290, weighted total: 0.0033060463\n",
      "Epoch 210. Current alpha: 0.229361, lr: 0.0000098057. Training losses: pinn: 9.067679457075428e-06, boundary: 0.013494, weighted total: 0.0031020895\n",
      "Epoch 211. Current alpha: 0.227758, lr: 0.0000098057. Training losses: pinn: 7.188897143350914e-06, boundary: 0.014199, weighted total: 0.0032394783\n",
      "Epoch 212. Current alpha: 0.226167, lr: 0.0000098057. Training losses: pinn: 9.355308066005819e-06, boundary: 0.013565, weighted total: 0.0030752584\n",
      "Epoch 213. Current alpha: 0.224587, lr: 0.0000093155. Training losses: pinn: 7.140520665416261e-06, boundary: 0.014181, weighted total: 0.0031903495\n",
      "Epoch 214. Current alpha: 0.223018, lr: 0.0000093155. Training losses: pinn: 9.171552846964914e-06, boundary: 0.013405, weighted total: 0.0029966333\n",
      "Epoch 215. Current alpha: 0.221459, lr: 0.0000093155. Training losses: pinn: 7.091561656125123e-06, boundary: 0.014164, weighted total: 0.0031422093\n",
      "Epoch 216. Current alpha: 0.219912, lr: 0.0000093155. Training losses: pinn: 9.110460268857423e-06, boundary: 0.013386, weighted total: 0.0029508776\n",
      "Epoch 217. Current alpha: 0.218375, lr: 0.0000093155. Training losses: pinn: 7.044707217573887e-06, boundary: 0.014148, weighted total: 0.0030950276\n",
      "Epoch 218. Current alpha: 0.216850, lr: 0.0000093155. Training losses: pinn: 9.195428901875857e-06, boundary: 0.013287, weighted total: 0.0028884489\n",
      "Epoch 219. Current alpha: 0.215334, lr: 0.0000093155. Training losses: pinn: 7.1095382736530155e-06, boundary: 0.014203, weighted total: 0.0030639347\n",
      "Epoch 220. Current alpha: 0.213830, lr: 0.0000093155. Training losses: pinn: 9.13003441382898e-06, boundary: 0.013267, weighted total: 0.0028439701\n",
      "Epoch 221. Current alpha: 0.212336, lr: 0.0000093155. Training losses: pinn: 7.058765731926542e-06, boundary: 0.014186, weighted total: 0.0030176505\n",
      "Epoch 222. Current alpha: 0.210852, lr: 0.0000093155. Training losses: pinn: 9.069320185517427e-06, boundary: 0.013249, weighted total: 0.0028006332\n",
      "Epoch 223. Current alpha: 0.209379, lr: 0.0000093155. Training losses: pinn: 7.271297363331541e-06, boundary: 0.014099, weighted total: 0.0029577252\n",
      "Epoch 224. Current alpha: 0.207916, lr: 0.0000093155. Training losses: pinn: 9.341717486677226e-06, boundary: 0.013322, weighted total: 0.0027773373\n",
      "Epoch 225. Current alpha: 0.206463, lr: 0.0000093155. Training losses: pinn: 7.22473760106368e-06, boundary: 0.014081, weighted total: 0.0029130415\n",
      "Epoch 226. Current alpha: 0.205021, lr: 0.0000093155. Training losses: pinn: 9.27838118514046e-06, boundary: 0.013304, weighted total: 0.0027349265\n",
      "Epoch 227. Current alpha: 0.203588, lr: 0.0000093155. Training losses: pinn: 7.175168775575003e-06, boundary: 0.014064, weighted total: 0.0028690408\n",
      "Epoch 228. Current alpha: 0.202166, lr: 0.0000093155. Training losses: pinn: 9.21647279028548e-06, boundary: 0.013285, weighted total: 0.0026931588\n",
      "Epoch 229. Current alpha: 0.200753, lr: 0.0000093155. Training losses: pinn: 7.127697699615965e-06, boundary: 0.014048, weighted total: 0.0028259464\n",
      "Epoch 230. Current alpha: 0.199350, lr: 0.0000093155. Training losses: pinn: 9.153990504273679e-06, boundary: 0.013267, weighted total: 0.0026520128\n",
      "Epoch 231. Current alpha: 0.197957, lr: 0.0000093155. Training losses: pinn: 7.078794624248985e-06, boundary: 0.014031, weighted total: 0.0027831951\n",
      "Epoch 232. Current alpha: 0.196574, lr: 0.0000093155. Training losses: pinn: 9.09196023712866e-06, boundary: 0.013248, weighted total: 0.0026115036\n",
      "Epoch 233. Current alpha: 0.195201, lr: 0.0000093155. Training losses: pinn: 7.030226697679609e-06, boundary: 0.014014, weighted total: 0.0027411059\n",
      "Epoch 234. Current alpha: 0.193837, lr: 0.0000093155. Training losses: pinn: 9.027253327076323e-06, boundary: 0.013228, weighted total: 0.0025713011\n",
      "Epoch 235. Current alpha: 0.192482, lr: 0.0000093155. Training losses: pinn: 6.979825229791459e-06, boundary: 0.013997, weighted total: 0.0026997955\n",
      "Epoch 236. Current alpha: 0.191138, lr: 0.0000093155. Training losses: pinn: 8.96987876330968e-06, boundary: 0.013211, weighted total: 0.0025323879\n",
      "Epoch 237. Current alpha: 0.189802, lr: 0.0000093155. Training losses: pinn: 6.935357760085026e-06, boundary: 0.013980, weighted total: 0.0026590024\n",
      "Epoch 238. Current alpha: 0.188476, lr: 0.0000093155. Training losses: pinn: 8.910051292332355e-06, boundary: 0.013211, weighted total: 0.0024971673\n",
      "Epoch 239. Current alpha: 0.187159, lr: 0.0000093155. Training losses: pinn: 7.143452876334777e-06, boundary: 0.013893, weighted total: 0.0026060213\n",
      "Epoch 240. Current alpha: 0.185851, lr: 0.0000093155. Training losses: pinn: 9.17674242373323e-06, boundary: 0.013267, weighted total: 0.0024730716\n",
      "Epoch 241. Current alpha: 0.184553, lr: 0.0000088497. Training losses: pinn: 7.09764935891144e-06, boundary: 0.013876, weighted total: 0.0025665720\n",
      "Epoch 242. Current alpha: 0.183263, lr: 0.0000088497. Training losses: pinn: 9.005207175505348e-06, boundary: 0.013114, weighted total: 0.0024106782\n",
      "Epoch 243. Current alpha: 0.181983, lr: 0.0000088497. Training losses: pinn: 7.0514288381673396e-06, boundary: 0.013860, weighted total: 0.0025279753\n",
      "Epoch 244. Current alpha: 0.180711, lr: 0.0000088497. Training losses: pinn: 8.947906280809548e-06, boundary: 0.013096, weighted total: 0.0023739880\n",
      "Epoch 245. Current alpha: 0.179448, lr: 0.0000088497. Training losses: pinn: 7.007042768236715e-06, boundary: 0.013844, weighted total: 0.0024900653\n",
      "Epoch 246. Current alpha: 0.178195, lr: 0.0000088497. Training losses: pinn: 8.890105164027773e-06, boundary: 0.013095, weighted total: 0.0023407073\n",
      "Epoch 247. Current alpha: 0.176950, lr: 0.0000088497. Training losses: pinn: 7.2045927481667604e-06, boundary: 0.013761, weighted total: 0.0024409657\n",
      "Epoch 248. Current alpha: 0.175713, lr: 0.0000088497. Training losses: pinn: 9.142512681137305e-06, boundary: 0.013149, weighted total: 0.0023179708\n",
      "Epoch 249. Current alpha: 0.174485, lr: 0.0000088497. Training losses: pinn: 7.160783752624411e-06, boundary: 0.013745, weighted total: 0.0024041716\n",
      "Epoch 250. Current alpha: 0.173266, lr: 0.0000088497. Training losses: pinn: 9.084400517167524e-06, boundary: 0.013131, weighted total: 0.0022827095\n",
      "Epoch 251. Current alpha: 0.172056, lr: 0.0000088497. Training losses: pinn: 7.115680546121439e-06, boundary: 0.013729, weighted total: 0.0023680736\n",
      "Epoch 252. Current alpha: 0.170853, lr: 0.0000088497. Training losses: pinn: 9.025631698023062e-06, boundary: 0.013113, weighted total: 0.0022479498\n",
      "Epoch 253. Current alpha: 0.169660, lr: 0.0000088497. Training losses: pinn: 7.068981176416855e-06, boundary: 0.013713, weighted total: 0.0023323411\n",
      "Epoch 254. Current alpha: 0.168474, lr: 0.0000088497. Training losses: pinn: 8.966870154836215e-06, boundary: 0.013096, weighted total: 0.0022137582\n",
      "Epoch 255. Current alpha: 0.167297, lr: 0.0000088497. Training losses: pinn: 7.022722456895281e-06, boundary: 0.013696, weighted total: 0.0022972211\n",
      "Epoch 256. Current alpha: 0.166128, lr: 0.0000088497. Training losses: pinn: 8.910001270123757e-06, boundary: 0.013078, weighted total: 0.0021800612\n",
      "Epoch 257. Current alpha: 0.164967, lr: 0.0000088497. Training losses: pinn: 6.978611963859294e-06, boundary: 0.013681, weighted total: 0.0022627547\n",
      "Epoch 258. Current alpha: 0.163815, lr: 0.0000088497. Training losses: pinn: 8.852262908476405e-06, boundary: 0.013060, weighted total: 0.0021468717\n",
      "Epoch 259. Current alpha: 0.162670, lr: 0.0000088497. Training losses: pinn: 6.932842552487273e-06, boundary: 0.013664, weighted total: 0.0022285988\n",
      "Epoch 260. Current alpha: 0.161533, lr: 0.0000088497. Training losses: pinn: 8.794993846095167e-06, boundary: 0.013058, weighted total: 0.0021167478\n",
      "Epoch 261. Current alpha: 0.160405, lr: 0.0000088497. Training losses: pinn: 7.127576736820629e-06, boundary: 0.013582, weighted total: 0.0021846583\n",
      "Epoch 262. Current alpha: 0.159284, lr: 0.0000088497. Training losses: pinn: 9.040913937496953e-06, boundary: 0.013111, weighted total: 0.0020959359\n",
      "Epoch 263. Current alpha: 0.158171, lr: 0.0000088497. Training losses: pinn: 7.08259312887094e-06, boundary: 0.013566, weighted total: 0.0021517253\n",
      "Epoch 264. Current alpha: 0.157066, lr: 0.0000088497. Training losses: pinn: 8.98624330147868e-06, boundary: 0.013095, weighted total: 0.0020643142\n",
      "Epoch 265. Current alpha: 0.155968, lr: 0.0000088497. Training losses: pinn: 7.0396577029896434e-06, boundary: 0.013550, weighted total: 0.0021192588\n",
      "Epoch 266. Current alpha: 0.154879, lr: 0.0000088497. Training losses: pinn: 8.926326700020581e-06, boundary: 0.013076, weighted total: 0.0020328133\n",
      "Epoch 267. Current alpha: 0.153797, lr: 0.0000088497. Training losses: pinn: 6.992607723077526e-06, boundary: 0.013534, weighted total: 0.0020873695\n",
      "Epoch 268. Current alpha: 0.152722, lr: 0.0000088497. Training losses: pinn: 8.869545126799494e-06, boundary: 0.013059, weighted total: 0.0020018544\n",
      "Epoch 269. Current alpha: 0.151655, lr: 0.0000088497. Training losses: pinn: 6.948581813048804e-06, boundary: 0.013523, weighted total: 0.0020566936\n",
      "Epoch 270. Current alpha: 0.150595, lr: 0.0000088497. Training losses: pinn: 8.947556125349365e-06, boundary: 0.012962, weighted total: 0.0019596797\n",
      "Epoch 271. Current alpha: 0.149543, lr: 0.0000088497. Training losses: pinn: 7.009424734860659e-06, boundary: 0.013572, weighted total: 0.0020356158\n",
      "Epoch 272. Current alpha: 0.148498, lr: 0.0000088497. Training losses: pinn: 8.88811337063089e-06, boundary: 0.012943, weighted total: 0.0019295860\n",
      "Epoch 273. Current alpha: 0.147460, lr: 0.0000088497. Training losses: pinn: 6.9617699409718625e-06, boundary: 0.013556, weighted total: 0.0020049196\n",
      "Epoch 274. Current alpha: 0.146430, lr: 0.0000088497. Training losses: pinn: 8.831603736325633e-06, boundary: 0.012925, weighted total: 0.0019001974\n",
      "Epoch 275. Current alpha: 0.145407, lr: 0.0000088497. Training losses: pinn: 6.9177795012365095e-06, boundary: 0.013541, weighted total: 0.0019748424\n",
      "Epoch 276. Current alpha: 0.144391, lr: 0.0000088497. Training losses: pinn: 8.77268576005008e-06, boundary: 0.012907, weighted total: 0.0018711453\n",
      "Epoch 277. Current alpha: 0.143382, lr: 0.0000088497. Training losses: pinn: 6.871414370834827e-06, boundary: 0.013524, weighted total: 0.0019450235\n",
      "Epoch 278. Current alpha: 0.142380, lr: 0.0000088497. Training losses: pinn: 8.715881449461449e-06, boundary: 0.012902, weighted total: 0.0018445154\n",
      "Epoch 279. Current alpha: 0.141385, lr: 0.0000088497. Training losses: pinn: 7.063834345899522e-06, boundary: 0.013442, weighted total: 0.0019065666\n",
      "Epoch 280. Current alpha: 0.140398, lr: 0.0000088497. Training losses: pinn: 8.961310413724277e-06, boundary: 0.012959, weighted total: 0.0018271453\n",
      "Epoch 281. Current alpha: 0.139417, lr: 0.0000088497. Training losses: pinn: 7.020226803433616e-06, boundary: 0.013426, weighted total: 0.0018778307\n",
      "Epoch 282. Current alpha: 0.138442, lr: 0.0000088497. Training losses: pinn: 8.90391311259009e-06, boundary: 0.012940, weighted total: 0.0017991542\n",
      "Epoch 283. Current alpha: 0.137475, lr: 0.0000088497. Training losses: pinn: 6.975586984481197e-06, boundary: 0.013410, weighted total: 0.0018495938\n",
      "Epoch 284. Current alpha: 0.136515, lr: 0.0000088497. Training losses: pinn: 8.84849032445345e-06, boundary: 0.012924, weighted total: 0.0017719379\n",
      "Epoch 285. Current alpha: 0.135561, lr: 0.0000088497. Training losses: pinn: 6.931955340405693e-06, boundary: 0.013394, weighted total: 0.0018216473\n",
      "Epoch 286. Current alpha: 0.134614, lr: 0.0000088497. Training losses: pinn: 8.79066556080943e-06, boundary: 0.012906, weighted total: 0.0017449591\n",
      "Epoch 287. Current alpha: 0.133673, lr: 0.0000088497. Training losses: pinn: 6.885969924042001e-06, boundary: 0.013377, weighted total: 0.0017941669\n",
      "Epoch 288. Current alpha: 0.132739, lr: 0.0000088497. Training losses: pinn: 8.734353286854457e-06, boundary: 0.012887, weighted total: 0.0017182397\n",
      "Epoch 289. Current alpha: 0.131812, lr: 0.0000088497. Training losses: pinn: 6.842178208898986e-06, boundary: 0.013362, weighted total: 0.0017672671\n",
      "Epoch 290. Current alpha: 0.130891, lr: 0.0000088497. Training losses: pinn: 8.677281584823504e-06, boundary: 0.012870, weighted total: 0.0016920680\n",
      "Epoch 291. Current alpha: 0.129976, lr: 0.0000088497. Training losses: pinn: 6.796758043492446e-06, boundary: 0.013346, weighted total: 0.0017405531\n",
      "Epoch 292. Current alpha: 0.129068, lr: 0.0000088497. Training losses: pinn: 8.623299436294474e-06, boundary: 0.012867, weighted total: 0.0016682473\n",
      "Epoch 293. Current alpha: 0.128166, lr: 0.0000088497. Training losses: pinn: 6.987857886997517e-06, boundary: 0.013266, weighted total: 0.0017062822\n",
      "Epoch 294. Current alpha: 0.127270, lr: 0.0000088497. Training losses: pinn: 8.861398782755714e-06, boundary: 0.012921, weighted total: 0.0016522035\n",
      "Epoch 295. Current alpha: 0.126381, lr: 0.0000088497. Training losses: pinn: 6.9429852374014445e-06, boundary: 0.013249, weighted total: 0.0016804864\n",
      "Epoch 296. Current alpha: 0.125498, lr: 0.0000088497. Training losses: pinn: 8.804430763120763e-06, boundary: 0.012904, weighted total: 0.0016270761\n",
      "Epoch 297. Current alpha: 0.124621, lr: 0.0000088497. Training losses: pinn: 6.89837315803743e-06, boundary: 0.013233, weighted total: 0.0016551051\n",
      "Epoch 298. Current alpha: 0.123751, lr: 0.0000088497. Training losses: pinn: 8.749414519115817e-06, boundary: 0.012887, weighted total: 0.0016024509\n",
      "Epoch 299. Current alpha: 0.122886, lr: 0.0000088497. Training losses: pinn: 6.855012998130405e-06, boundary: 0.013216, weighted total: 0.0016301150\n",
      "Epoch 300. Current alpha: 0.122027, lr: 0.0000088497. Training losses: pinn: 8.692326446180232e-06, boundary: 0.012868, weighted total: 0.0015779099\n",
      "Epoch 301. Current alpha: 0.121175, lr: 0.0000088497. Training losses: pinn: 6.810676040913677e-06, boundary: 0.013201, weighted total: 0.0016055969\n",
      "Epoch 302. Current alpha: 0.120328, lr: 0.0000088497. Training losses: pinn: 8.636341590317897e-06, boundary: 0.012851, weighted total: 0.0015539171\n",
      "Epoch 303. Current alpha: 0.119487, lr: 0.0000088497. Training losses: pinn: 6.766179922124138e-06, boundary: 0.013185, weighted total: 0.0015813370\n",
      "Epoch 304. Current alpha: 0.118652, lr: 0.0000088497. Training losses: pinn: 8.582105692767072e-06, boundary: 0.012834, weighted total: 0.0015303589\n",
      "Epoch 305. Current alpha: 0.117823, lr: 0.0000088497. Training losses: pinn: 6.723771548422519e-06, boundary: 0.013168, weighted total: 0.0015574823\n",
      "Epoch 306. Current alpha: 0.117000, lr: 0.0000088497. Training losses: pinn: 8.524162694811821e-06, boundary: 0.012833, weighted total: 0.0015089413\n",
      "Epoch 307. Current alpha: 0.116183, lr: 0.0000088497. Training losses: pinn: 6.908754585310817e-06, boundary: 0.013089, weighted total: 0.0015267712\n",
      "Epoch 308. Current alpha: 0.115371, lr: 0.0000088497. Training losses: pinn: 8.758759577176534e-06, boundary: 0.012885, weighted total: 0.0014942486\n",
      "Epoch 309. Current alpha: 0.114565, lr: 0.0000088497. Training losses: pinn: 6.864274382678559e-06, boundary: 0.013073, weighted total: 0.0015037283\n",
      "Epoch 310. Current alpha: 0.113764, lr: 0.0000088497. Training losses: pinn: 8.701432307134382e-06, boundary: 0.012866, weighted total: 0.0014713549\n",
      "Epoch 311. Current alpha: 0.112969, lr: 0.0000088497. Training losses: pinn: 6.819763257226441e-06, boundary: 0.013057, weighted total: 0.0014810506\n",
      "Epoch 312. Current alpha: 0.112180, lr: 0.0000088497. Training losses: pinn: 8.645536581752822e-06, boundary: 0.012849, weighted total: 0.0014490909\n",
      "Epoch 313. Current alpha: 0.111396, lr: 0.0000088497. Training losses: pinn: 6.775876045139739e-06, boundary: 0.013047, weighted total: 0.0014594542\n",
      "Epoch 314. Current alpha: 0.110618, lr: 0.0000088497. Training losses: pinn: 8.617877938377205e-06, boundary: 0.012732, weighted total: 0.0014160984\n",
      "Epoch 315. Current alpha: 0.109845, lr: 0.0000088497. Training losses: pinn: 6.833882252976764e-06, boundary: 0.013113, weighted total: 0.0014464824\n",
      "Epoch 316. Current alpha: 0.109077, lr: 0.0000088497. Training losses: pinn: 8.662097570777405e-06, boundary: 0.012715, weighted total: 0.0013946600\n",
      "Epoch 317. Current alpha: 0.108315, lr: 0.0000088497. Training losses: pinn: 6.7886403485317715e-06, boundary: 0.013096, weighted total: 0.0014245757\n",
      "Epoch 318. Current alpha: 0.107558, lr: 0.0000084072. Training losses: pinn: 8.605839866504539e-06, boundary: 0.012698, weighted total: 0.0013734728\n",
      "Epoch 319. Current alpha: 0.106807, lr: 0.0000084072. Training losses: pinn: 6.822494469815865e-06, boundary: 0.012948, weighted total: 0.0013890363\n",
      "Epoch 320. Current alpha: 0.106061, lr: 0.0000084072. Training losses: pinn: 8.552569852326997e-06, boundary: 0.012682, weighted total: 0.0013527031\n",
      "Epoch 321. Current alpha: 0.105319, lr: 0.0000084072. Training losses: pinn: 6.779778232157696e-06, boundary: 0.012933, weighted total: 0.0013681136\n",
      "Epoch 322. Current alpha: 0.104584, lr: 0.0000084072. Training losses: pinn: 8.497258022543974e-06, boundary: 0.012665, weighted total: 0.0013321468\n",
      "Epoch 323. Current alpha: 0.103853, lr: 0.0000084072. Training losses: pinn: 6.735815986758098e-06, boundary: 0.012917, weighted total: 0.0013474838\n",
      "Epoch 324. Current alpha: 0.103127, lr: 0.0000084072. Training losses: pinn: 8.444751983915921e-06, boundary: 0.012666, weighted total: 0.0013137819\n",
      "Epoch 325. Current alpha: 0.102407, lr: 0.0000084072. Training losses: pinn: 6.912861863384023e-06, boundary: 0.012838, weighted total: 0.0013209346\n",
      "Epoch 326. Current alpha: 0.101691, lr: 0.0000084072. Training losses: pinn: 8.66213122208137e-06, boundary: 0.012713, weighted total: 0.0013005791\n",
      "Epoch 327. Current alpha: 0.100981, lr: 0.0000084072. Training losses: pinn: 6.8690546868310776e-06, boundary: 0.012823, weighted total: 0.0013010221\n",
      "Epoch 328. Current alpha: 0.100275, lr: 0.0000084072. Training losses: pinn: 8.607001291238703e-06, boundary: 0.012696, weighted total: 0.0012808799\n",
      "Epoch 329. Current alpha: 0.099574, lr: 0.0000084072. Training losses: pinn: 6.82534391671652e-06, boundary: 0.012808, weighted total: 0.0012814646\n",
      "Epoch 330. Current alpha: 0.098879, lr: 0.0000084072. Training losses: pinn: 8.555512067687232e-06, boundary: 0.012681, weighted total: 0.0012615818\n",
      "Epoch 331. Current alpha: 0.098188, lr: 0.0000084072. Training losses: pinn: 6.78453579894267e-06, boundary: 0.012799, weighted total: 0.0012628309\n",
      "Epoch 332. Current alpha: 0.097502, lr: 0.0000084072. Training losses: pinn: 8.629672265669797e-06, boundary: 0.012587, weighted total: 0.0012350673\n",
      "Epoch 333. Current alpha: 0.096820, lr: 0.0000084072. Training losses: pinn: 6.842911716375966e-06, boundary: 0.012849, weighted total: 0.0012501804\n",
      "Epoch 334. Current alpha: 0.096144, lr: 0.0000084072. Training losses: pinn: 8.578464985475875e-06, boundary: 0.012572, weighted total: 0.0012164587\n",
      "Epoch 335. Current alpha: 0.095472, lr: 0.0000084072. Training losses: pinn: 6.801974905101815e-06, boundary: 0.012834, weighted total: 0.0012314085\n",
      "Epoch 336. Current alpha: 0.094805, lr: 0.0000084072. Training losses: pinn: 8.527988029527478e-06, boundary: 0.012557, weighted total: 0.0011981455\n",
      "Epoch 337. Current alpha: 0.094143, lr: 0.0000084072. Training losses: pinn: 6.761173608538229e-06, boundary: 0.012819, weighted total: 0.0012129187\n",
      "Epoch 338. Current alpha: 0.093485, lr: 0.0000084072. Training losses: pinn: 8.473470188619103e-06, boundary: 0.012540, weighted total: 0.0011799662\n",
      "Epoch 339. Current alpha: 0.092832, lr: 0.0000084072. Training losses: pinn: 6.718243184877792e-06, boundary: 0.012804, weighted total: 0.0011947139\n",
      "Epoch 340. Current alpha: 0.092183, lr: 0.0000084072. Training losses: pinn: 8.423368853982538e-06, boundary: 0.012525, weighted total: 0.0011622313\n",
      "Epoch 341. Current alpha: 0.091539, lr: 0.0000084072. Training losses: pinn: 6.678398676740471e-06, boundary: 0.012789, weighted total: 0.0011767499\n",
      "Epoch 342. Current alpha: 0.090899, lr: 0.0000084072. Training losses: pinn: 8.372386218979955e-06, boundary: 0.012523, weighted total: 0.0011459717\n",
      "Epoch 343. Current alpha: 0.090264, lr: 0.0000084072. Training losses: pinn: 6.852750175312394e-06, boundary: 0.012713, weighted total: 0.0011537474\n",
      "Epoch 344. Current alpha: 0.089634, lr: 0.0000084072. Training losses: pinn: 8.588238415541127e-06, boundary: 0.012573, weighted total: 0.0011347921\n",
      "Epoch 345. Current alpha: 0.089007, lr: 0.0000079868. Training losses: pinn: 6.811050297983456e-06, boundary: 0.012698, weighted total: 0.0011363799\n",
      "Epoch 346. Current alpha: 0.088385, lr: 0.0000079868. Training losses: pinn: 8.346975846507121e-06, boundary: 0.012437, weighted total: 0.0011068917\n",
      "Epoch 347. Current alpha: 0.087768, lr: 0.0000079868. Training losses: pinn: 6.772500455554109e-06, boundary: 0.012683, weighted total: 0.0011193500\n",
      "Epoch 348. Current alpha: 0.087155, lr: 0.0000079868. Training losses: pinn: 8.396814337174874e-06, boundary: 0.012422, weighted total: 0.0010902785\n",
      "Epoch 349. Current alpha: 0.086546, lr: 0.0000079868. Training losses: pinn: 6.7333189690543804e-06, boundary: 0.012670, weighted total: 0.0011026577\n",
      "Epoch 350. Current alpha: 0.085941, lr: 0.0000079868. Training losses: pinn: 8.347744369530119e-06, boundary: 0.012423, weighted total: 0.0010752576\n",
      "Epoch 351. Current alpha: 0.085340, lr: 0.0000079868. Training losses: pinn: 6.898977972014109e-06, boundary: 0.012597, weighted total: 0.0010813576\n",
      "Epoch 352. Current alpha: 0.084744, lr: 0.0000079868. Training losses: pinn: 8.453884220216423e-06, boundary: 0.012468, weighted total: 0.0010642883\n",
      "Epoch 353. Current alpha: 0.084152, lr: 0.0000079868. Training losses: pinn: 6.859175300633069e-06, boundary: 0.012583, weighted total: 0.0010652039\n",
      "Epoch 354. Current alpha: 0.083564, lr: 0.0000079868. Training losses: pinn: 8.407592758885585e-06, boundary: 0.012453, weighted total: 0.0010483477\n",
      "Epoch 355. Current alpha: 0.082980, lr: 0.0000079868. Training losses: pinn: 6.821637271059444e-06, boundary: 0.012569, weighted total: 0.0010492662\n",
      "Epoch 356. Current alpha: 0.082400, lr: 0.0000079868. Training losses: pinn: 8.456774594378658e-06, boundary: 0.012438, weighted total: 0.0010326468\n",
      "Epoch 357. Current alpha: 0.081825, lr: 0.0000075875. Training losses: pinn: 6.781631327612558e-06, boundary: 0.012556, weighted total: 0.0010335948\n",
      "Epoch 358. Current alpha: 0.081253, lr: 0.0000075875. Training losses: pinn: 8.228247679653578e-06, boundary: 0.012327, weighted total: 0.0010091792\n",
      "Epoch 359. Current alpha: 0.080685, lr: 0.0000075875. Training losses: pinn: 6.943153948668623e-06, boundary: 0.012486, weighted total: 0.0010137939\n",
      "Epoch 360. Current alpha: 0.080121, lr: 0.0000075875. Training losses: pinn: 8.522924872522708e-06, boundary: 0.012367, weighted total: 0.0009987316\n",
      "Epoch 361. Current alpha: 0.079562, lr: 0.0000075875. Training losses: pinn: 6.905475856910925e-06, boundary: 0.012473, weighted total: 0.0009986901\n",
      "Epoch 362. Current alpha: 0.079006, lr: 0.0000075875. Training losses: pinn: 8.478119525534566e-06, boundary: 0.012354, weighted total: 0.0009838558\n",
      "Epoch 363. Current alpha: 0.078454, lr: 0.0000075875. Training losses: pinn: 6.869369826745242e-06, boundary: 0.012459, weighted total: 0.0009838112\n",
      "Epoch 364. Current alpha: 0.077905, lr: 0.0000075875. Training losses: pinn: 8.431441528955474e-06, boundary: 0.012339, weighted total: 0.0009690723\n",
      "Epoch 365. Current alpha: 0.077361, lr: 0.0000075875. Training losses: pinn: 6.832055987615604e-06, boundary: 0.012446, weighted total: 0.0009691629\n",
      "Epoch 366. Current alpha: 0.076821, lr: 0.0000075875. Training losses: pinn: 8.386846275243443e-06, boundary: 0.012326, weighted total: 0.0009546514\n",
      "Epoch 367. Current alpha: 0.076284, lr: 0.0000075875. Training losses: pinn: 6.796343313908437e-06, boundary: 0.012433, weighted total: 0.0009547362\n",
      "Epoch 368. Current alpha: 0.075751, lr: 0.0000075875. Training losses: pinn: 8.33967078506248e-06, boundary: 0.012311, weighted total: 0.0009403122\n",
      "Epoch 369. Current alpha: 0.075221, lr: 0.0000075875. Training losses: pinn: 6.757596565876156e-06, boundary: 0.012420, weighted total: 0.0009404962\n",
      "Epoch 370. Current alpha: 0.074696, lr: 0.0000075875. Training losses: pinn: 8.2935630416614e-06, boundary: 0.012299, weighted total: 0.0009263500\n",
      "Epoch 371. Current alpha: 0.074174, lr: 0.0000075875. Training losses: pinn: 6.915503035997972e-06, boundary: 0.012352, weighted total: 0.0009225931\n",
      "Epoch 372. Current alpha: 0.073656, lr: 0.0000075875. Training losses: pinn: 8.389888535020873e-06, boundary: 0.012356, weighted total: 0.0009178710\n",
      "Epoch 373. Current alpha: 0.073141, lr: 0.0000072081. Training losses: pinn: 6.879297870909795e-06, boundary: 0.012338, weighted total: 0.0009088145\n",
      "Epoch 374. Current alpha: 0.072630, lr: 0.0000072081. Training losses: pinn: 8.26550785859581e-06, boundary: 0.012234, weighted total: 0.0008962069\n",
      "Epoch 375. Current alpha: 0.072123, lr: 0.0000072081. Training losses: pinn: 6.844768449809635e-06, boundary: 0.012325, weighted total: 0.0008952783\n",
      "Epoch 376. Current alpha: 0.071619, lr: 0.0000072081. Training losses: pinn: 8.222740689234342e-06, boundary: 0.012220, weighted total: 0.0008828125\n",
      "Epoch 377. Current alpha: 0.071118, lr: 0.0000072081. Training losses: pinn: 6.809373189753387e-06, boundary: 0.012313, weighted total: 0.0008819694\n",
      "Epoch 378. Current alpha: 0.070621, lr: 0.0000072081. Training losses: pinn: 8.181393241102342e-06, boundary: 0.012209, weighted total: 0.0008698324\n",
      "Epoch 379. Current alpha: 0.070128, lr: 0.0000072081. Training losses: pinn: 6.965677130210679e-06, boundary: 0.012244, weighted total: 0.0008651509\n",
      "Epoch 380. Current alpha: 0.069638, lr: 0.0000072081. Training losses: pinn: 8.467940460832324e-06, boundary: 0.012265, weighted total: 0.0008620065\n",
      "Epoch 381. Current alpha: 0.069151, lr: 0.0000072081. Training losses: pinn: 6.9297793743317015e-06, boundary: 0.012232, weighted total: 0.0008523252\n",
      "Epoch 382. Current alpha: 0.068668, lr: 0.0000072081. Training losses: pinn: 8.425499800068792e-06, boundary: 0.012253, weighted total: 0.0008492114\n",
      "Epoch 383. Current alpha: 0.068188, lr: 0.0000072081. Training losses: pinn: 6.894955731695518e-06, boundary: 0.012220, weighted total: 0.0008396737\n",
      "Epoch 384. Current alpha: 0.067712, lr: 0.0000072081. Training losses: pinn: 8.38314645079663e-06, boundary: 0.012240, weighted total: 0.0008366088\n",
      "Epoch 385. Current alpha: 0.067239, lr: 0.0000068477. Training losses: pinn: 6.860303983557969e-06, boundary: 0.012207, weighted total: 0.0008272055\n",
      "Epoch 386. Current alpha: 0.066769, lr: 0.0000068477. Training losses: pinn: 8.26159339339938e-06, boundary: 0.012123, weighted total: 0.0008171696\n",
      "Epoch 387. Current alpha: 0.066302, lr: 0.0000068477. Training losses: pinn: 7.003155133133987e-06, boundary: 0.012146, weighted total: 0.0008118549\n",
      "Epoch 388. Current alpha: 0.065839, lr: 0.0000068477. Training losses: pinn: 8.336775863426737e-06, boundary: 0.012175, weighted total: 0.0008093918\n",
      "Epoch 389. Current alpha: 0.065379, lr: 0.0000068477. Training losses: pinn: 6.969559763092548e-06, boundary: 0.012134, weighted total: 0.0007998196\n",
      "Epoch 390. Current alpha: 0.064922, lr: 0.0000068477. Training losses: pinn: 8.296641681226902e-06, boundary: 0.012163, weighted total: 0.0007974150\n",
      "Epoch 391. Current alpha: 0.064469, lr: 0.0000068477. Training losses: pinn: 6.9358839027700014e-06, boundary: 0.012122, weighted total: 0.0007879504\n",
      "Epoch 392. Current alpha: 0.064018, lr: 0.0000068477. Training losses: pinn: 8.255229658971075e-06, boundary: 0.012150, weighted total: 0.0007855324\n",
      "Epoch 393. Current alpha: 0.063571, lr: 0.0000068477. Training losses: pinn: 6.901318556629121e-06, boundary: 0.012109, weighted total: 0.0007762623\n",
      "Epoch 394. Current alpha: 0.063127, lr: 0.0000068477. Training losses: pinn: 8.312679710797966e-06, boundary: 0.012138, weighted total: 0.0007740075\n",
      "Epoch 395. Current alpha: 0.062686, lr: 0.0000068477. Training losses: pinn: 6.8676008595502935e-06, boundary: 0.012097, weighted total: 0.0007647639\n",
      "Epoch 396. Current alpha: 0.062248, lr: 0.0000068477. Training losses: pinn: 8.269104910141323e-06, boundary: 0.012124, weighted total: 0.0007624601\n",
      "Epoch 397. Current alpha: 0.061813, lr: 0.0000065053. Training losses: pinn: 6.831846803834196e-06, boundary: 0.012087, weighted total: 0.0007535130\n",
      "Epoch 398. Current alpha: 0.061381, lr: 0.0000065053. Training losses: pinn: 8.14898703538347e-06, boundary: 0.011942, weighted total: 0.0007406361\n",
      "Epoch 399. Current alpha: 0.060952, lr: 0.0000065053. Training losses: pinn: 7.043209734547418e-06, boundary: 0.012092, weighted total: 0.0007436490\n",
      "Epoch 400. Current alpha: 0.060526, lr: 0.0000065053. Training losses: pinn: 8.405817425227724e-06, boundary: 0.011984, weighted total: 0.0007332657\n",
      "Epoch 401. Current alpha: 0.060103, lr: 0.0000065053. Training losses: pinn: 7.006822215771535e-06, boundary: 0.012081, weighted total: 0.0007326641\n",
      "Epoch 402. Current alpha: 0.059683, lr: 0.0000065053. Training losses: pinn: 8.362183507415466e-06, boundary: 0.011971, weighted total: 0.0007223417\n",
      "Epoch 403. Current alpha: 0.059266, lr: 0.0000065053. Training losses: pinn: 6.970671620365465e-06, boundary: 0.012069, weighted total: 0.0007218359\n",
      "Epoch 404. Current alpha: 0.058852, lr: 0.0000065053. Training losses: pinn: 8.31855595606612e-06, boundary: 0.011958, weighted total: 0.0007115782\n",
      "Epoch 405. Current alpha: 0.058441, lr: 0.0000065053. Training losses: pinn: 6.934571047167992e-06, boundary: 0.012057, weighted total: 0.0007111788\n",
      "Epoch 406. Current alpha: 0.058033, lr: 0.0000065053. Training losses: pinn: 8.275089385278989e-06, boundary: 0.011945, weighted total: 0.0007009746\n",
      "Epoch 407. Current alpha: 0.057627, lr: 0.0000065053. Training losses: pinn: 6.89852731738938e-06, boundary: 0.012046, weighted total: 0.0007006688\n",
      "Epoch 408. Current alpha: 0.057224, lr: 0.0000065053. Training losses: pinn: 8.137528311635833e-06, boundary: 0.011932, weighted total: 0.0006904731\n",
      "Epoch 409. Current alpha: 0.056825, lr: 0.0000065053. Training losses: pinn: 6.8642643782368395e-06, boundary: 0.012034, weighted total: 0.0006903087\n",
      "Epoch 410. Current alpha: 0.056428, lr: 0.0000065053. Training losses: pinn: 8.191495908249635e-06, boundary: 0.011923, weighted total: 0.0006804866\n",
      "Epoch 411. Current alpha: 0.056033, lr: 0.0000065053. Training losses: pinn: 6.994207979005296e-06, boundary: 0.011977, weighted total: 0.0006776860\n",
      "Epoch 412. Current alpha: 0.055642, lr: 0.0000065053. Training losses: pinn: 8.252592124335933e-06, boundary: 0.011968, weighted total: 0.0006736927\n",
      "Epoch 413. Current alpha: 0.055253, lr: 0.0000065053. Training losses: pinn: 6.961505278013647e-06, boundary: 0.011965, weighted total: 0.0006676807\n",
      "Epoch 414. Current alpha: 0.054867, lr: 0.0000065053. Training losses: pinn: 8.214115041482728e-06, boundary: 0.011956, weighted total: 0.0006637486\n",
      "Epoch 415. Current alpha: 0.054484, lr: 0.0000065053. Training losses: pinn: 6.928994935151422e-06, boundary: 0.011953, weighted total: 0.0006578154\n",
      "Epoch 416. Current alpha: 0.054103, lr: 0.0000065053. Training losses: pinn: 8.271101251011714e-06, boundary: 0.011943, weighted total: 0.0006539918\n",
      "Epoch 417. Current alpha: 0.053725, lr: 0.0000065053. Training losses: pinn: 6.895114438520977e-06, boundary: 0.011942, weighted total: 0.0006481266\n",
      "Epoch 418. Current alpha: 0.053349, lr: 0.0000065053. Training losses: pinn: 8.231847459683195e-06, boundary: 0.011932, weighted total: 0.0006443396\n",
      "Epoch 419. Current alpha: 0.052977, lr: 0.0000065053. Training losses: pinn: 6.862368081783643e-06, boundary: 0.011931, weighted total: 0.0006385675\n",
      "Epoch 420. Current alpha: 0.052607, lr: 0.0000065053. Training losses: pinn: 8.188278115994763e-06, boundary: 0.011918, weighted total: 0.0006347455\n",
      "Epoch 421. Current alpha: 0.052239, lr: 0.0000065053. Training losses: pinn: 6.826423486927524e-06, boundary: 0.011920, weighted total: 0.0006291493\n",
      "Epoch 422. Current alpha: 0.051874, lr: 0.0000065053. Training losses: pinn: 8.149256245815195e-06, boundary: 0.011907, weighted total: 0.0006253660\n",
      "Epoch 423. Current alpha: 0.051511, lr: 0.0000065053. Training losses: pinn: 6.794064574933145e-06, boundary: 0.011909, weighted total: 0.0006198796\n",
      "Epoch 424. Current alpha: 0.051152, lr: 0.0000065053. Training losses: pinn: 8.014687409740873e-06, boundary: 0.011905, weighted total: 0.0006165710\n",
      "Epoch 425. Current alpha: 0.050794, lr: 0.0000065053. Training losses: pinn: 6.924916760908673e-06, boundary: 0.011850, weighted total: 0.0006084904\n",
      "Epoch 426. Current alpha: 0.050439, lr: 0.0000065053. Training losses: pinn: 8.266995791927911e-06, boundary: 0.011942, weighted total: 0.0006101803\n",
      "Epoch 427. Current alpha: 0.050087, lr: 0.0000065053. Training losses: pinn: 6.890665645187255e-06, boundary: 0.011839, weighted total: 0.0005995145\n",
      "Epoch 428. Current alpha: 0.049737, lr: 0.0000065053. Training losses: pinn: 8.227142643590923e-06, boundary: 0.011930, weighted total: 0.0006011784\n",
      "Epoch 429. Current alpha: 0.049389, lr: 0.0000065053. Training losses: pinn: 6.857866537757218e-06, boundary: 0.011828, weighted total: 0.0005906727\n",
      "Epoch 430. Current alpha: 0.049044, lr: 0.0000065053. Training losses: pinn: 8.187660569092259e-06, boundary: 0.011918, weighted total: 0.0005923019\n",
      "Epoch 431. Current alpha: 0.048702, lr: 0.0000065053. Training losses: pinn: 6.825189302617218e-06, boundary: 0.011817, weighted total: 0.0005819834\n",
      "Epoch 432. Current alpha: 0.048361, lr: 0.0000065053. Training losses: pinn: 8.145403626258485e-06, boundary: 0.011905, weighted total: 0.0005835064\n",
      "Epoch 433. Current alpha: 0.048023, lr: 0.0000065053. Training losses: pinn: 6.790079169149976e-06, boundary: 0.011806, weighted total: 0.0005734046\n",
      "Epoch 434. Current alpha: 0.047688, lr: 0.0000065053. Training losses: pinn: 8.105975211947225e-06, boundary: 0.011893, weighted total: 0.0005748913\n",
      "Epoch 435. Current alpha: 0.047355, lr: 0.0000065053. Training losses: pinn: 6.757515166100347e-06, boundary: 0.011794, weighted total: 0.0005649474\n",
      "Epoch 436. Current alpha: 0.047024, lr: 0.0000065053. Training losses: pinn: 8.066937880357727e-06, boundary: 0.011887, weighted total: 0.0005666418\n",
      "Epoch 437. Current alpha: 0.046695, lr: 0.0000065053. Training losses: pinn: 6.8866261244693305e-06, boundary: 0.011737, weighted total: 0.0005546085\n",
      "Epoch 438. Current alpha: 0.046369, lr: 0.0000065053. Training losses: pinn: 8.122891813400201e-06, boundary: 0.011930, weighted total: 0.0005609124\n",
      "Epoch 439. Current alpha: 0.046045, lr: 0.0000065053. Training losses: pinn: 6.852458227513125e-06, boundary: 0.011725, weighted total: 0.0005464210\n",
      "Epoch 440. Current alpha: 0.045723, lr: 0.0000065053. Training losses: pinn: 8.082468411885202e-06, boundary: 0.011917, weighted total: 0.0005525803\n",
      "Epoch 441. Current alpha: 0.045404, lr: 0.0000065053. Training losses: pinn: 6.81830579196685e-06, boundary: 0.011714, weighted total: 0.0005383531\n",
      "Epoch 442. Current alpha: 0.045086, lr: 0.0000065053. Training losses: pinn: 8.043374691624194e-06, boundary: 0.011905, weighted total: 0.0005444197\n",
      "Epoch 443. Current alpha: 0.044771, lr: 0.0000065053. Training losses: pinn: 6.785538971598726e-06, boundary: 0.011702, weighted total: 0.0005303904\n",
      "Epoch 444. Current alpha: 0.044459, lr: 0.0000065053. Training losses: pinn: 8.09927769296337e-06, boundary: 0.011893, weighted total: 0.0005364776\n",
      "Epoch 445. Current alpha: 0.044148, lr: 0.0000065053. Training losses: pinn: 6.752291483280715e-06, boundary: 0.011691, weighted total: 0.0005225770\n",
      "Epoch 446. Current alpha: 0.043840, lr: 0.0000065053. Training losses: pinn: 8.056644219323061e-06, boundary: 0.011880, weighted total: 0.0005285080\n",
      "Epoch 447. Current alpha: 0.043533, lr: 0.0000065053. Training losses: pinn: 6.716955795127433e-06, boundary: 0.011680, weighted total: 0.0005148737\n",
      "Epoch 448. Current alpha: 0.043229, lr: 0.0000065053. Training losses: pinn: 8.017007530725095e-06, boundary: 0.011868, weighted total: 0.0005207289\n",
      "Epoch 449. Current alpha: 0.042927, lr: 0.0000065053. Training losses: pinn: 6.843848041171441e-06, boundary: 0.011622, weighted total: 0.0005054593\n",
      "Epoch 450. Current alpha: 0.042627, lr: 0.0000065053. Training losses: pinn: 8.071605407167226e-06, boundary: 0.011916, weighted total: 0.0005156595\n",
      "Epoch 451. Current alpha: 0.042329, lr: 0.0000065053. Training losses: pinn: 6.8093759182374924e-06, boundary: 0.011611, weighted total: 0.0004979954\n",
      "Epoch 452. Current alpha: 0.042033, lr: 0.0000065053. Training losses: pinn: 8.032051482587121e-06, boundary: 0.011904, weighted total: 0.0005080471\n",
      "Epoch 453. Current alpha: 0.041740, lr: 0.0000065053. Training losses: pinn: 6.775982001272496e-06, boundary: 0.011599, weighted total: 0.0004906437\n",
      "Epoch 454. Current alpha: 0.041448, lr: 0.0000065053. Training losses: pinn: 8.076616722973995e-06, boundary: 0.011811, weighted total: 0.0004972855\n",
      "Epoch 455. Current alpha: 0.041158, lr: 0.0000065053. Training losses: pinn: 6.8136932895868085e-06, boundary: 0.011658, weighted total: 0.0004863474\n",
      "Epoch 456. Current alpha: 0.040871, lr: 0.0000065053. Training losses: pinn: 8.032530786294956e-06, boundary: 0.011798, weighted total: 0.0004898845\n",
      "Epoch 457. Current alpha: 0.040585, lr: 0.0000065053. Training losses: pinn: 6.776402187824715e-06, boundary: 0.011645, weighted total: 0.0004791369\n",
      "Epoch 458. Current alpha: 0.040302, lr: 0.0000065053. Training losses: pinn: 7.988358447619248e-06, boundary: 0.011784, weighted total: 0.0004825831\n",
      "Epoch 459. Current alpha: 0.040020, lr: 0.0000065053. Training losses: pinn: 6.7393184508546256e-06, boundary: 0.011634, weighted total: 0.0004720536\n",
      "Epoch 460. Current alpha: 0.039741, lr: 0.0000061801. Training losses: pinn: 8.039428394113202e-06, boundary: 0.011771, weighted total: 0.0004755188\n",
      "Epoch 461. Current alpha: 0.039463, lr: 0.0000061801. Training losses: pinn: 6.760084943380207e-06, boundary: 0.011526, weighted total: 0.0004613595\n",
      "Epoch 462. Current alpha: 0.039187, lr: 0.0000061801. Training losses: pinn: 7.997216016519815e-06, boundary: 0.011758, weighted total: 0.0004684564\n",
      "Epoch 463. Current alpha: 0.038913, lr: 0.0000061801. Training losses: pinn: 6.724579634465044e-06, boundary: 0.011516, weighted total: 0.0004545758\n",
      "Epoch 464. Current alpha: 0.038641, lr: 0.0000061801. Training losses: pinn: 7.862161510274746e-06, boundary: 0.011753, weighted total: 0.0004616972\n",
      "Epoch 465. Current alpha: 0.038371, lr: 0.0000061801. Training losses: pinn: 6.844076324341586e-06, boundary: 0.011458, weighted total: 0.0004462515\n",
      "Epoch 466. Current alpha: 0.038103, lr: 0.0000061801. Training losses: pinn: 8.102090760075953e-06, boundary: 0.011793, weighted total: 0.0004571618\n",
      "Epoch 467. Current alpha: 0.037837, lr: 0.0000061801. Training losses: pinn: 6.811163075326476e-06, boundary: 0.011448, weighted total: 0.0004396946\n",
      "Epoch 468. Current alpha: 0.037573, lr: 0.0000061801. Training losses: pinn: 8.060974323598202e-06, boundary: 0.011781, weighted total: 0.0004503979\n",
      "Epoch 469. Current alpha: 0.037310, lr: 0.0000061801. Training losses: pinn: 6.777203907404328e-06, boundary: 0.011437, weighted total: 0.0004332433\n",
      "Epoch 470. Current alpha: 0.037049, lr: 0.0000061801. Training losses: pinn: 8.021827852644492e-06, boundary: 0.011769, weighted total: 0.0004437674\n",
      "Epoch 471. Current alpha: 0.036791, lr: 0.0000061801. Training losses: pinn: 6.744523943780223e-06, boundary: 0.011430, weighted total: 0.0004270148\n",
      "Epoch 472. Current alpha: 0.036534, lr: 0.0000061801. Training losses: pinn: 8.062495908234268e-06, boundary: 0.011687, weighted total: 0.0004347228\n",
      "Epoch 473. Current alpha: 0.036278, lr: 0.0000061801. Training losses: pinn: 6.77884145261487e-06, boundary: 0.011477, weighted total: 0.0004229068\n",
      "Epoch 474. Current alpha: 0.036025, lr: 0.0000061801. Training losses: pinn: 8.01906662672991e-06, boundary: 0.011674, weighted total: 0.0004282900\n",
      "Epoch 475. Current alpha: 0.035773, lr: 0.0000061801. Training losses: pinn: 6.742736786691239e-06, boundary: 0.011467, weighted total: 0.0004167026\n",
      "Epoch 476. Current alpha: 0.035523, lr: 0.0000058711. Training losses: pinn: 7.88670513429679e-06, boundary: 0.011662, weighted total: 0.0004218791\n",
      "Epoch 477. Current alpha: 0.035275, lr: 0.0000058711. Training losses: pinn: 6.765259513485944e-06, boundary: 0.011365, weighted total: 0.0004074307\n",
      "Epoch 478. Current alpha: 0.035028, lr: 0.0000058711. Training losses: pinn: 7.940443538245745e-06, boundary: 0.011650, weighted total: 0.0004157533\n",
      "Epoch 479. Current alpha: 0.034784, lr: 0.0000058711. Training losses: pinn: 6.730866971338401e-06, boundary: 0.011355, weighted total: 0.0004014795\n",
      "Epoch 480. Current alpha: 0.034541, lr: 0.0000058711. Training losses: pinn: 7.811279829184059e-06, boundary: 0.011648, weighted total: 0.0004098649\n",
      "Epoch 481. Current alpha: 0.034299, lr: 0.0000058711. Training losses: pinn: 6.845738880656427e-06, boundary: 0.011304, weighted total: 0.0003943386\n",
      "Epoch 482. Current alpha: 0.034060, lr: 0.0000058711. Training losses: pinn: 8.035223800106905e-06, boundary: 0.011681, weighted total: 0.0004055992\n",
      "Epoch 483. Current alpha: 0.033822, lr: 0.0000058711. Training losses: pinn: 6.810820195823908e-06, boundary: 0.011294, weighted total: 0.0003885675\n",
      "Epoch 484. Current alpha: 0.033585, lr: 0.0000058711. Training losses: pinn: 7.995340638444759e-06, boundary: 0.011669, weighted total: 0.0003996208\n",
      "Epoch 485. Current alpha: 0.033351, lr: 0.0000058711. Training losses: pinn: 6.776986992917955e-06, boundary: 0.011284, weighted total: 0.0003828822\n",
      "Epoch 486. Current alpha: 0.033118, lr: 0.0000058711. Training losses: pinn: 7.955898581712972e-06, boundary: 0.011657, weighted total: 0.0003937396\n",
      "Epoch 487. Current alpha: 0.032886, lr: 0.0000058711. Training losses: pinn: 6.744422080373624e-06, boundary: 0.011278, weighted total: 0.0003774160\n",
      "Epoch 488. Current alpha: 0.032657, lr: 0.0000058711. Training losses: pinn: 7.991418897290714e-06, boundary: 0.011577, weighted total: 0.0003857958\n",
      "Epoch 489. Current alpha: 0.032428, lr: 0.0000058711. Training losses: pinn: 6.774739631509874e-06, boundary: 0.011324, weighted total: 0.0003737798\n",
      "Epoch 490. Current alpha: 0.032202, lr: 0.0000058711. Training losses: pinn: 7.859360266593285e-06, boundary: 0.011566, weighted total: 0.0003800554\n",
      "Epoch 491. Current alpha: 0.031977, lr: 0.0000058711. Training losses: pinn: 6.741503057128284e-06, boundary: 0.011314, weighted total: 0.0003683004\n",
      "Epoch 492. Current alpha: 0.031753, lr: 0.0000055775. Training losses: pinn: 7.914188245194964e-06, boundary: 0.011555, weighted total: 0.0003745623\n",
      "Epoch 493. Current alpha: 0.031531, lr: 0.0000055775. Training losses: pinn: 6.760788437532028e-06, boundary: 0.011218, weighted total: 0.0003602560\n",
      "Epoch 494. Current alpha: 0.031311, lr: 0.0000055775. Training losses: pinn: 7.876997187850066e-06, boundary: 0.011550, weighted total: 0.0003692651\n",
      "Epoch 495. Current alpha: 0.031092, lr: 0.0000055775. Training losses: pinn: 6.864829629193991e-06, boundary: 0.011169, weighted total: 0.0003539204\n",
      "Epoch 496. Current alpha: 0.030875, lr: 0.0000055775. Training losses: pinn: 7.903502591943834e-06, boundary: 0.011583, weighted total: 0.0003652810\n",
      "Epoch 497. Current alpha: 0.030659, lr: 0.0000055775. Training losses: pinn: 6.831812243035529e-06, boundary: 0.011159, weighted total: 0.0003487583\n",
      "Epoch 498. Current alpha: 0.030445, lr: 0.0000055775. Training losses: pinn: 7.867962267482653e-06, boundary: 0.011572, weighted total: 0.0003599362\n",
      "Epoch 499. Current alpha: 0.030232, lr: 0.0000055775. Training losses: pinn: 6.801488780183718e-06, boundary: 0.011150, weighted total: 0.0003436776\n"
     ]
    }
   ],
   "source": [
    "# Train the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], f_boundary=f_boundary[:, 0], f_bound=f_bound, size=size, n_samples=n_samples)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_predict, client=None, trial=None, alpha=alpha, batchsize=batchsize, \n",
    "                                                 boundary_batchsize=boundary_batchsize, epochs=epochs, lr=lr, size=size, save=save, load_epoch=load_epoch, \n",
    "                                                 lr_decay=lr_decay, alpha_decay=alpha_decay, alpha_limit=alpha_limit, patience=patience, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1efedbc-5843-44e8-981d-b48ca8bf011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open(OUTPUTS_PATH + '/pinn_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open(OUTPUTS_PATH + '/boundary_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "     \n",
    "with open(OUTPUTS_PATH + '/predictions_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b77f7-94b5-4e71-a474-83faf92d8618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2381ed-7107-41f2-be53-030f601d79cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
