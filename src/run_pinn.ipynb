{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d7e6e69-8377-4874-aee5-22f99dbf95b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "# from pinn import PINN\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfm = tf.math\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c8fd5e8-fb2d-4ec5-a104-5f606294015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"data\"))\n",
    "OUTPUTS_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"outputs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc4ede82-5306-4443-be72-353fbda27c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(DATA_PATH + '/f_boundary.pkl', 'rb') as file:\n",
    "    f_boundary = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/p.pkl', 'rb') as file:\n",
    "    p = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/T.pkl', 'rb') as file:\n",
    "    T = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/r_119au.pkl', 'rb') as file:\n",
    "    r = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/P_predict.pkl', 'rb') as file:\n",
    "    P_predict = pkl.load(file)\n",
    "    \n",
    "# Get upper and lower bounds\n",
    "lb = np.log(np.array([p[0], r[0]], dtype='float32'))\n",
    "ub = np.log(np.array([p[-1], r[-1]], dtype='float32'))\n",
    "f_bound = np.array([-34.54346331847909, 6.466899920699378], dtype='float32')\n",
    "size = len(f_boundary[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7dab597-dc24-4b1c-a440-00e78daca9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, f_boundary, f_bound, size, n_samples=20000):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.size = size\n",
    "        self.f_bound = f_bound\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and f_boundary.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                t1.watch(p)\n",
    "                t1.watch(r)\n",
    "                t1.watch(p_boundary)\n",
    "                t1.watch(r_boundary)\n",
    "                \n",
    "                # PINN loss\n",
    "                p_scaled = (tfm.log(p) - self.lower_bound[0])/tfm.abs(self.upper_bound[0] - self.lower_bound[0])\n",
    "                r_scaled = (tfm.log(r) - self.lower_bound[1])/tfm.abs(self.upper_bound[1] - self.lower_bound[1])\n",
    "                \n",
    "                P = tf.concat((p_scaled, r_scaled), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Boundary loss\n",
    "                p_boundary_scaled = (tfm.log(p_boundary) - self.lower_bound[0])/tfm.abs(self.upper_bound[0] - self.lower_bound[0])\n",
    "                r_boundary_scaled = (tfm.log(r_boundary) - self.lower_bound[1])/tfm.abs(self.upper_bound[1] - self.lower_bound[1])\n",
    "                \n",
    "                P_boundary = tf.concat((p_boundary_scaled, r_boundary_scaled), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "                \n",
    "                boundary_loss = tfm.reduce_mean(tfm.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            g_p = t1.gradient(f, p)\n",
    "            g_r = t1.gradient(f, r)\n",
    "            \n",
    "            g_p_boundary = t1.gradient(f_pred_boundary, p_boundary)\n",
    "            g_r_boundary = t1.gradient(f_pred_boundary, r_boundary)\n",
    "            \n",
    "            # Calculate f_p and f_r using chain rule\n",
    "            diff = tfm.abs(self.f_bound[1] - self.f_bound[0])\n",
    "            f_g = diff*tfm.exp(diff*f + self.f_bound[0])\n",
    "            f_g_boundary = diff*tfm.exp(diff*f_pred_boundary + self.f_bound[0])\n",
    "            \n",
    "            f_p = f_g*g_p\n",
    "            f_r = f_g*g_r\n",
    "            \n",
    "            f_p_boundary = f_g_boundary*g_p_boundary\n",
    "            f_r_boundary = f_g_boundary*g_r_boundary\n",
    "            \n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r) + self.pinn_loss(p_boundary, r_boundary, f_p_boundary, f_r_boundary)\n",
    "            total_loss = (1-alpha)*pinn_loss + alpha*boundary_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        client, trial: Sherpa client and trial\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_decay: If -1, learning rate will not be decayed. Otherwise, lr = lr_decay*lr if loss hasn't \n",
    "        decreased\n",
    "        \n",
    "        alpha_decay: If -1, alpha will not be changed. Otherwise, alpha = alpha_decay*alpha if loss \n",
    "        hasn't decreased\n",
    "        \n",
    "        alpha_limit = Minimum alpha value to decay to\n",
    "        \n",
    "        patience: Number of epochs to check whether loss has decreased before updating lr or alpha\n",
    "        \n",
    "        filename: Name for the checkpoint file\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, client=None, trial=None, alpha=0.5, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, size=256, \n",
    "            save=False, load_epoch=-1, lr_decay=-1, alpha_decay=-1, alpha_limit = 0.5, patience=3, filename=''):\n",
    "        \n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(load_epoch)\n",
    "            self.load_weights(name)\n",
    "        \n",
    "        # Initialize\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs,))\n",
    "        total_boundary_loss = np.zeros((epochs,))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, sample data and pass to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Sample p and r according to a uniform distribution between upper and lower bounds\n",
    "                dist = tfd.Uniform(0, 1)\n",
    "\n",
    "                p = (dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[0] - self.lower_bound[0])) + self.lower_bound[0]\n",
    "                r = (dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[1] - self.lower_bound[1])) + self.lower_bound[1]\n",
    "                \n",
    "                p = tfm.exp(p)\n",
    "                r = tfm.exp(r)\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = tf.Variable(self.p[p_idx], dtype=tf.float32)\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_boundary = np.zeros((boundary_batchsize, 1))\n",
    "                upper_boundary[:] = tfm.exp(self.upper_bound[1])\n",
    "                r_boundary = tf.Variable(upper_boundary, dtype=tf.float32)\n",
    "                \n",
    "                # Train and get loss\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Sum losses\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Epoch {epoch}. Current alpha: {alpha:.6f}, lr: {lr:.10f}. Training losses: pinn: {total_pinn_loss[epoch]}, ' +\n",
    "                  f'boundary: {total_boundary_loss[epoch]:.6f}, weighted total: {((alpha*total_boundary_loss[epoch])+((1-alpha)*total_pinn_loss[epoch])):.50f}')\n",
    "            \n",
    "            predictions[:, :, epoch] = self.predict(P_predict, batchsize)\n",
    "            \n",
    "            # Decay lr if loss hasn't decreased since current epoch - patience\n",
    "            if (epoch > patience):\n",
    "                hasntDecreased = False\n",
    "                if (total_pinn_loss[epoch] + total_boundary_loss[epoch]) > (total_pinn_loss[epoch-patience] + total_boundary_loss[epoch-patience]):\n",
    "                    hasntDecreased = True\n",
    "                        \n",
    "                if (lr_decay != -1) & hasntDecreased:\n",
    "                    lr = lr_decay*lr\n",
    "\n",
    "            # Decrease alpha each epoch\n",
    "            if (alpha_decay != -1) & (alpha >= alpha_limit):\n",
    "                alpha = alpha_decay*alpha\n",
    "\n",
    "            # If the epoch is a multiple of 100, save to a checkpoint\n",
    "            if (epoch%100 == 0) & (save == True):\n",
    "                name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(epoch)\n",
    "                self.save_weights(name, overwrite=True, save_format=None, options=None)\n",
    "                \n",
    "            # Send metrics\n",
    "            if client:\n",
    "                if (np.isnan(total_pinn_loss[epoch]) and np.isnan(total_boundary_loss[epoch])):\n",
    "                    obj = np.inf\n",
    "                else:\n",
    "                    obj = total_pinn_loss[epoch] + total_boundary_loss[epoch]\n",
    "                client.send_metrics(\n",
    "                         trial=trial,\n",
    "                         iteration=epoch,\n",
    "                         objective=obj)\n",
    "        \n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, batchsize):\n",
    "        P_size = P.shape[0]\n",
    "        steps_per_epoch = np.ceil(P_size / batchsize).astype(int)\n",
    "        predictions = np.zeros((P_size, 1))\n",
    "        \n",
    "        # For each step predict on data between start and end indices\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # Calculate end_idx\n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P_size - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Predict\n",
    "            predictions[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r):\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tfm.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tfm.divide(p, tfm.sqrt(tfm.square(p) + tfm.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tfm.reduce_mean(tfm.abs(f_r + (tfm.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "442d0c7b-4143-45df-95fc-a534727e3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 100\n",
    "alpha = 1\n",
    "alpha_decay = 0.99\n",
    "alpha_limit = 0\n",
    "lr_decay = 0.95\n",
    "patience = 10\n",
    "batchsize = 1032\n",
    "boundary_batchsize = 512\n",
    "activation = 'selu'\n",
    "save = False\n",
    "load_epoch = -1\n",
    "filename = 'boundaryDataInPinnLoss'\n",
    "n_samples = 20000\n",
    "lr = 3e-6\n",
    "num_layers = 10\n",
    "num_hidden_units = 50\n",
    "\n",
    "# Create model\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(inputs)\n",
    "for _ in range(num_layers-1):\n",
    "    x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64a9cf11-b7a4-4914-866b-6fa66a51ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Current alpha: 1.000000, lr: 0.0000030000. Training losses: pinn: 96.49177277088165, boundary: 27.802813, weighted total: 27.80281317234039306640625000000000000000000000000000\n",
      "Epoch 1. Current alpha: 0.990000, lr: 0.0000030000. Training losses: pinn: 39.616055101156235, boundary: 25.902817, weighted total: 26.03994927376508528027443389873951673507690429687500\n",
      "Epoch 2. Current alpha: 0.980100, lr: 0.0000030000. Training losses: pinn: 5.260436214506626, boundary: 23.997134, weighted total: 23.62427333441004151382003328762948513031005859375000\n",
      "Epoch 3. Current alpha: 0.970299, lr: 0.0000030000. Training losses: pinn: 2.5201121643185616, boundary: 21.825288, weighted total: 21.25190502812901627294195350259542465209960937500000\n",
      "Epoch 4. Current alpha: 0.960596, lr: 0.0000030000. Training losses: pinn: 1.4548051990568638, boundary: 19.816794, weighted total: 19.09325847049220215012610424309968948364257812500000\n",
      "Epoch 5. Current alpha: 0.950990, lr: 0.0000030000. Training losses: pinn: 1.1533994227647781, boundary: 17.949375, weighted total: 17.12620505013817506778650567866861820220947265625000\n",
      "Epoch 6. Current alpha: 0.941480, lr: 0.0000030000. Training losses: pinn: 1.0977470129728317, boundary: 16.253495, weighted total: 15.36658281520783475571079179644584655761718750000000\n",
      "Epoch 7. Current alpha: 0.932065, lr: 0.0000030000. Training losses: pinn: 0.9691715054214001, boundary: 14.763187, weighted total: 13.82609568155433166225520835723727941513061523437500\n",
      "Epoch 8. Current alpha: 0.922745, lr: 0.0000030000. Training losses: pinn: 0.6571264248341322, boundary: 13.431164, weighted total: 12.44430195797269078639146755449473857879638671875000\n",
      "Epoch 9. Current alpha: 0.913517, lr: 0.0000030000. Training losses: pinn: 0.4121333910152316, boundary: 12.211294, weighted total: 11.19087038118166788080998230725526809692382812500000\n",
      "Epoch 10. Current alpha: 0.904382, lr: 0.0000030000. Training losses: pinn: 0.2538448250852525, boundary: 11.092371, weighted total: 10.05601387490614406772237998666241765022277832031250\n",
      "Epoch 11. Current alpha: 0.895338, lr: 0.0000030000. Training losses: pinn: 0.12728745071217418, boundary: 10.099824, weighted total: 9.05608119225482610659128113184124231338500976562500\n",
      "Epoch 12. Current alpha: 0.886385, lr: 0.0000030000. Training losses: pinn: 0.04804615827742964, boundary: 9.219098, weighted total: 8.17712771719527431457663624314591288566589355468750\n",
      "Epoch 13. Current alpha: 0.877521, lr: 0.0000030000. Training losses: pinn: 0.01481805497314781, boundary: 8.418119, weighted total: 7.38889138695714553506377342273481190204620361328125\n",
      "Epoch 14. Current alpha: 0.868746, lr: 0.0000030000. Training losses: pinn: 0.0030627183987235185, boundary: 7.683523, weighted total: 6.67543063468370334589963022153824567794799804687500\n",
      "Epoch 15. Current alpha: 0.860058, lr: 0.0000030000. Training losses: pinn: 0.0005675081010849681, boundary: 7.010704, weighted total: 6.02969399999039978865766897797584533691406250000000\n",
      "Epoch 16. Current alpha: 0.851458, lr: 0.0000030000. Training losses: pinn: 9.894933668874728e-05, boundary: 6.378011, weighted total: 5.43062199734622907953962567262351512908935546875000\n",
      "Epoch 17. Current alpha: 0.842943, lr: 0.0000030000. Training losses: pinn: 1.6462533153571712e-05, boundary: 5.773742, weighted total: 4.86693927250635294257108398596756160259246826171875\n",
      "Epoch 18. Current alpha: 0.834514, lr: 0.0000030000. Training losses: pinn: 2.1022288692051916e-06, boundary: 5.195265, weighted total: 4.33552072938372656807359817321412265300750732421875\n",
      "Epoch 19. Current alpha: 0.826169, lr: 0.0000030000. Training losses: pinn: 3.3915585406774085e-07, boundary: 4.640212, weighted total: 3.83359759619736761848685091536026448011398315429688\n",
      "Epoch 20. Current alpha: 0.817907, lr: 0.0000030000. Training losses: pinn: 1.1997981008704528e-07, boundary: 4.112257, weighted total: 3.36344349348655713427547198079992085695266723632812\n",
      "Epoch 21. Current alpha: 0.809728, lr: 0.0000030000. Training losses: pinn: 2.1874087829942823e-07, boundary: 3.596108, weighted total: 2.91186880136467207691453040752094238996505737304688\n",
      "Epoch 22. Current alpha: 0.801631, lr: 0.0000030000. Training losses: pinn: 6.618229075883164e-07, boundary: 3.079825, weighted total: 2.46888243119270534009501716354861855506896972656250\n",
      "Epoch 23. Current alpha: 0.793614, lr: 0.0000030000. Training losses: pinn: 2.5360058444334754e-06, boundary: 2.569441, weighted total: 2.03914558275785307017713421373628079891204833984375\n",
      "Epoch 24. Current alpha: 0.785678, lr: 0.0000030000. Training losses: pinn: 7.074925662209353e-06, boundary: 2.091911, weighted total: 1.64357042192885893783227402309421449899673461914062\n",
      "Epoch 25. Current alpha: 0.777821, lr: 0.0000030000. Training losses: pinn: 1.1300736389330268e-05, boundary: 1.705680, weighted total: 1.32671669469662423246347771055297926068305969238281\n",
      "Epoch 26. Current alpha: 0.770043, lr: 0.0000030000. Training losses: pinn: 1.2890823143152375e-05, boundary: 1.409118, weighted total: 1.08508486028228801067996300844242796301841735839844\n",
      "Epoch 27. Current alpha: 0.762343, lr: 0.0000030000. Training losses: pinn: 1.2709614452433016e-05, boundary: 1.189478, weighted total: 0.90679328163898986758795217610895633697509765625000\n",
      "Epoch 28. Current alpha: 0.754719, lr: 0.0000030000. Training losses: pinn: 1.24840892112843e-05, boundary: 1.036153, weighted total: 0.78200797143649247722407835681224241852760314941406\n",
      "Epoch 29. Current alpha: 0.747172, lr: 0.0000030000. Training losses: pinn: 1.1928577350772684e-05, boundary: 0.925521, weighted total: 0.69152650575736174243246523474226705729961395263672\n",
      "Epoch 30. Current alpha: 0.739700, lr: 0.0000030000. Training losses: pinn: 1.1317058806525893e-05, boundary: 0.810368, weighted total: 0.59943275118227345998889177280943840742111206054688\n",
      "Epoch 31. Current alpha: 0.732303, lr: 0.0000030000. Training losses: pinn: 1.196360648236805e-05, boundary: 0.720341, weighted total: 0.52751111830679642267227791307959705591201782226562\n",
      "Epoch 32. Current alpha: 0.724980, lr: 0.0000030000. Training losses: pinn: 1.2943610670390626e-05, boundary: 0.655436, weighted total: 0.47518209926410565069332392340584192425012588500977\n",
      "Epoch 33. Current alpha: 0.717731, lr: 0.0000030000. Training losses: pinn: 1.3256412273676688e-05, boundary: 0.598647, weighted total: 0.42967068211884335005024126985517796128988265991211\n",
      "Epoch 34. Current alpha: 0.710553, lr: 0.0000030000. Training losses: pinn: 1.3024868110278476e-05, boundary: 0.537450, weighted total: 0.38189064084268481424899732701305765658617019653320\n",
      "Epoch 35. Current alpha: 0.703448, lr: 0.0000030000. Training losses: pinn: 1.2433802794475923e-05, boundary: 0.477320, weighted total: 0.33577359823599389754278377040463965386152267456055\n",
      "Epoch 36. Current alpha: 0.696413, lr: 0.0000030000. Training losses: pinn: 1.2715293280507467e-05, boundary: 0.430436, weighted total: 0.29976550067925045173211628934950567781925201416016\n",
      "Epoch 37. Current alpha: 0.689449, lr: 0.0000030000. Training losses: pinn: 1.326478900409711e-05, boundary: 0.397560, weighted total: 0.27410169731061012621253780707775149494409561157227\n",
      "Epoch 38. Current alpha: 0.682555, lr: 0.0000030000. Training losses: pinn: 1.3946905198736204e-05, boundary: 0.361518, weighted total: 0.24676000066493855578109162252076203003525733947754\n",
      "Epoch 39. Current alpha: 0.675729, lr: 0.0000030000. Training losses: pinn: 1.4300271971023903e-05, boundary: 0.330991, weighted total: 0.22366469458768983846752576027938630431890487670898\n",
      "Epoch 40. Current alpha: 0.668972, lr: 0.0000030000. Training losses: pinn: 1.4695478284920682e-05, boundary: 0.302052, weighted total: 0.20206933941132301790766234717011684551835060119629\n",
      "Epoch 41. Current alpha: 0.662282, lr: 0.0000030000. Training losses: pinn: 1.5297653419565904e-05, boundary: 0.274074, weighted total: 0.18151913756159349100371969143452588468790054321289\n",
      "Epoch 42. Current alpha: 0.655659, lr: 0.0000030000. Training losses: pinn: 1.612517684179693e-05, boundary: 0.248194, weighted total: 0.16273645497979466467697307052731048315763473510742\n",
      "Epoch 43. Current alpha: 0.649103, lr: 0.0000030000. Training losses: pinn: 1.6594714907114394e-05, boundary: 0.224347, weighted total: 0.14562975661647259428832512639928609132766723632812\n",
      "Epoch 44. Current alpha: 0.642612, lr: 0.0000030000. Training losses: pinn: 1.7089373272938246e-05, boundary: 0.203919, weighted total: 0.13104705922339121570985298603773117065429687500000\n",
      "Epoch 45. Current alpha: 0.636185, lr: 0.0000030000. Training losses: pinn: 1.768189309814261e-05, boundary: 0.188456, weighted total: 0.11989935442014672262178720529846032150089740753174\n",
      "Epoch 46. Current alpha: 0.629824, lr: 0.0000030000. Training losses: pinn: 1.8008620429554867e-05, boundary: 0.177232, weighted total: 0.11163171337559127382910162396001396700739860534668\n",
      "Epoch 47. Current alpha: 0.623525, lr: 0.0000030000. Training losses: pinn: 1.7579247753474192e-05, boundary: 0.160011, weighted total: 0.09977726310679202459397174607147462666034698486328\n",
      "Epoch 48. Current alpha: 0.617290, lr: 0.0000030000. Training losses: pinn: 1.758180462729797e-05, boundary: 0.148318, weighted total: 0.09156170578662843617667732587506179697811603546143\n",
      "Epoch 49. Current alpha: 0.611117, lr: 0.0000030000. Training losses: pinn: 1.770807301681998e-05, boundary: 0.138224, weighted total: 0.08447775028730952762145989254349842667579650878906\n",
      "Epoch 50. Current alpha: 0.605006, lr: 0.0000030000. Training losses: pinn: 1.777119797452542e-05, boundary: 0.130585, weighted total: 0.07901184295491146813983363017541705630719661712646\n",
      "Epoch 51. Current alpha: 0.598956, lr: 0.0000030000. Training losses: pinn: 1.7648233665568114e-05, boundary: 0.121899, weighted total: 0.07301902595430252718156083346912055276334285736084\n",
      "Epoch 52. Current alpha: 0.592966, lr: 0.0000030000. Training losses: pinn: 1.7613729085041996e-05, boundary: 0.113695, weighted total: 0.06742469897936580192077116180371376685798168182373\n",
      "Epoch 53. Current alpha: 0.587037, lr: 0.0000030000. Training losses: pinn: 1.731710011654286e-05, boundary: 0.107316, weighted total: 0.06300545580303058834648055608340655453503131866455\n",
      "Epoch 54. Current alpha: 0.581166, lr: 0.0000030000. Training losses: pinn: 1.7524579845940025e-05, boundary: 0.101985, weighted total: 0.05927737502613947967455843013340199831873178482056\n",
      "Epoch 55. Current alpha: 0.575355, lr: 0.0000030000. Training losses: pinn: 1.7457639785334322e-05, boundary: 0.097473, weighted total: 0.05608907572541543123945118054507474880665540695190\n",
      "Epoch 56. Current alpha: 0.569601, lr: 0.0000030000. Training losses: pinn: 1.7264638984215708e-05, boundary: 0.092891, weighted total: 0.05291841179608771511544418331141059752553701400757\n",
      "Epoch 57. Current alpha: 0.563905, lr: 0.0000030000. Training losses: pinn: 1.715716200578754e-05, boundary: 0.089327, weighted total: 0.05037920967077597067529382002248894423246383666992\n",
      "Epoch 58. Current alpha: 0.558266, lr: 0.0000030000. Training losses: pinn: 1.7020602911088645e-05, boundary: 0.086108, weighted total: 0.04807867571796531347416703283670358359813690185547\n",
      "Epoch 59. Current alpha: 0.552683, lr: 0.0000030000. Training losses: pinn: 1.703126628171958e-05, boundary: 0.083196, weighted total: 0.04598845634126420139908830719832621980458498001099\n",
      "Epoch 60. Current alpha: 0.547157, lr: 0.0000030000. Training losses: pinn: 1.6765729128565e-05, boundary: 0.080651, weighted total: 0.04413615070925334948892526654162793420255184173584\n",
      "Epoch 61. Current alpha: 0.541685, lr: 0.0000030000. Training losses: pinn: 1.6559318851250282e-05, boundary: 0.077935, weighted total: 0.04222361521927150279642759755915903951972723007202\n",
      "Epoch 62. Current alpha: 0.536268, lr: 0.0000030000. Training losses: pinn: 1.659171971368778e-05, boundary: 0.075325, weighted total: 0.04040206064213824194153446001109841745346784591675\n",
      "Epoch 63. Current alpha: 0.530906, lr: 0.0000030000. Training losses: pinn: 1.6525775890841032e-05, boundary: 0.073237, weighted total: 0.03888948957253492849650555740481649991124868392944\n",
      "Epoch 64. Current alpha: 0.525596, lr: 0.0000030000. Training losses: pinn: 1.6456618595839245e-05, boundary: 0.071613, weighted total: 0.03764728495246088407766293926215439569205045700073\n",
      "Epoch 65. Current alpha: 0.520341, lr: 0.0000030000. Training losses: pinn: 1.6418733537193475e-05, boundary: 0.069642, weighted total: 0.03624532922988919075857339180402050260454416275024\n",
      "Epoch 66. Current alpha: 0.515137, lr: 0.0000030000. Training losses: pinn: 1.628855136459606e-05, boundary: 0.068007, weighted total: 0.03504101237786168970833600155856402125209569931030\n",
      "Epoch 67. Current alpha: 0.509986, lr: 0.0000030000. Training losses: pinn: 1.6337617978479102e-05, boundary: 0.066701, weighted total: 0.03402440099215637420160263104662590194493532180786\n",
      "Epoch 68. Current alpha: 0.504886, lr: 0.0000030000. Training losses: pinn: 1.6429660547601088e-05, boundary: 0.066660, weighted total: 0.03366385153717758871305676393603789620101451873779\n",
      "Epoch 69. Current alpha: 0.499837, lr: 0.0000030000. Training losses: pinn: 1.6372291838706587e-05, boundary: 0.068841, weighted total: 0.03441741560131163918390129197177884634584188461304\n",
      "Epoch 70. Current alpha: 0.494839, lr: 0.0000030000. Training losses: pinn: 1.6841688534441346e-05, boundary: 0.067392, weighted total: 0.03335673289921537582047861292267043609172105789185\n",
      "Epoch 71. Current alpha: 0.489890, lr: 0.0000030000. Training losses: pinn: 1.652993250900181e-05, boundary: 0.061249, weighted total: 0.03001348394457557106074219177571649197489023208618\n",
      "Epoch 72. Current alpha: 0.484991, lr: 0.0000030000. Training losses: pinn: 1.6583810804604582e-05, boundary: 0.059032, weighted total: 0.02863865047307005001764323992574645671993494033813\n",
      "Epoch 73. Current alpha: 0.480141, lr: 0.0000030000. Training losses: pinn: 1.6883060141026363e-05, boundary: 0.057774, weighted total: 0.02774867673779963031521766936293715843930840492249\n",
      "Epoch 74. Current alpha: 0.475340, lr: 0.0000030000. Training losses: pinn: 1.6676270604421006e-05, boundary: 0.056781, weighted total: 0.02699889206916518749612698968576296465471386909485\n",
      "Epoch 75. Current alpha: 0.470587, lr: 0.0000030000. Training losses: pinn: 1.6685122034232336e-05, boundary: 0.055936, weighted total: 0.02633159886351249534541807406640145927667617797852\n",
      "Epoch 76. Current alpha: 0.465881, lr: 0.0000030000. Training losses: pinn: 1.6316173116592836e-05, boundary: 0.057855, weighted total: 0.02696207462377504110451198471309908200055360794067\n",
      "Epoch 77. Current alpha: 0.461222, lr: 0.0000030000. Training losses: pinn: 1.6579725922838406e-05, boundary: 0.053711, weighted total: 0.02478165944935279904615121893129980890080332756042\n",
      "Epoch 78. Current alpha: 0.456610, lr: 0.0000030000. Training losses: pinn: 1.6258549521808163e-05, boundary: 0.058487, weighted total: 0.02671470840399083404581581646652921335771679878235\n",
      "Epoch 79. Current alpha: 0.452044, lr: 0.0000030000. Training losses: pinn: 1.6385614117098157e-05, boundary: 0.061339, weighted total: 0.02773708788409922493256587472387764137238264083862\n",
      "Epoch 80. Current alpha: 0.447523, lr: 0.0000030000. Training losses: pinn: 1.659527794117821e-05, boundary: 0.060492, weighted total: 0.02708070178568156710641190443311643321067094802856\n",
      "Epoch 81. Current alpha: 0.443048, lr: 0.0000030000. Training losses: pinn: 1.635767659990961e-05, boundary: 0.059723, weighted total: 0.02646925495712400613901316148712794529274106025696\n",
      "Epoch 82. Current alpha: 0.438618, lr: 0.0000030000. Training losses: pinn: 1.6470015737013455e-05, boundary: 0.058414, weighted total: 0.02563055338380024889000097232383268419653177261353\n",
      "Epoch 83. Current alpha: 0.434231, lr: 0.0000030000. Training losses: pinn: 1.6458512220651755e-05, boundary: 0.051523, weighted total: 0.02238207835056753650104788277985790045931935310364\n",
      "Epoch 84. Current alpha: 0.429889, lr: 0.0000030000. Training losses: pinn: 1.634043985632161e-05, boundary: 0.056166, weighted total: 0.02415461497256588332960269838167732814326882362366\n",
      "Epoch 85. Current alpha: 0.425590, lr: 0.0000030000. Training losses: pinn: 1.600338225671294e-05, boundary: 0.045933, weighted total: 0.01955793216155880809292710864610853604972362518311\n",
      "Epoch 86. Current alpha: 0.421334, lr: 0.0000030000. Training losses: pinn: 1.6342894468834857e-05, boundary: 0.049790, weighted total: 0.02098766080479440646056055186363664688542485237122\n",
      "Epoch 87. Current alpha: 0.417121, lr: 0.0000030000. Training losses: pinn: 1.6556417051560857e-05, boundary: 0.047476, weighted total: 0.01981291140731763350535388212847465183585882186890\n",
      "Epoch 88. Current alpha: 0.412950, lr: 0.0000030000. Training losses: pinn: 1.6590803397775744e-05, boundary: 0.054500, weighted total: 0.02251533440399019342370579011003428604453802108765\n",
      "Epoch 89. Current alpha: 0.408820, lr: 0.0000030000. Training losses: pinn: 1.6052021692303242e-05, boundary: 0.042699, weighted total: 0.01746551925738419955291824692267255159094929695129\n",
      "Epoch 90. Current alpha: 0.404732, lr: 0.0000030000. Training losses: pinn: 1.6471937271944626e-05, boundary: 0.044199, weighted total: 0.01789849176477482581293365626606828300282359123230\n",
      "Epoch 91. Current alpha: 0.400685, lr: 0.0000030000. Training losses: pinn: 1.6142534434493427e-05, boundary: 0.036636, weighted total: 0.01468935324677580860075476465453903074376285076141\n",
      "Epoch 92. Current alpha: 0.396678, lr: 0.0000030000. Training losses: pinn: 1.6553860234580497e-05, boundary: 0.048275, weighted total: 0.01915972804724978481583086420414474559947848320007\n",
      "Epoch 93. Current alpha: 0.392711, lr: 0.0000030000. Training losses: pinn: 1.62889366492891e-05, boundary: 0.051691, weighted total: 0.02030934463474201936983298821814969414845108985901\n",
      "Epoch 94. Current alpha: 0.388784, lr: 0.0000028500. Training losses: pinn: 1.5789808117006032e-05, boundary: 0.033575, weighted total: 0.01306309783148469974567440488044667290523648262024\n",
      "Epoch 95. Current alpha: 0.384896, lr: 0.0000028500. Training losses: pinn: 1.616135790527551e-05, boundary: 0.040649, weighted total: 0.01565544275586911587838656600979447830468416213989\n",
      "Epoch 96. Current alpha: 0.381047, lr: 0.0000028500. Training losses: pinn: 1.6257204265457403e-05, boundary: 0.041528, weighted total: 0.01583429013467964677208676960162847535684704780579\n",
      "Epoch 97. Current alpha: 0.377237, lr: 0.0000028500. Training losses: pinn: 1.6246802431396645e-05, boundary: 0.039184, weighted total: 0.01479190922836389962546910226137697463855147361755\n",
      "Epoch 98. Current alpha: 0.373464, lr: 0.0000028500. Training losses: pinn: 1.6471333594836324e-05, boundary: 0.050906, weighted total: 0.01902189835076650847867085758480243384838104248047\n",
      "Epoch 99. Current alpha: 0.369730, lr: 0.0000028500. Training losses: pinn: 1.6165798399470077e-05, boundary: 0.037651, weighted total: 0.01393081552222825113263038332434007315896451473236\n"
     ]
    }
   ],
   "source": [
    "# Train the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], f_boundary=f_boundary[:, 0], f_bound=f_bound, size=size, n_samples=n_samples)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_predict, client=None, trial=None, alpha=alpha, batchsize=batchsize, \n",
    "                                                 boundary_batchsize=boundary_batchsize, epochs=epochs, lr=lr, size=size, save=save, load_epoch=load_epoch, \n",
    "                                                 lr_decay=lr_decay, alpha_decay=alpha_decay, alpha_limit=alpha_limit, patience=patience, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1efedbc-5843-44e8-981d-b48ca8bf011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open(OUTPUTS_PATH + '/pinn_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open(OUTPUTS_PATH + '/boundary_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "     \n",
    "with open(OUTPUTS_PATH + '/predictions_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b77f7-94b5-4e71-a474-83faf92d8618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
