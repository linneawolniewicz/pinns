{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7e6e69-8377-4874-aee5-22f99dbf95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 14:07:44.945838: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/system/CUDA/11.0.2/nvvm/lib64:/opt/apps/software/system/CUDA/11.0.2/extras/CUPTI/lib64:/opt/apps/software/system/CUDA/11.0.2/lib:/opt/apps/software/lib/slurm-drmaa/1.1.3/lib:/opt/apps/software/lib/libevent/2.1.8/lib:/opt/apps/software/devel/PCRE/8.41-GCCcore-7.3.0/lib:/opt/apps/software/lang/Perl/5.28.0-GCCcore-7.3.0/lib:/opt/apps/software/tools/expat/2.2.5-GCCcore-7.3.0/lib:/opt/apps/software/lang/Python/2.7.15-foss-2018b/lib/python2.7/site-packages/numpy-1.14.5-py2.7-linux-x86_64.egg/numpy/core/lib:/opt/apps/software/lang/Python/2.7.15-foss-2018b/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-7.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-7.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-7.3.0/lib:/opt/apps/software/devel/SQLite/3.24.0-GCCcore-7.3.0/lib:/opt/apps/software/lang/Tcl/8.6.8-GCCcore-7.3.0/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-7.3.0/lib:/opt/apps/software/devel/ncurses/6.1-GCCcore-7.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6-GCCcore-7.3.0/lib:/opt/apps/software/numlib/ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1/lib:/opt/apps/software/numlib/FFTW/3.3.8-gompi-2018b/lib:/opt/apps/software/numlib/OpenBLAS/0.3.1-GCC-7.3.0-2.30/lib:/opt/apps/software/system/hwloc/1.11.10-GCCcore-7.3.0/lib:/opt/apps/software/system/libpciaccess/0.14-GCCcore-7.3.0/lib:/opt/apps/software/lib/libxml2/2.9.8-GCCcore-7.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-7.3.0/lib:/opt/apps/software/tools/numactl/2.0.11-GCCcore-7.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-7.3.0/lib:/opt/apps/software/tools/binutils/2.30-GCCcore-7.3.0/lib:/opt/apps/software/compiler/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0:/opt/apps/software/compiler/GCCcore/7.3.0/lib64:/opt/apps/software/compiler/GCCcore/7.3.0/lib:/opt/apps/software/tools/zsh/5.7.1/lib\n",
      "2022-12-05 14:07:45.127089: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/system/CUDA/11.0.2/nvvm/lib64:/opt/apps/software/system/CUDA/11.0.2/extras/CUPTI/lib64:/opt/apps/software/system/CUDA/11.0.2/lib:/opt/apps/software/lib/slurm-drmaa/1.1.3/lib:/opt/apps/software/lib/libevent/2.1.8/lib:/opt/apps/software/devel/PCRE/8.41-GCCcore-7.3.0/lib:/opt/apps/software/lang/Perl/5.28.0-GCCcore-7.3.0/lib:/opt/apps/software/tools/expat/2.2.5-GCCcore-7.3.0/lib:/opt/apps/software/lang/Python/2.7.15-foss-2018b/lib/python2.7/site-packages/numpy-1.14.5-py2.7-linux-x86_64.egg/numpy/core/lib:/opt/apps/software/lang/Python/2.7.15-foss-2018b/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-7.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-7.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-7.3.0/lib:/opt/apps/software/devel/SQLite/3.24.0-GCCcore-7.3.0/lib:/opt/apps/software/lang/Tcl/8.6.8-GCCcore-7.3.0/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-7.3.0/lib:/opt/apps/software/devel/ncurses/6.1-GCCcore-7.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6-GCCcore-7.3.0/lib:/opt/apps/software/numlib/ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1/lib:/opt/apps/software/numlib/FFTW/3.3.8-gompi-2018b/lib:/opt/apps/software/numlib/OpenBLAS/0.3.1-GCC-7.3.0-2.30/lib:/opt/apps/software/system/hwloc/1.11.10-GCCcore-7.3.0/lib:/opt/apps/software/system/libpciaccess/0.14-GCCcore-7.3.0/lib:/opt/apps/software/lib/libxml2/2.9.8-GCCcore-7.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-7.3.0/lib:/opt/apps/software/tools/numactl/2.0.11-GCCcore-7.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-7.3.0/lib:/opt/apps/software/tools/binutils/2.30-GCCcore-7.3.0/lib:/opt/apps/software/compiler/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0:/opt/apps/software/compiler/GCCcore/7.3.0/lib64:/opt/apps/software/compiler/GCCcore/7.3.0/lib:/opt/apps/software/tools/zsh/5.7.1/lib\n",
      "2022-12-05 14:07:45.127131: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfm = tf.math\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8fd5e8-fb2d-4ec5-a104-5f606294015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"data\"))\n",
    "OUTPUTS_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"outputs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4ede82-5306-4443-be72-353fbda27c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(DATA_PATH + '/f_boundary.pkl', 'rb') as file:\n",
    "    f_boundary = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/p.pkl', 'rb') as file:\n",
    "    p = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/T.pkl', 'rb') as file:\n",
    "    T = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/r.pkl', 'rb') as file:\n",
    "    r = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/P_predict.pkl', 'rb') as file:\n",
    "    P_predict = pkl.load(file)\n",
    "    \n",
    "# Get upper and lower bounds\n",
    "lb = np.log(np.array([p[0], r[0]], dtype='float32'))\n",
    "ub = np.log(np.array([p[-1], r[-1]], dtype='float32'))\n",
    "min_f_log_space = -34.54346331847909\n",
    "max_f_log_space = 6.466899920699378\n",
    "f_bound = np.array([min_f_log_space, max_f_log_space], dtype='float32')\n",
    "size = len(f_boundary[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7dab597-dc24-4b1c-a440-00e78daca9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, f_boundary, f_bound, size, num_samples=20_000):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound # In log space\n",
    "        self.upper_bound = upper_bound # In log space\n",
    "        self.p = p # In real space\n",
    "        self.f_boundary = f_boundary # In scaled space (0 to 1)\n",
    "        self.num_samples = num_samples\n",
    "        self.size = size\n",
    "        self.f_bound = f_bound # In log space\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and f_boundary.\n",
    "        \n",
    "        g_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs. G_boundary is the\n",
    "                    scaled version of f_boundary, and relates to f_boundary via g = (log(f) - min(log(f)))/(max(log(f)) - min(log(f)))\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "        beta: weight on pinn_loss to scale it to the same order of magnitude as boundary_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha, beta):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                t1.watch(p)\n",
    "                t1.watch(r)\n",
    "                t1.watch(p_boundary)\n",
    "                t1.watch(r_boundary)\n",
    "                \n",
    "                # PINN loss data\n",
    "                p_scaled = self.scale(p, self.upper_bound[0], self.lower_bound[0], should_take_log=True)\n",
    "                r_scaled = self.scale(r, self.upper_bound[1], self.lower_bound[1], should_take_log=True)\n",
    "                P = tf.concat((p_scaled, r_scaled), axis=1)\n",
    "                g = self.tf_call(P)\n",
    "\n",
    "                # Boundary loss data\n",
    "                p_boundary_scaled = self.scale(p_boundary, self.upper_bound[0], self.lower_bound[0], should_take_log=True)\n",
    "                r_boundary_scaled = self.scale(r_boundary, self.upper_bound[1], self.lower_bound[1], should_take_log=True)\n",
    "                P_boundary = tf.concat((p_boundary_scaled, r_boundary_scaled), axis=1)\n",
    "                g_pred_boundary = self.tf_call(P_boundary)\n",
    "                \n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tfm.reduce_mean(tfm.square(g_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order gradients\n",
    "            dg_dp = t1.gradient(g, p)\n",
    "            dg_dr = t1.gradient(g, r)\n",
    "            \n",
    "            dg_dp_boundary = t1.gradient(g_pred_boundary, p_boundary)\n",
    "            dg_dr_boundary = t1.gradient(g_pred_boundary, r_boundary)\n",
    "            \n",
    "            # Calculate f (real space) from g (scaled sapce) and get df/dg\n",
    "            with tf.GradientTape(persistent=True) as t3: \n",
    "                t3.watch(g)\n",
    "                t3.watch(g_pred_boundary)\n",
    "                \n",
    "                diff = tfm.abs(self.f_bound[1] - self.f_bound[0])\n",
    "\n",
    "                f = tfm.exp(g*diff + self.f_bound[0])\n",
    "                f_pred_boundary = tfm.exp(g_pred_boundary*diff + self.f_bound[0])\n",
    "\n",
    "                df_dg = t3.gradient(f, g)\n",
    "                df_dg_boundary = t3.gradient(f_pred_boundary, g_pred_boundary)\n",
    "            \n",
    "            # Use chain rule to calculate df/dp and df/dr\n",
    "            df_dp = df_dg*dg_dp\n",
    "            df_dr = df_dg*dg_dr\n",
    "            \n",
    "            df_dp_boundary = df_dg_boundary*dg_dp_boundary\n",
    "            df_dr_boundary = df_dg_boundary*dg_dr_boundary\n",
    "            \n",
    "            # Calculate PINN loss and total loss\n",
    "            pinn_loss = beta*(self.pinn_loss(p, r, df_dp, df_dr) + self.pinn_loss(p_boundary, r_boundary, df_dp_boundary, df_dr_boundary))\n",
    "            \n",
    "            total_loss = (1-alpha)*pinn_loss + alpha*boundary_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        client, trial: Sherpa client and trial\n",
    "        \n",
    "        beta: weight on pinn_loss to scale it to the same order of magnitude as boundary_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_schedule: Determines the schedule lr will be on. Options include 'decay' and 'oscillate', else lr will remain constant\n",
    "        \n",
    "        alpha_schedule: Determines the schedule alpha will be on. Choices include 'decay', 'grow', and 'oscillate'\n",
    "        \n",
    "        r_lower: Changes the r sampling. R will be sampled between r_lower and self.upper_bound[1] according to\n",
    "        the sampling_method. Default r_lower is self.lower_bound[1], or np.log(0.4 * 150e6)\n",
    "        \n",
    "        patience: Number of epochs to check whether loss has decreased before updating lr\n",
    "        \n",
    "        filename: Name for the checkpoint file\n",
    "        \n",
    "        sampling_method = Method for sampling r. Choices include 'beta_3_1' or 'beta_1_3', otherwise will sample uniformlly in real space\n",
    "        \n",
    "        should_r_lower_change = Toggle for whether to decrease r_lower from self.upper_bound[1] to self.lower_bound[1] or not.\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, client=None, trial=None, beta=1, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, \n",
    "            size=256, save=False, load_epoch=-1, lr_schedule='', alpha_schedule='', r_lower=17.909855, patience=3, filename='', \n",
    "            sampling_method='uniform', should_r_lower_change=False):\n",
    "        \n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(load_epoch)\n",
    "            self.load_weights(name)\n",
    "        \n",
    "        # Initialize alpha based on alpha_schedule\n",
    "        if (alpha_schedule == 'decay'): alpha = 1.0\n",
    "        elif (alpha_schedule == 'grow'): alpha = 0.001\n",
    "        else: alpha = 0.5\n",
    "        \n",
    "        # Initialize\n",
    "        steps_per_epoch = np.ceil(self.num_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs,))\n",
    "        total_boundary_loss = np.zeros((epochs,))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, sample data and pass to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Sample p according to a uniform distribution in log space\n",
    "                uniform_dist = tfd.Uniform(0, 1)\n",
    "                p = (uniform_dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[0] - self.lower_bound[0])) + self.lower_bound[0]\n",
    "                p = tfm.exp(p)\n",
    "                \n",
    "                # Sample r according to sampling_method variable\n",
    "                if(sampling_method=='beta_1_3'): dist = tfd.Beta(1, 3)\n",
    "                elif(sampling_method=='beta_3_1'): dist = tfd.Beta(3, 1)\n",
    "                else: dist = tfd.Uniform(0, 1)\n",
    "                r = (dist.sample((batchsize, 1))*tfm.abs(tfm.exp(self.upper_bound[1]) - tfm.exp(r_lower))) + tfm.exp(r_lower)\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = tf.Variable(self.p[p_idx], dtype=tf.float32)\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_boundary = np.zeros((boundary_batchsize, 1))\n",
    "                upper_boundary[:] = tfm.exp(self.upper_bound[1])\n",
    "                r_boundary = tf.Variable(upper_boundary, dtype=tf.float32)\n",
    "                \n",
    "                # Train and get loss\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Sum losses\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Epoch {epoch}. Current alpha: {alpha:.6f}, lr: {lr:.10f}, ' + \n",
    "                  f'Training losses: pinn: {total_pinn_loss[epoch]:.10f}, boundary: {total_boundary_loss[epoch]:.10f}, ' +\n",
    "                  f'weighted total: {((alpha*total_boundary_loss[epoch])+((1-alpha)*total_pinn_loss[epoch])):.10f}')\n",
    "            \n",
    "            predictions[:, :, epoch] = self.predict(P_predict, batchsize)\n",
    "            \n",
    "            # Adjust alpha based on the schedule, only every 10 epochs\n",
    "            if (epoch%10 == 0):\n",
    "                if alpha_schedule=='decay': alpha = alpha*0.995\n",
    "                elif (alpha_schedule=='grow') & (alpha <= 1): alpha = alpha*1.015\n",
    "\n",
    "            # Check if loss has decreased\n",
    "            hasnt_decreased = False\n",
    "            if (epoch > patience):\n",
    "                if (total_pinn_loss[epoch] + total_boundary_loss[epoch]) > (total_pinn_loss[epoch-patience] + total_boundary_loss[epoch-patience]):\n",
    "                    hasnt_decreased = True\n",
    "                        \n",
    "            # If loss hasn't decreased, adjust lr based on the assigned schedule\n",
    "            if ((lr_schedule == 'decay') & hasnt_decreased): lr = lr*0.95\n",
    "            elif ((lr_schedule == 'oscillate') & hasnt_decreased): lr = self.oscillate_lr(lr)\n",
    "                \n",
    "            \n",
    "            # Decrease lower r if told to\n",
    "            if should_r_lower_change & (epoch%10==0): r_lower = r_lower*0.95\n",
    "\n",
    "            # Save the model to a checkpoint\n",
    "            should_save = (epoch%100 == 0) & (save == True)\n",
    "            if should_save:\n",
    "                name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(epoch)\n",
    "                self.save_weights(name, overwrite=True, save_format=None, options=None)\n",
    "                \n",
    "            # Send metrics if running Sherpa optimization\n",
    "            if client:\n",
    "                if (np.isnan(total_pinn_loss[epoch]) and np.isnan(total_boundary_loss[epoch])):\n",
    "                    obj = np.inf\n",
    "                else:\n",
    "                    obj = total_pinn_loss[epoch] + total_boundary_loss[epoch]\n",
    "                client.send_metrics(\n",
    "                         trial=trial,\n",
    "                         iteration=epoch,\n",
    "                         objective=obj)\n",
    "        \n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, batchsize):\n",
    "        P_size = P.shape[0]\n",
    "        steps_per_epoch = np.ceil(P_size / batchsize).astype(int)\n",
    "        predictions = np.zeros((P_size, 1))\n",
    "        \n",
    "        # For each step predict on data between start and end indices\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # Calculate end_idx\n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P_size - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Predict\n",
    "            predictions[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, df_dp, df_dr):\n",
    "        V = 400 # 400 km/s\n",
    "        M = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tfm.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tfm.divide(p, tfm.sqrt(tfm.square(p) + tfm.square(M))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tfm.reduce_mean(tfm.square(df_dr + (tfm.divide(R*V, 3*k) * df_dp)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)\n",
    "    \n",
    "    # Scales input data and returns scaled version\n",
    "    def scale(self, data, upper_bound, lower_bound, should_take_log=True):\n",
    "        if should_take_log: scaled_data = (tfm.log(data) - lower_bound)/tfm.abs(upper_bound - lower_bound)   \n",
    "        else: scaled_data = (data - lower_bound)/tfm.abs(upper_bound - lower_bound)\n",
    "            \n",
    "        return scaled_data\n",
    "    \n",
    "    ################################## Still being implemented\n",
    "    # Implements an oscillating lr according to the triangular CLR schedule\n",
    "    def oscillate_lr(lr): #stepsize, min_lr=3e-4, max_lr=3e-3):\n",
    "        return lr\n",
    "#         # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "#         scaler = lambda x: 1.\n",
    "\n",
    "#         # Lambda function to calculate the LR\n",
    "#         lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
    "\n",
    "#         # Additional function to see where on the cycle we are\n",
    "#         def relative(it, stepsize):\n",
    "#             cycle = math.floor(1 + it / (2 * stepsize))\n",
    "#             x = abs(it / stepsize - 2 * cycle + 1)\n",
    "#             return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "#         return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442d0c7b-4143-45df-95fc-a534727e3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 5\n",
    "r_lower = np.log(119*150e6).astype('float32')\n",
    "beta = 1e13\n",
    "alpha_schedule = ''\n",
    "lr_schedule = 'decay'\n",
    "patience = 3\n",
    "batchsize = 2048\n",
    "boundary_batchsize = 512\n",
    "activation = 'selu'\n",
    "save = False\n",
    "load_epoch = -1\n",
    "num_samples = 20000\n",
    "lr = 3e-4\n",
    "num_layers = 2\n",
    "num_hidden_units = 100\n",
    "sampling_method = ''\n",
    "final_activation = 'sigmoid'\n",
    "should_r_lower_change = False\n",
    "filename = 'test'\n",
    "\n",
    "# Create model\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(inputs)\n",
    "for _ in range(num_layers-1):\n",
    "    x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation=final_activation)(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64a9cf11-b7a4-4914-866b-6fa66a51ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Epoch 0. Current alpha: 0.500000, lr: 0.0003000000, Training losses: pinn: 0.0000000000, boundary: 1.0506984815, weighted total: 0.5253492408\n",
      "Epoch 1. Current alpha: 0.500000, lr: 0.0003000000, Training losses: pinn: 0.0000000001, boundary: 0.6884833686, weighted total: 0.3442416844\n",
      "Epoch 2. Current alpha: 0.500000, lr: 0.0003000000, Training losses: pinn: 0.0000001283, boundary: 0.4335830770, weighted total: 0.2167916027\n",
      "Epoch 3. Current alpha: 0.500000, lr: 0.0003000000, Training losses: pinn: 0.0000289210, boundary: 0.2454523314, weighted total: 0.1227406262\n",
      "Epoch 4. Current alpha: 0.500000, lr: 0.0003000000, Training losses: pinn: 0.0018801339, boundary: 0.1197382696, weighted total: 0.0608092018\n"
     ]
    }
   ],
   "source": [
    "# Train the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], f_boundary=f_boundary[:, 0], f_bound=f_bound, size=size, num_samples=num_samples)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_predict, client=None, trial=None, beta=beta, batchsize=batchsize, \n",
    "                                                 boundary_batchsize=boundary_batchsize, epochs=epochs, lr=lr, size=size, save=save, load_epoch=load_epoch, \n",
    "                                                 lr_schedule=lr_schedule, alpha_schedule=alpha_schedule, r_lower=r_lower, patience=patience, \n",
    "                                                 filename=filename, sampling_method=sampling_method, should_r_lower_change=should_r_lower_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1efedbc-5843-44e8-981d-b48ca8bf011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open(OUTPUTS_PATH + '/pinn_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open(OUTPUTS_PATH + '/boundary_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "     \n",
    "with open(OUTPUTS_PATH + '/predictions_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b77f7-94b5-4e71-a474-83faf92d8618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
