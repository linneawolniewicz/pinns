{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7e6e69-8377-4874-aee5-22f99dbf95b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfm = tf.math\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8fd5e8-fb2d-4ec5-a104-5f606294015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"data\"))\n",
    "OUTPUTS_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"outputs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4ede82-5306-4443-be72-353fbda27c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(DATA_PATH + '/f_boundary.pkl', 'rb') as file:\n",
    "    f_boundary = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/p.pkl', 'rb') as file:\n",
    "    p = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/T.pkl', 'rb') as file:\n",
    "    T = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/r.pkl', 'rb') as file:\n",
    "    r = pkl.load(file)\n",
    "    \n",
    "with open(DATA_PATH + '/P_predict.pkl', 'rb') as file:\n",
    "    P_predict = pkl.load(file)\n",
    "    \n",
    "# Get upper and lower bounds\n",
    "lb = np.log(np.array([p[0], r[0]], dtype='float32'))\n",
    "ub = np.log(np.array([p[-1], r[-1]], dtype='float32'))\n",
    "f_bound = np.array([-34.54346331847909, 6.466899920699378], dtype='float32')\n",
    "size = len(f_boundary[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7dab597-dc24-4b1c-a440-00e78daca9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, f_boundary, f_bound, size, n_samples=20000):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound # In log space\n",
    "        self.upper_bound = upper_bound # In log space\n",
    "        self.p = p # In real space\n",
    "        self.f_boundary = f_boundary # In log and scaled space\n",
    "        self.n_samples = n_samples\n",
    "        self.size = size\n",
    "        self.f_bound = f_bound # In log space\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and f_boundary.\n",
    "        \n",
    "        g_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs. G_boundary is the\n",
    "                    scaled version of f_boundary, and relates to f_boundary via g = (log(f) - min(log(f)))/(max(log(f)) - min(log(f)))\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "        beta: weight on pinn_loss to scale it to the same order of magnitude as boundary_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha, beta):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                t1.watch(p)\n",
    "                t1.watch(r)\n",
    "                t1.watch(p_boundary)\n",
    "                t1.watch(r_boundary)\n",
    "                \n",
    "                # print(\"Before scaling --------------------------\")\n",
    "                # print(f'p {tfm.reduce_max(p), tfm.reduce_min(p)} r {tfm.reduce_max(r), tfm.reduce_min(r)}')\n",
    "                # print(f'p_boundary {tfm.reduce_max(p_boundary), tfm.reduce_min(p_boundary)} r_boundary {tfm.reduce_max(r_boundary), tfm.reduce_min(r_boundary)}')\n",
    "                # print(f'f_boundary {tfm.reduce_max(f_boundary), tfm.reduce_min(f_boundary)}')\n",
    "                \n",
    "                # PINN loss data\n",
    "                p_scaled = (tfm.log(p) - self.lower_bound[0])/tfm.abs(self.upper_bound[0] - self.lower_bound[0])\n",
    "                r_scaled = (tfm.log(r) - self.lower_bound[1])/tfm.abs(self.upper_bound[1] - self.lower_bound[1])\n",
    "                P = tf.concat((p_scaled, r_scaled), axis=1)\n",
    "                g = self.tf_call(P)\n",
    "\n",
    "                # Boundary loss data\n",
    "                p_boundary_scaled = (tfm.log(p_boundary) - self.lower_bound[0])/tfm.abs(self.upper_bound[0] - self.lower_bound[0])\n",
    "                r_boundary_scaled = (tfm.log(r_boundary) - self.lower_bound[1])/tfm.abs(self.upper_bound[1] - self.lower_bound[1])\n",
    "                P_boundary = tf.concat((p_boundary_scaled, r_boundary_scaled), axis=1)\n",
    "                g_pred_boundary = self.tf_call(P_boundary)\n",
    "                \n",
    "                # print(\"After scaling --------------------------\")\n",
    "                # print(f'p_scaled {tfm.reduce_max(p_scaled), tfm.reduce_min(p_scaled)} r_scaled {tfm.reduce_max(r_scaled), tfm.reduce_min(r_scaled)}')\n",
    "                # print(f'p_boundary_scaled {tfm.reduce_max(p_boundary_scaled), tfm.reduce_min(p_boundary_scaled)} r_boundary_scaled {tfm.reduce_max(r_boundary_scaled), tfm.reduce_min(r_boundary_scaled)}')\n",
    "                # print(f'f {tfm.reduce_max(f), tfm.reduce_min(f)} f_pred_boundary {tfm.reduce_max(f_pred_boundary), tfm.reduce_min(f_pred_boundary)}')\n",
    "                \n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tfm.reduce_mean(tfm.square(g_pred_boundary - f_boundary))\n",
    "                # print(f'Boundary loss: {boundary_loss}')\n",
    "\n",
    "            # Calculate first-order gradients\n",
    "            g_p = t1.gradient(g, p)\n",
    "            g_r = t1.gradient(g, r)\n",
    "            \n",
    "            g_p_boundary = t1.gradient(g_pred_boundary, p_boundary)\n",
    "            g_r_boundary = t1.gradient(g_pred_boundary, r_boundary)\n",
    "            \n",
    "            # Calculate f (real space) from g (scaled sapce) and get df/dg\n",
    "            with tf.GradientTape(persistent=True) as t3: \n",
    "                t3.watch(g)\n",
    "                t3.watch(g_pred_boundary)\n",
    "                \n",
    "                diff = tfm.abs(self.f_bound[1] - self.f_bound[0])\n",
    "\n",
    "                f = tfm.exp(g*diff + self.f_bound[0])\n",
    "                f_pred_boundary = tfm.exp(g_pred_boundary*diff + self.f_bound[0])\n",
    "\n",
    "                f_g = t3.gradient(f, g)\n",
    "                f_g_boundary = t3.gradient(f_pred_boundary, g_pred_boundary)\n",
    "            \n",
    "            # Use df/dg in the chain rule to calculate df/dp and df/dr\n",
    "            f_p = f_g*g_p\n",
    "            f_r = f_g*g_r\n",
    "            \n",
    "            f_p_boundary = f_g_boundary*g_p_boundary\n",
    "            f_r_boundary = f_g_boundary*g_r_boundary\n",
    "            \n",
    "            # Calculate PINN loss and total loss\n",
    "            pinn_loss = beta*self.pinn_loss(p, r, f_p, f_r) + beta*self.pinn_loss(p_boundary, r_boundary, f_p_boundary, f_r_boundary)\n",
    "            # print(f'PINN loss: {pinn_loss}')\n",
    "            \n",
    "            total_loss = (1-alpha)*pinn_loss + alpha*boundary_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        client, trial: Sherpa client and trial\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "        beta: weight on pinn_loss to scale it to the same order of magnitude as boundary_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_decay: If -1, learning rate will not be decayed. Otherwise, lr = lr_decay*lr if loss hasn't \n",
    "        decreased\n",
    "        \n",
    "        alpha_decay: If -1, alpha will not be changed. Otherwise, alpha = alpha_decay*alpha if loss \n",
    "        hasn't decreased\n",
    "        \n",
    "        r_lower_change: If -1, r_lower will not be changed. Otherwise r_lower will decrease from the self.upper_bound[1] to \n",
    "        self.lower_bound[1].\n",
    "        \n",
    "        alpha_limit = Minimum alpha value to decay to\n",
    "        \n",
    "        patience: Number of epochs to check whether loss has decreased before updating lr or alpha\n",
    "        \n",
    "        filename: Name for the checkpoint file\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, client=None, trial=None, alpha=0.5, beta=1, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, size=256, \n",
    "            save=False, load_epoch=-1, lr_decay=-1, alpha_decay=-1, r_lower_change=-1, alpha_limit = 0, patience=3, filename=''):\n",
    "        \n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(load_epoch)\n",
    "            self.load_weights(name)\n",
    "        \n",
    "        # Initialize\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs,))\n",
    "        total_boundary_loss = np.zeros((epochs,))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # Lower r boundary\n",
    "        r_lower = self.upper_bound[1]\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, sample data and pass to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Sample p and r\n",
    "                beta_dist = tfd.Beta(3, 1)\n",
    "                uniform_dist = tfd.Uniform(0, 1)\n",
    "\n",
    "                p = (uniform_dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[0] - self.lower_bound[0])) + self.lower_bound[0]\n",
    "                p = tfm.exp(p)\n",
    "                r = (beta_dist.sample((batchsize, 1))*tfm.abs(tfm.exp(self.upper_bound[1]) - tfm.exp(r_lower))) + tfm.exp(r_lower)\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = tf.Variable(self.p[p_idx], dtype=tf.float32)\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_boundary = np.zeros((boundary_batchsize, 1))\n",
    "                upper_boundary[:] = tfm.exp(self.upper_bound[1])\n",
    "                r_boundary = tf.Variable(upper_boundary, dtype=tf.float32)\n",
    "                \n",
    "                # Train and get loss\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Sum losses\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Epoch {epoch}. Current alpha: {alpha:.6f}, lr: {lr:.10f}, r_lower: {(np.exp(r_lower[0])/150e6):.1f}. ' + \n",
    "                  f'Training losses: pinn: {total_pinn_loss[epoch]:.10f}, boundary: {total_boundary_loss[epoch]:.10f}, ' +\n",
    "                  f'weighted total: {((alpha*total_boundary_loss[epoch])+((1-alpha)*total_pinn_loss[epoch])):.10f}')\n",
    "            \n",
    "            predictions[:, :, epoch] = self.predict(P_predict, batchsize)\n",
    "            \n",
    "            # Decay lr if loss hasn't decreased since current epoch - patience\n",
    "            hasntDecreased = False\n",
    "            if (epoch > patience):\n",
    "                if (total_pinn_loss[epoch] + total_boundary_loss[epoch]) > (total_pinn_loss[epoch-patience] + total_boundary_loss[epoch-patience]):\n",
    "                    hasntDecreased = True\n",
    "                        \n",
    "            if (lr_decay != -1) & hasntDecreased:\n",
    "                lr = lr_decay*lr\n",
    "\n",
    "            # Decrease alpha each epoch\n",
    "            if (alpha_decay != -1) & (alpha >= alpha_limit) & (epoch%10 == 0):\n",
    "                alpha = alpha_decay*alpha\n",
    "            \n",
    "            # Decrease lower r boundary from 120au to 0.4au every 10 epochs\n",
    "            if (r_lower_change != -1) & (r_lower >= self.lower_bound[1]) & (epoch%10 == 0): #23.6) & (epoch%10 == 0):\n",
    "                r_lower = r_lower_change*r_lower\n",
    "\n",
    "            # If the epoch is a multiple of 100, save to a checkpoint\n",
    "            if (epoch%100 == 0) & (save == True):\n",
    "                name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(epoch)\n",
    "                self.save_weights(name, overwrite=True, save_format=None, options=None)\n",
    "                \n",
    "            # Send metrics\n",
    "            if client:\n",
    "                if (np.isnan(total_pinn_loss[epoch]) and np.isnan(total_boundary_loss[epoch])):\n",
    "                    obj = np.inf\n",
    "                else:\n",
    "                    obj = total_pinn_loss[epoch] + total_boundary_loss[epoch]\n",
    "                client.send_metrics(\n",
    "                         trial=trial,\n",
    "                         iteration=epoch,\n",
    "                         objective=obj)\n",
    "        \n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, batchsize):\n",
    "        P_size = P.shape[0]\n",
    "        steps_per_epoch = np.ceil(P_size / batchsize).astype(int)\n",
    "        predictions = np.zeros((P_size, 1))\n",
    "        \n",
    "        # For each step predict on data between start and end indices\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # Calculate end_idx\n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P_size - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Predict\n",
    "            predictions[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r):\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tfm.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tfm.divide(p, tfm.sqrt(tfm.square(p) + tfm.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tfm.reduce_mean(tfm.square(f_r + (tfm.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "442d0c7b-4143-45df-95fc-a534727e3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "r_lower_change = 0.995\n",
    "alpha = 1\n",
    "beta = 1e8\n",
    "alpha_decay = 0.9924626108148258\n",
    "alpha_limit = 0\n",
    "lr_decay = 0.95\n",
    "patience = 10\n",
    "batchsize = 1032\n",
    "boundary_batchsize = 512\n",
    "activation = 'selu'\n",
    "save = False\n",
    "load_epoch = -1\n",
    "filename = 'test'\n",
    "n_samples = 20000\n",
    "lr = 8.41450877832526e-06\n",
    "num_layers = 4\n",
    "num_hidden_units = 54\n",
    "\n",
    "# Create model\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(inputs)\n",
    "for _ in range(num_layers-1):\n",
    "    x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64a9cf11-b7a4-4914-866b-6fa66a51ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Epoch 0. Current alpha: 1.000000, lr: 0.0000084145, r_lower: 120.0. Training losses: pinn: 0.0000000000, boundary: 3.740793, weighted total: 3.74079324\n",
      "Epoch 1. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000000, boundary: 2.710452, weighted total: 2.69002257\n",
      "Epoch 2. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000000, boundary: 2.138586, weighted total: 2.12246673\n",
      "Epoch 3. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000000, boundary: 1.819125, weighted total: 1.80541352\n",
      "Epoch 4. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000000, boundary: 1.552193, weighted total: 1.54049373\n",
      "Epoch 5. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000000, boundary: 1.308647, weighted total: 1.29878286\n",
      "Epoch 6. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000000, boundary: 1.086105, weighted total: 1.07791867\n",
      "Epoch 7. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000001, boundary: 0.883854, weighted total: 0.87719202\n",
      "Epoch 8. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000017, boundary: 0.702606, weighted total: 0.69730988\n",
      "Epoch 9. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000000390, boundary: 0.545498, weighted total: 0.54138645\n",
      "Epoch 10. Current alpha: 0.992463, lr: 0.0000084145, r_lower: 119.9. Training losses: pinn: 0.0000006617, boundary: 0.413659, weighted total: 0.41054095\n",
      "Epoch 11. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.0000094053, boundary: 0.305229, weighted total: 0.30064555\n",
      "Epoch 12. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.0001386744, boundary: 0.217245, weighted total: 0.21398446\n",
      "Epoch 13. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.0017210895, boundary: 0.148598, weighted total: 0.14639248\n",
      "Epoch 14. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.0185409275, boundary: 0.098345, weighted total: 0.09714680\n",
      "Epoch 15. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.1139104909, boundary: 0.065221, weighted total: 0.06595185\n",
      "Epoch 16. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.2277081488, boundary: 0.046379, weighted total: 0.04910195\n",
      "Epoch 17. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.2274727924, boundary: 0.035287, weighted total: 0.03817296\n",
      "Epoch 18. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.1916308254, boundary: 0.027471, weighted total: 0.02993586\n",
      "Epoch 19. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.1560208024, boundary: 0.021389, weighted total: 0.02341062\n",
      "Epoch 20. Current alpha: 0.984982, lr: 0.0000084145, r_lower: 119.7. Training losses: pinn: 0.1217798260, boundary: 0.016958, weighted total: 0.01853220\n",
      "Epoch 21. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0803497809, boundary: 0.013871, weighted total: 0.01536266\n",
      "Epoch 22. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0681301756, boundary: 0.011386, weighted total: 0.01265977\n",
      "Epoch 23. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0579554008, boundary: 0.009322, weighted total: 0.01041364\n",
      "Epoch 24. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0499345066, boundary: 0.007699, weighted total: 0.00864721\n",
      "Epoch 25. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0420859582, boundary: 0.006458, weighted total: 0.00725778\n",
      "Epoch 26. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0365723615, boundary: 0.005490, weighted total: 0.00618718\n",
      "Epoch 27. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0328034792, boundary: 0.004721, weighted total: 0.00535127\n",
      "Epoch 28. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0280558545, boundary: 0.004078, weighted total: 0.00461613\n",
      "Epoch 29. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0246898858, boundary: 0.003517, weighted total: 0.00399206\n",
      "Epoch 30. Current alpha: 0.977558, lr: 0.0000084145, r_lower: 119.6. Training losses: pinn: 0.0212709727, boundary: 0.003065, weighted total: 0.00347345\n",
      "Epoch 31. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0179141845, boundary: 0.002699, weighted total: 0.00315231\n",
      "Epoch 32. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0152599590, boundary: 0.002367, weighted total: 0.00275110\n",
      "Epoch 33. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0139047089, boundary: 0.002087, weighted total: 0.00243925\n",
      "Epoch 34. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0120311367, boundary: 0.001838, weighted total: 0.00214177\n",
      "Epoch 35. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0106362405, boundary: 0.001637, weighted total: 0.00190574\n",
      "Epoch 36. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0094274283, boundary: 0.001459, weighted total: 0.00169667\n",
      "Epoch 37. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0083533702, boundary: 0.001317, weighted total: 0.00152672\n",
      "Epoch 38. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0074961950, boundary: 0.001192, weighted total: 0.00138018\n",
      "Epoch 39. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0072265955, boundary: 0.001091, weighted total: 0.00127430\n",
      "Epoch 40. Current alpha: 0.970190, lr: 0.0000084145, r_lower: 119.4. Training losses: pinn: 0.0065188294, boundary: 0.000999, weighted total: 0.00116350\n",
      "Epoch 41. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0058033304, boundary: 0.000936, weighted total: 0.00111637\n",
      "Epoch 42. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0053473628, boundary: 0.000852, weighted total: 0.00101893\n",
      "Epoch 43. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0048141635, boundary: 0.000808, weighted total: 0.00095694\n",
      "Epoch 44. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0045410228, boundary: 0.000739, weighted total: 0.00087992\n",
      "Epoch 45. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0043263789, boundary: 0.000712, weighted total: 0.00084602\n",
      "Epoch 46. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0039994106, boundary: 0.000648, weighted total: 0.00077284\n",
      "Epoch 47. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0038324599, boundary: 0.000633, weighted total: 0.00075130\n",
      "Epoch 48. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0034543221, boundary: 0.000575, weighted total: 0.00068178\n",
      "Epoch 49. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0033369183, boundary: 0.000564, weighted total: 0.00066714\n",
      "Epoch 50. Current alpha: 0.962877, lr: 0.0000084145, r_lower: 119.3. Training losses: pinn: 0.0031074693, boundary: 0.000513, weighted total: 0.00060895\n",
      "Epoch 51. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0030636479, boundary: 0.000514, weighted total: 0.00062758\n",
      "Epoch 52. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0029578242, boundary: 0.000458, weighted total: 0.00056911\n",
      "Epoch 53. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0027017570, boundary: 0.000465, weighted total: 0.00056408\n",
      "Epoch 54. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0026002727, boundary: 0.000412, weighted total: 0.00050870\n",
      "Epoch 55. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0025764429, boundary: 0.000420, weighted total: 0.00051580\n",
      "Epoch 56. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0024399898, boundary: 0.000370, weighted total: 0.00046192\n",
      "Epoch 57. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0023830570, boundary: 0.000383, weighted total: 0.00047165\n",
      "Epoch 58. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0023358818, boundary: 0.000334, weighted total: 0.00042317\n",
      "Epoch 59. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0022728991, boundary: 0.000347, weighted total: 0.00043261\n",
      "Epoch 60. Current alpha: 0.955619, lr: 0.0000084145, r_lower: 119.2. Training losses: pinn: 0.0022244655, boundary: 0.000304, weighted total: 0.00038935\n",
      "Epoch 61. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0021945709, boundary: 0.000320, weighted total: 0.00041696\n",
      "Epoch 62. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0021338195, boundary: 0.000278, weighted total: 0.00037409\n",
      "Epoch 63. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0020900065, boundary: 0.000294, weighted total: 0.00038643\n",
      "Epoch 64. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0020682016, boundary: 0.000254, weighted total: 0.00034745\n",
      "Epoch 65. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0020259223, boundary: 0.000272, weighted total: 0.00036256\n",
      "Epoch 66. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0019323249, boundary: 0.000232, weighted total: 0.00031966\n",
      "Epoch 67. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0019629989, boundary: 0.000252, weighted total: 0.00034037\n",
      "Epoch 68. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0019280325, boundary: 0.000214, weighted total: 0.00030217\n",
      "Epoch 69. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0019413627, boundary: 0.000236, weighted total: 0.00032361\n",
      "Epoch 70. Current alpha: 0.948416, lr: 0.0000084145, r_lower: 119.0. Training losses: pinn: 0.0019278076, boundary: 0.000197, weighted total: 0.00028650\n",
      "Epoch 71. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0018808600, boundary: 0.000222, weighted total: 0.00031984\n",
      "Epoch 72. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0018283495, boundary: 0.000184, weighted total: 0.00028101\n",
      "Epoch 73. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0018258816, boundary: 0.000210, weighted total: 0.00030500\n",
      "Epoch 74. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0017901829, boundary: 0.000172, weighted total: 0.00026658\n",
      "Epoch 75. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0018116958, boundary: 0.000200, weighted total: 0.00029491\n",
      "Epoch 76. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0017599209, boundary: 0.000161, weighted total: 0.00025534\n",
      "Epoch 77. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0017798056, boundary: 0.000190, weighted total: 0.00028313\n",
      "Epoch 78. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0017400135, boundary: 0.000152, weighted total: 0.00024521\n",
      "Epoch 79. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0017325165, boundary: 0.000182, weighted total: 0.00027301\n",
      "Epoch 80. Current alpha: 0.941268, lr: 0.0000084145, r_lower: 118.9. Training losses: pinn: 0.0017945000, boundary: 0.000145, weighted total: 0.00024152\n",
      "Epoch 81. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0017433108, boundary: 0.000174, weighted total: 0.00027742\n",
      "Epoch 82. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0016218244, boundary: 0.000140, weighted total: 0.00023798\n",
      "Epoch 83. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0017108091, boundary: 0.000166, weighted total: 0.00026774\n",
      "Epoch 84. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0016373678, boundary: 0.000134, weighted total: 0.00023291\n",
      "Epoch 85. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0016791716, boundary: 0.000163, weighted total: 0.00026261\n",
      "Epoch 86. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0016581970, boundary: 0.000129, weighted total: 0.00022972\n",
      "Epoch 87. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0016636569, boundary: 0.000159, weighted total: 0.00025812\n",
      "Epoch 88. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0015803321, boundary: 0.000125, weighted total: 0.00022071\n",
      "Epoch 89. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0016116287, boundary: 0.000153, weighted total: 0.00024939\n",
      "Epoch 90. Current alpha: 0.934173, lr: 0.0000084145, r_lower: 118.7. Training losses: pinn: 0.0016414394, boundary: 0.000122, weighted total: 0.00022174\n",
      "Epoch 91. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0015883317, boundary: 0.000151, weighted total: 0.00025557\n",
      "Epoch 92. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0015986061, boundary: 0.000120, weighted total: 0.00022781\n",
      "Epoch 93. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0015802991, boundary: 0.000148, weighted total: 0.00025227\n",
      "Epoch 94. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0015399956, boundary: 0.000119, weighted total: 0.00022228\n",
      "Epoch 95. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0015697605, boundary: 0.000145, weighted total: 0.00024859\n",
      "Epoch 96. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0015318188, boundary: 0.000118, weighted total: 0.00022057\n",
      "Epoch 97. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0015560215, boundary: 0.000141, weighted total: 0.00024383\n",
      "Epoch 98. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0014937159, boundary: 0.000116, weighted total: 0.00021675\n",
      "Epoch 99. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0015480875, boundary: 0.000139, weighted total: 0.00024177\n",
      "Epoch 100. Current alpha: 0.927132, lr: 0.0000084145, r_lower: 118.6. Training losses: pinn: 0.0014960313, boundary: 0.000115, weighted total: 0.00021524\n",
      "Epoch 101. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014662742, boundary: 0.000139, weighted total: 0.00024514\n",
      "Epoch 102. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0015024226, boundary: 0.000114, weighted total: 0.00022482\n",
      "Epoch 103. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014830562, boundary: 0.000140, weighted total: 0.00024718\n",
      "Epoch 104. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014272450, boundary: 0.000114, weighted total: 0.00021889\n",
      "Epoch 105. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014664929, boundary: 0.000138, weighted total: 0.00024397\n",
      "Epoch 106. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014092389, boundary: 0.000112, weighted total: 0.00021578\n",
      "Epoch 107. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014552112, boundary: 0.000136, weighted total: 0.00024169\n",
      "Epoch 108. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014513861, boundary: 0.000113, weighted total: 0.00021999\n",
      "Epoch 109. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014853896, boundary: 0.000136, weighted total: 0.00024333\n",
      "Epoch 110. Current alpha: 0.920144, lr: 0.0000084145, r_lower: 118.5. Training losses: pinn: 0.0014166573, boundary: 0.000112, weighted total: 0.00021573\n",
      "Epoch 111. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0014364460, boundary: 0.000138, weighted total: 0.00025040\n",
      "Epoch 112. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0014029581, boundary: 0.000112, weighted total: 0.00022442\n",
      "Epoch 113. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0013996428, boundary: 0.000139, weighted total: 0.00024802\n",
      "Epoch 114. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0014098037, boundary: 0.000111, weighted total: 0.00022413\n",
      "Epoch 115. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0013578383, boundary: 0.000138, weighted total: 0.00024427\n",
      "Epoch 116. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0013207508, boundary: 0.000112, weighted total: 0.00021678\n",
      "Epoch 117. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0013689336, boundary: 0.000139, weighted total: 0.00024586\n",
      "Epoch 118. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0013579755, boundary: 0.000112, weighted total: 0.00021984\n",
      "Epoch 119. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0013303262, boundary: 0.000138, weighted total: 0.00024143\n",
      "Epoch 120. Current alpha: 0.913208, lr: 0.0000084145, r_lower: 118.3. Training losses: pinn: 0.0013263250, boundary: 0.000111, weighted total: 0.00021652\n",
      "Epoch 121. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0013450080, boundary: 0.000139, weighted total: 0.00025159\n",
      "Epoch 122. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0013227522, boundary: 0.000113, weighted total: 0.00022612\n",
      "Epoch 123. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0013062322, boundary: 0.000139, weighted total: 0.00024802\n",
      "Epoch 124. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0012671799, boundary: 0.000111, weighted total: 0.00021952\n",
      "Epoch 125. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0012807788, boundary: 0.000140, weighted total: 0.00024694\n",
      "Epoch 126. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0012872602, boundary: 0.000112, weighted total: 0.00022181\n",
      "Epoch 127. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0012737832, boundary: 0.000138, weighted total: 0.00024406\n",
      "Epoch 128. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0012282359, boundary: 0.000111, weighted total: 0.00021569\n",
      "Epoch 129. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0012412120, boundary: 0.000140, weighted total: 0.00024350\n",
      "Epoch 130. Current alpha: 0.906325, lr: 0.0000084145, r_lower: 118.2. Training losses: pinn: 0.0012510792, boundary: 0.000110, weighted total: 0.00021692\n",
      "Epoch 131. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0012096996, boundary: 0.000142, weighted total: 0.00024952\n",
      "Epoch 132. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0012282575, boundary: 0.000110, weighted total: 0.00022263\n",
      "Epoch 133. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0012155473, boundary: 0.000144, weighted total: 0.00025186\n",
      "Epoch 134. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0011988521, boundary: 0.000111, weighted total: 0.00022044\n",
      "Epoch 135. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0012246782, boundary: 0.000141, weighted total: 0.00025034\n",
      "Epoch 136. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0011862476, boundary: 0.000112, weighted total: 0.00022014\n",
      "Epoch 137. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0011924310, boundary: 0.000141, weighted total: 0.00024696\n",
      "Epoch 138. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0012047161, boundary: 0.000112, weighted total: 0.00022144\n",
      "Epoch 139. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0011747435, boundary: 0.000142, weighted total: 0.00024556\n",
      "Epoch 140. Current alpha: 0.899494, lr: 0.0000084145, r_lower: 118.0. Training losses: pinn: 0.0011708372, boundary: 0.000112, weighted total: 0.00021839\n",
      "Epoch 141. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011667076, boundary: 0.000142, weighted total: 0.00025238\n",
      "Epoch 142. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011689635, boundary: 0.000113, weighted total: 0.00022586\n",
      "Epoch 143. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011767446, boundary: 0.000144, weighted total: 0.00025439\n",
      "Epoch 144. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011649798, boundary: 0.000112, weighted total: 0.00022512\n",
      "Epoch 145. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011584746, boundary: 0.000145, weighted total: 0.00025371\n",
      "Epoch 146. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011479239, boundary: 0.000114, weighted total: 0.00022482\n",
      "Epoch 147. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011537963, boundary: 0.000143, weighted total: 0.00025108\n",
      "Epoch 148. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011435971, boundary: 0.000114, weighted total: 0.00022449\n",
      "Epoch 149. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011505081, boundary: 0.000142, weighted total: 0.00025029\n",
      "Epoch 150. Current alpha: 0.892714, lr: 0.0000084145, r_lower: 117.9. Training losses: pinn: 0.0011313659, boundary: 0.000115, weighted total: 0.00022395\n",
      "Epoch 151. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0011599408, boundary: 0.000144, weighted total: 0.00025968\n",
      "Epoch 152. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0011158404, boundary: 0.000117, weighted total: 0.00023076\n",
      "Epoch 153. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0011242941, boundary: 0.000141, weighted total: 0.00025321\n",
      "Epoch 154. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0011087227, boundary: 0.000117, weighted total: 0.00023051\n",
      "Epoch 155. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0011079681, boundary: 0.000142, weighted total: 0.00025238\n",
      "Epoch 156. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0010755568, boundary: 0.000117, weighted total: 0.00022616\n",
      "Epoch 157. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0011097644, boundary: 0.000143, weighted total: 0.00025305\n",
      "Epoch 158. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0010706208, boundary: 0.000115, weighted total: 0.00022419\n",
      "Epoch 159. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0010877229, boundary: 0.000145, weighted total: 0.00025291\n",
      "Epoch 160. Current alpha: 0.885985, lr: 0.0000084145, r_lower: 117.8. Training losses: pinn: 0.0010812251, boundary: 0.000116, weighted total: 0.00022640\n",
      "Epoch 161. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0011279996, boundary: 0.000146, weighted total: 0.00026449\n",
      "Epoch 162. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010489479, boundary: 0.000116, weighted total: 0.00022880\n",
      "Epoch 163. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010718962, boundary: 0.000147, weighted total: 0.00025885\n",
      "Epoch 164. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010665501, boundary: 0.000116, weighted total: 0.00023075\n",
      "Epoch 165. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010918300, boundary: 0.000150, weighted total: 0.00026341\n",
      "Epoch 166. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010368057, boundary: 0.000117, weighted total: 0.00022811\n",
      "Epoch 167. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010410003, boundary: 0.000148, weighted total: 0.00025566\n",
      "Epoch 168. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010366146, boundary: 0.000117, weighted total: 0.00022793\n",
      "Epoch 169. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010474581, boundary: 0.000147, weighted total: 0.00025567\n",
      "Epoch 170. Current alpha: 0.879307, lr: 0.0000084145, r_lower: 117.6. Training losses: pinn: 0.0010625440, boundary: 0.000119, weighted total: 0.00023255\n",
      "Epoch 171. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010390359, boundary: 0.000149, weighted total: 0.00026199\n",
      "Epoch 172. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010118865, boundary: 0.000119, weighted total: 0.00023292\n",
      "Epoch 173. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010460186, boundary: 0.000147, weighted total: 0.00026144\n",
      "Epoch 174. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010343475, boundary: 0.000121, weighted total: 0.00023699\n",
      "Epoch 175. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010306451, boundary: 0.000147, weighted total: 0.00025961\n",
      "Epoch 176. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010077700, boundary: 0.000120, weighted total: 0.00023271\n",
      "Epoch 177. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010137220, boundary: 0.000150, weighted total: 0.00026027\n",
      "Epoch 178. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010009832, boundary: 0.000120, weighted total: 0.00023246\n",
      "Epoch 179. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010373285, boundary: 0.000150, weighted total: 0.00026304\n",
      "Epoch 180. Current alpha: 0.872680, lr: 0.0000084145, r_lower: 117.5. Training losses: pinn: 0.0010189259, boundary: 0.000120, weighted total: 0.00023478\n",
      "Epoch 181. Current alpha: 0.866102, lr: 0.0000084145, r_lower: 117.3. Training losses: pinn: 0.0009875361, boundary: 0.000153, weighted total: 0.00026431\n",
      "Epoch 182. Current alpha: 0.866102, lr: 0.0000084145, r_lower: 117.3. Training losses: pinn: 0.0009929034, boundary: 0.000120, weighted total: 0.00023706\n",
      "Epoch 183. Current alpha: 0.866102, lr: 0.0000084145, r_lower: 117.3. Training losses: pinn: 0.0009890473, boundary: 0.000153, weighted total: 0.00026504\n",
      "Epoch 184. Current alpha: 0.866102, lr: 0.0000084145, r_lower: 117.3. Training losses: pinn: 0.0009923875, boundary: 0.000120, weighted total: 0.00023684\n",
      "Epoch 185. Current alpha: 0.866102, lr: 0.0000084145, r_lower: 117.3. Training losses: pinn: 0.0009771504, boundary: 0.000154, weighted total: 0.00026387\n",
      "Epoch 186. Current alpha: 0.866102, lr: 0.0000084145, r_lower: 117.3. Training losses: pinn: 0.0009864753, boundary: 0.000120, weighted total: 0.00023588\n",
      "Epoch 187. Current alpha: 0.866102, lr: 0.0000084145, r_lower: 117.3. Training losses: pinn: 0.0010101385, boundary: 0.000156, weighted total: 0.00027011\n",
      "Epoch 188. Current alpha: 0.866102, lr: 0.0000079938, r_lower: 117.3. Training losses: pinn: 0.0009780298, boundary: 0.000114, weighted total: 0.00022932\n",
      "Epoch 189. Current alpha: 0.866102, lr: 0.0000079938, r_lower: 117.3. Training losses: pinn: 0.0009564148, boundary: 0.000147, weighted total: 0.00025512\n",
      "Epoch 190. Current alpha: 0.866102, lr: 0.0000079938, r_lower: 117.3. Training losses: pinn: 0.0009563714, boundary: 0.000114, weighted total: 0.00022709\n",
      "Epoch 191. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009694456, boundary: 0.000148, weighted total: 0.00026331\n",
      "Epoch 192. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009721394, boundary: 0.000117, weighted total: 0.00023691\n",
      "Epoch 193. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009508704, boundary: 0.000142, weighted total: 0.00025585\n",
      "Epoch 194. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009488971, boundary: 0.000119, weighted total: 0.00023523\n",
      "Epoch 195. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009364875, boundary: 0.000142, weighted total: 0.00025335\n",
      "Epoch 196. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009398213, boundary: 0.000118, weighted total: 0.00023306\n",
      "Epoch 197. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009402069, boundary: 0.000145, weighted total: 0.00025665\n",
      "Epoch 198. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009410055, boundary: 0.000117, weighted total: 0.00023312\n",
      "Epoch 199. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009293360, boundary: 0.000146, weighted total: 0.00025621\n",
      "Epoch 200. Current alpha: 0.859574, lr: 0.0000079938, r_lower: 117.2. Training losses: pinn: 0.0009148542, boundary: 0.000118, weighted total: 0.00023025\n",
      "Epoch 201. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0009228318, boundary: 0.000145, weighted total: 0.00025888\n",
      "Epoch 202. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0009082320, boundary: 0.000121, weighted total: 0.00023633\n",
      "Epoch 203. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0009031478, boundary: 0.000143, weighted total: 0.00025484\n",
      "Epoch 204. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0009066358, boundary: 0.000120, weighted total: 0.00023577\n",
      "Epoch 205. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0009224128, boundary: 0.000147, weighted total: 0.00026106\n",
      "Epoch 206. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0009048442, boundary: 0.000120, weighted total: 0.00023561\n",
      "Epoch 207. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0009316410, boundary: 0.000147, weighted total: 0.00026222\n",
      "Epoch 208. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0008996213, boundary: 0.000120, weighted total: 0.00023480\n",
      "Epoch 209. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0009101799, boundary: 0.000149, weighted total: 0.00026066\n",
      "Epoch 210. Current alpha: 0.853095, lr: 0.0000079938, r_lower: 117.1. Training losses: pinn: 0.0008989378, boundary: 0.000120, weighted total: 0.00023434\n",
      "Epoch 211. Current alpha: 0.846665, lr: 0.0000079938, r_lower: 116.9. Training losses: pinn: 0.0008833190, boundary: 0.000148, weighted total: 0.00026111\n",
      "Epoch 212. Current alpha: 0.846665, lr: 0.0000079938, r_lower: 116.9. Training losses: pinn: 0.0009097908, boundary: 0.000122, weighted total: 0.00024288\n",
      "Epoch 213. Current alpha: 0.846665, lr: 0.0000075941, r_lower: 116.9. Training losses: pinn: 0.0008839309, boundary: 0.000137, weighted total: 0.00025168\n",
      "Epoch 214. Current alpha: 0.846665, lr: 0.0000075941, r_lower: 116.9. Training losses: pinn: 0.0008783725, boundary: 0.000117, weighted total: 0.00023395\n",
      "Epoch 215. Current alpha: 0.846665, lr: 0.0000075941, r_lower: 116.9. Training losses: pinn: 0.0008848240, boundary: 0.000141, weighted total: 0.00025541\n",
      "Epoch 216. Current alpha: 0.846665, lr: 0.0000075941, r_lower: 116.9. Training losses: pinn: 0.0008762354, boundary: 0.000118, weighted total: 0.00023389\n",
      "Epoch 217. Current alpha: 0.846665, lr: 0.0000075941, r_lower: 116.9. Training losses: pinn: 0.0008776025, boundary: 0.000144, weighted total: 0.00025655\n",
      "Epoch 218. Current alpha: 0.846665, lr: 0.0000075941, r_lower: 116.9. Training losses: pinn: 0.0008676694, boundary: 0.000116, weighted total: 0.00023125\n",
      "Epoch 219. Current alpha: 0.846665, lr: 0.0000075941, r_lower: 116.9. Training losses: pinn: 0.0008613883, boundary: 0.000146, weighted total: 0.00025580\n",
      "Epoch 220. Current alpha: 0.846665, lr: 0.0000075941, r_lower: 116.9. Training losses: pinn: 0.0008334066, boundary: 0.000115, weighted total: 0.00022545\n",
      "Epoch 221. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008442276, boundary: 0.000146, weighted total: 0.00025739\n",
      "Epoch 222. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008479554, boundary: 0.000117, weighted total: 0.00023395\n",
      "Epoch 223. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008381664, boundary: 0.000145, weighted total: 0.00025560\n",
      "Epoch 224. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008367800, boundary: 0.000119, weighted total: 0.00023333\n",
      "Epoch 225. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008396232, boundary: 0.000143, weighted total: 0.00025459\n",
      "Epoch 226. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008413748, boundary: 0.000119, weighted total: 0.00023411\n",
      "Epoch 227. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008615925, boundary: 0.000143, weighted total: 0.00025766\n",
      "Epoch 228. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008171570, boundary: 0.000121, weighted total: 0.00023207\n",
      "Epoch 229. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008259566, boundary: 0.000140, weighted total: 0.00024978\n",
      "Epoch 230. Current alpha: 0.840283, lr: 0.0000075941, r_lower: 116.8. Training losses: pinn: 0.0008396538, boundary: 0.000121, weighted total: 0.00023607\n",
      "Epoch 231. Current alpha: 0.833949, lr: 0.0000072144, r_lower: 116.6. Training losses: pinn: 0.0008197999, boundary: 0.000134, weighted total: 0.00024829\n",
      "Epoch 232. Current alpha: 0.833949, lr: 0.0000072144, r_lower: 116.6. Training losses: pinn: 0.0008595777, boundary: 0.000118, weighted total: 0.00024080\n",
      "Epoch 233. Current alpha: 0.833949, lr: 0.0000068537, r_lower: 116.6. Training losses: pinn: 0.0008119161, boundary: 0.000129, weighted total: 0.00024223\n",
      "Epoch 234. Current alpha: 0.833949, lr: 0.0000068537, r_lower: 116.6. Training losses: pinn: 0.0008153297, boundary: 0.000112, weighted total: 0.00022846\n",
      "Epoch 235. Current alpha: 0.833949, lr: 0.0000068537, r_lower: 116.6. Training losses: pinn: 0.0008291302, boundary: 0.000131, weighted total: 0.00024665\n",
      "Epoch 236. Current alpha: 0.833949, lr: 0.0000068537, r_lower: 116.6. Training losses: pinn: 0.0008182589, boundary: 0.000112, weighted total: 0.00022926\n",
      "Epoch 237. Current alpha: 0.833949, lr: 0.0000068537, r_lower: 116.6. Training losses: pinn: 0.0008058084, boundary: 0.000131, weighted total: 0.00024287\n",
      "Epoch 238. Current alpha: 0.833949, lr: 0.0000068537, r_lower: 116.6. Training losses: pinn: 0.0007985697, boundary: 0.000113, weighted total: 0.00022680\n",
      "Epoch 239. Current alpha: 0.833949, lr: 0.0000068537, r_lower: 116.6. Training losses: pinn: 0.0008196455, boundary: 0.000131, weighted total: 0.00024534\n",
      "Epoch 240. Current alpha: 0.833949, lr: 0.0000068537, r_lower: 116.6. Training losses: pinn: 0.0007840011, boundary: 0.000112, weighted total: 0.00022367\n",
      "Epoch 241. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0008181379, boundary: 0.000132, weighted total: 0.00024991\n",
      "Epoch 242. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007731484, boundary: 0.000113, weighted total: 0.00022705\n",
      "Epoch 243. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007731506, boundary: 0.000130, weighted total: 0.00024047\n",
      "Epoch 244. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007887841, boundary: 0.000114, weighted total: 0.00023046\n",
      "Epoch 245. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007751798, boundary: 0.000130, weighted total: 0.00024141\n",
      "Epoch 246. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007805642, boundary: 0.000114, weighted total: 0.00022886\n",
      "Epoch 247. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007913038, boundary: 0.000131, weighted total: 0.00024461\n",
      "Epoch 248. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007596616, boundary: 0.000112, weighted total: 0.00022392\n",
      "Epoch 249. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007975932, boundary: 0.000133, weighted total: 0.00024739\n",
      "Epoch 250. Current alpha: 0.827664, lr: 0.0000068537, r_lower: 116.5. Training losses: pinn: 0.0007485252, boundary: 0.000113, weighted total: 0.00022223\n",
      "Epoch 251. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007433361, boundary: 0.000131, weighted total: 0.00024040\n",
      "Epoch 252. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007632477, boundary: 0.000113, weighted total: 0.00022914\n",
      "Epoch 253. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007504868, boundary: 0.000130, weighted total: 0.00024120\n",
      "Epoch 254. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007352276, boundary: 0.000114, weighted total: 0.00022484\n",
      "Epoch 255. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007435840, boundary: 0.000129, weighted total: 0.00023872\n",
      "Epoch 256. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007477784, boundary: 0.000113, weighted total: 0.00022648\n",
      "Epoch 257. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007463787, boundary: 0.000129, weighted total: 0.00023945\n",
      "Epoch 258. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007395139, boundary: 0.000113, weighted total: 0.00022455\n",
      "Epoch 259. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007415401, boundary: 0.000129, weighted total: 0.00023824\n",
      "Epoch 260. Current alpha: 0.821425, lr: 0.0000068537, r_lower: 116.4. Training losses: pinn: 0.0007326482, boundary: 0.000112, weighted total: 0.00022256\n",
      "Epoch 261. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007260480, boundary: 0.000130, weighted total: 0.00024019\n",
      "Epoch 262. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007133783, boundary: 0.000112, weighted total: 0.00022286\n",
      "Epoch 263. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007144540, boundary: 0.000129, weighted total: 0.00023740\n",
      "Epoch 264. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007078436, boundary: 0.000111, weighted total: 0.00022106\n",
      "Epoch 265. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007196584, boundary: 0.000130, weighted total: 0.00023929\n",
      "Epoch 266. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007123121, boundary: 0.000109, weighted total: 0.00022081\n",
      "Epoch 267. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007161060, boundary: 0.000129, weighted total: 0.00023738\n",
      "Epoch 268. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007098613, boundary: 0.000110, weighted total: 0.00022052\n",
      "Epoch 269. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0007092891, boundary: 0.000128, weighted total: 0.00023514\n",
      "Epoch 270. Current alpha: 0.815234, lr: 0.0000068537, r_lower: 116.2. Training losses: pinn: 0.0006837243, boundary: 0.000109, weighted total: 0.00021502\n",
      "Epoch 271. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006978114, boundary: 0.000130, weighted total: 0.00023808\n",
      "Epoch 272. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006836658, boundary: 0.000108, weighted total: 0.00021801\n",
      "Epoch 273. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006856165, boundary: 0.000129, weighted total: 0.00023514\n",
      "Epoch 274. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006828670, boundary: 0.000108, weighted total: 0.00021749\n",
      "Epoch 275. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006839422, boundary: 0.000127, weighted total: 0.00023338\n",
      "Epoch 276. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006802941, boundary: 0.000109, weighted total: 0.00021785\n",
      "Epoch 277. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006721623, boundary: 0.000124, weighted total: 0.00022898\n",
      "Epoch 278. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006488502, boundary: 0.000107, weighted total: 0.00021066\n",
      "Epoch 279. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006519139, boundary: 0.000123, weighted total: 0.00022367\n",
      "Epoch 280. Current alpha: 0.809089, lr: 0.0000068537, r_lower: 116.1. Training losses: pinn: 0.0006574809, boundary: 0.000106, weighted total: 0.00021106\n",
      "Epoch 281. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006687152, boundary: 0.000124, weighted total: 0.00023165\n",
      "Epoch 282. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006454712, boundary: 0.000105, weighted total: 0.00021171\n",
      "Epoch 283. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006667334, boundary: 0.000123, weighted total: 0.00023022\n",
      "Epoch 284. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006371651, boundary: 0.000104, weighted total: 0.00020937\n",
      "Epoch 285. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006370336, boundary: 0.000123, weighted total: 0.00022441\n",
      "Epoch 286. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006397945, boundary: 0.000104, weighted total: 0.00020963\n",
      "Epoch 287. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006404960, boundary: 0.000122, weighted total: 0.00022406\n",
      "Epoch 288. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006455319, boundary: 0.000102, weighted total: 0.00020943\n",
      "Epoch 289. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006421162, boundary: 0.000122, weighted total: 0.00022414\n",
      "Epoch 290. Current alpha: 0.802991, lr: 0.0000068537, r_lower: 116.0. Training losses: pinn: 0.0006465199, boundary: 0.000103, weighted total: 0.00021032\n",
      "Epoch 291. Current alpha: 0.796938, lr: 0.0000068537, r_lower: 115.8. Training losses: pinn: 0.0006403896, boundary: 0.000119, weighted total: 0.00022454\n",
      "Epoch 292. Current alpha: 0.796938, lr: 0.0000068537, r_lower: 115.8. Training losses: pinn: 0.0006303894, boundary: 0.000104, weighted total: 0.00021116\n",
      "Epoch 293. Current alpha: 0.796938, lr: 0.0000068537, r_lower: 115.8. Training losses: pinn: 0.0006279442, boundary: 0.000117, weighted total: 0.00022092\n",
      "Epoch 294. Current alpha: 0.796938, lr: 0.0000068537, r_lower: 115.8. Training losses: pinn: 0.0006343889, boundary: 0.000103, weighted total: 0.00021117\n",
      "Epoch 295. Current alpha: 0.796938, lr: 0.0000068537, r_lower: 115.8. Training losses: pinn: 0.0006513269, boundary: 0.000117, weighted total: 0.00022566\n",
      "Epoch 296. Current alpha: 0.796938, lr: 0.0000065110, r_lower: 115.8. Training losses: pinn: 0.0006332890, boundary: 0.000097, weighted total: 0.00020597\n",
      "Epoch 297. Current alpha: 0.796938, lr: 0.0000065110, r_lower: 115.8. Training losses: pinn: 0.0006305080, boundary: 0.000113, weighted total: 0.00021827\n",
      "Epoch 298. Current alpha: 0.796938, lr: 0.0000065110, r_lower: 115.8. Training losses: pinn: 0.0006217471, boundary: 0.000098, weighted total: 0.00020422\n",
      "Epoch 299. Current alpha: 0.796938, lr: 0.0000065110, r_lower: 115.8. Training losses: pinn: 0.0006463470, boundary: 0.000112, weighted total: 0.00022079\n",
      "Epoch 300. Current alpha: 0.796938, lr: 0.0000065110, r_lower: 115.8. Training losses: pinn: 0.0006206251, boundary: 0.000097, weighted total: 0.00020359\n",
      "Epoch 301. Current alpha: 0.790931, lr: 0.0000065110, r_lower: 115.7. Training losses: pinn: 0.0006272025, boundary: 0.000115, weighted total: 0.00022238\n",
      "Epoch 302. Current alpha: 0.790931, lr: 0.0000065110, r_lower: 115.7. Training losses: pinn: 0.0006306457, boundary: 0.000098, weighted total: 0.00020903\n",
      "Epoch 303. Current alpha: 0.790931, lr: 0.0000065110, r_lower: 115.7. Training losses: pinn: 0.0006350348, boundary: 0.000117, weighted total: 0.00022538\n",
      "Epoch 304. Current alpha: 0.790931, lr: 0.0000061854, r_lower: 115.7. Training losses: pinn: 0.0006183176, boundary: 0.000093, weighted total: 0.00020314\n",
      "Epoch 305. Current alpha: 0.790931, lr: 0.0000061854, r_lower: 115.7. Training losses: pinn: 0.0006061582, boundary: 0.000112, weighted total: 0.00021536\n",
      "Epoch 306. Current alpha: 0.790931, lr: 0.0000061854, r_lower: 115.7. Training losses: pinn: 0.0006219589, boundary: 0.000094, weighted total: 0.00020461\n",
      "Epoch 307. Current alpha: 0.790931, lr: 0.0000061854, r_lower: 115.7. Training losses: pinn: 0.0006395545, boundary: 0.000113, weighted total: 0.00022292\n",
      "Epoch 308. Current alpha: 0.790931, lr: 0.0000058762, r_lower: 115.7. Training losses: pinn: 0.0006299133, boundary: 0.000091, weighted total: 0.00020385\n",
      "Epoch 309. Current alpha: 0.790931, lr: 0.0000055824, r_lower: 115.7. Training losses: pinn: 0.0006233097, boundary: 0.000103, weighted total: 0.00021162\n",
      "Epoch 310. Current alpha: 0.790931, lr: 0.0000055824, r_lower: 115.7. Training losses: pinn: 0.0006278073, boundary: 0.000090, weighted total: 0.00020222\n",
      "Epoch 311. Current alpha: 0.784970, lr: 0.0000055824, r_lower: 115.6. Training losses: pinn: 0.0006381041, boundary: 0.000105, weighted total: 0.00021961\n",
      "Epoch 312. Current alpha: 0.784970, lr: 0.0000053032, r_lower: 115.6. Training losses: pinn: 0.0006268416, boundary: 0.000089, weighted total: 0.00020463\n",
      "Epoch 313. Current alpha: 0.784970, lr: 0.0000053032, r_lower: 115.6. Training losses: pinn: 0.0006438329, boundary: 0.000102, weighted total: 0.00021853\n",
      "Epoch 314. Current alpha: 0.784970, lr: 0.0000053032, r_lower: 115.6. Training losses: pinn: 0.0006423733, boundary: 0.000090, weighted total: 0.00020883\n",
      "Epoch 315. Current alpha: 0.784970, lr: 0.0000050381, r_lower: 115.6. Training losses: pinn: 0.0006379552, boundary: 0.000100, weighted total: 0.00021533\n",
      "Epoch 316. Current alpha: 0.784970, lr: 0.0000047862, r_lower: 115.6. Training losses: pinn: 0.0006354956, boundary: 0.000087, weighted total: 0.00020482\n",
      "Epoch 317. Current alpha: 0.784970, lr: 0.0000045469, r_lower: 115.6. Training losses: pinn: 0.0006347317, boundary: 0.000093, weighted total: 0.00020914\n",
      "Epoch 318. Current alpha: 0.784970, lr: 0.0000045469, r_lower: 115.6. Training losses: pinn: 0.0006334493, boundary: 0.000087, weighted total: 0.00020416\n",
      "Epoch 319. Current alpha: 0.784970, lr: 0.0000045469, r_lower: 115.6. Training losses: pinn: 0.0006438919, boundary: 0.000093, weighted total: 0.00021147\n",
      "Epoch 320. Current alpha: 0.784970, lr: 0.0000043195, r_lower: 115.6. Training losses: pinn: 0.0006493162, boundary: 0.000084, weighted total: 0.00020582\n",
      "Epoch 321. Current alpha: 0.779053, lr: 0.0000041035, r_lower: 115.4. Training losses: pinn: 0.0006308907, boundary: 0.000090, weighted total: 0.00020984\n",
      "Epoch 322. Current alpha: 0.779053, lr: 0.0000041035, r_lower: 115.4. Training losses: pinn: 0.0006325797, boundary: 0.000084, weighted total: 0.00020552\n",
      "Epoch 323. Current alpha: 0.779053, lr: 0.0000038984, r_lower: 115.4. Training losses: pinn: 0.0006337003, boundary: 0.000089, weighted total: 0.00020946\n",
      "Epoch 324. Current alpha: 0.779053, lr: 0.0000038984, r_lower: 115.4. Training losses: pinn: 0.0006275510, boundary: 0.000084, weighted total: 0.00020386\n",
      "Epoch 325. Current alpha: 0.779053, lr: 0.0000038984, r_lower: 115.4. Training losses: pinn: 0.0006370736, boundary: 0.000090, weighted total: 0.00021066\n",
      "Epoch 326. Current alpha: 0.779053, lr: 0.0000038984, r_lower: 115.4. Training losses: pinn: 0.0006421665, boundary: 0.000086, weighted total: 0.00020857\n",
      "Epoch 327. Current alpha: 0.779053, lr: 0.0000037034, r_lower: 115.4. Training losses: pinn: 0.0006438011, boundary: 0.000088, weighted total: 0.00021117\n",
      "Epoch 328. Current alpha: 0.779053, lr: 0.0000035183, r_lower: 115.4. Training losses: pinn: 0.0006404242, boundary: 0.000084, weighted total: 0.00020673\n",
      "Epoch 329. Current alpha: 0.779053, lr: 0.0000033424, r_lower: 115.4. Training losses: pinn: 0.0006632609, boundary: 0.000086, weighted total: 0.00021380\n",
      "Epoch 330. Current alpha: 0.779053, lr: 0.0000031752, r_lower: 115.4. Training losses: pinn: 0.0006703338, boundary: 0.000084, weighted total: 0.00021390\n",
      "Epoch 331. Current alpha: 0.773181, lr: 0.0000030165, r_lower: 115.3. Training losses: pinn: 0.0006622418, boundary: 0.000086, weighted total: 0.00021657\n",
      "Epoch 332. Current alpha: 0.773181, lr: 0.0000028657, r_lower: 115.3. Training losses: pinn: 0.0006286214, boundary: 0.000083, weighted total: 0.00020660\n",
      "Epoch 333. Current alpha: 0.773181, lr: 0.0000028657, r_lower: 115.3. Training losses: pinn: 0.0006484898, boundary: 0.000086, weighted total: 0.00021368\n",
      "Epoch 334. Current alpha: 0.773181, lr: 0.0000027224, r_lower: 115.3. Training losses: pinn: 0.0006101716, boundary: 0.000083, weighted total: 0.00020234\n",
      "Epoch 335. Current alpha: 0.773181, lr: 0.0000027224, r_lower: 115.3. Training losses: pinn: 0.0006286966, boundary: 0.000084, weighted total: 0.00020787\n",
      "Epoch 336. Current alpha: 0.773181, lr: 0.0000027224, r_lower: 115.3. Training losses: pinn: 0.0006282023, boundary: 0.000084, weighted total: 0.00020750\n",
      "Epoch 337. Current alpha: 0.773181, lr: 0.0000027224, r_lower: 115.3. Training losses: pinn: 0.0006035664, boundary: 0.000084, weighted total: 0.00020212\n",
      "Epoch 338. Current alpha: 0.773181, lr: 0.0000027224, r_lower: 115.3. Training losses: pinn: 0.0006087358, boundary: 0.000083, weighted total: 0.00020257\n",
      "Epoch 339. Current alpha: 0.773181, lr: 0.0000027224, r_lower: 115.3. Training losses: pinn: 0.0006145963, boundary: 0.000085, weighted total: 0.00020526\n",
      "Epoch 340. Current alpha: 0.773181, lr: 0.0000027224, r_lower: 115.3. Training losses: pinn: 0.0006073395, boundary: 0.000084, weighted total: 0.00020245\n",
      "Epoch 341. Current alpha: 0.767353, lr: 0.0000027224, r_lower: 115.1. Training losses: pinn: 0.0006001484, boundary: 0.000085, weighted total: 0.00020513\n",
      "Epoch 342. Current alpha: 0.767353, lr: 0.0000027224, r_lower: 115.1. Training losses: pinn: 0.0006020414, boundary: 0.000084, weighted total: 0.00020490\n",
      "Epoch 343. Current alpha: 0.767353, lr: 0.0000027224, r_lower: 115.1. Training losses: pinn: 0.0006098681, boundary: 0.000087, weighted total: 0.00020849\n",
      "Epoch 344. Current alpha: 0.767353, lr: 0.0000027224, r_lower: 115.1. Training losses: pinn: 0.0005905413, boundary: 0.000085, weighted total: 0.00020261\n",
      "Epoch 345. Current alpha: 0.767353, lr: 0.0000027224, r_lower: 115.1. Training losses: pinn: 0.0005924315, boundary: 0.000086, weighted total: 0.00020394\n",
      "Epoch 346. Current alpha: 0.767353, lr: 0.0000027224, r_lower: 115.1. Training losses: pinn: 0.0005857045, boundary: 0.000085, weighted total: 0.00020169\n",
      "Epoch 347. Current alpha: 0.767353, lr: 0.0000027224, r_lower: 115.1. Training losses: pinn: 0.0006030932, boundary: 0.000087, weighted total: 0.00020682\n",
      "Epoch 348. Current alpha: 0.767353, lr: 0.0000025863, r_lower: 115.1. Training losses: pinn: 0.0005820887, boundary: 0.000084, weighted total: 0.00020018\n",
      "Epoch 349. Current alpha: 0.767353, lr: 0.0000025863, r_lower: 115.1. Training losses: pinn: 0.0005920416, boundary: 0.000086, weighted total: 0.00020379\n",
      "Epoch 350. Current alpha: 0.767353, lr: 0.0000025863, r_lower: 115.1. Training losses: pinn: 0.0005885213, boundary: 0.000085, weighted total: 0.00020194\n",
      "Epoch 351. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005852877, boundary: 0.000087, weighted total: 0.00020571\n",
      "Epoch 352. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005648329, boundary: 0.000086, weighted total: 0.00019980\n",
      "Epoch 353. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005831332, boundary: 0.000087, weighted total: 0.00020527\n",
      "Epoch 354. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005735282, boundary: 0.000086, weighted total: 0.00020222\n",
      "Epoch 355. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005845995, boundary: 0.000087, weighted total: 0.00020599\n",
      "Epoch 356. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005758725, boundary: 0.000087, weighted total: 0.00020329\n",
      "Epoch 357. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005623025, boundary: 0.000087, weighted total: 0.00020036\n",
      "Epoch 358. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005694023, boundary: 0.000086, weighted total: 0.00020141\n",
      "Epoch 359. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005688018, boundary: 0.000087, weighted total: 0.00020165\n",
      "Epoch 360. Current alpha: 0.761570, lr: 0.0000025863, r_lower: 115.0. Training losses: pinn: 0.0005673217, boundary: 0.000087, weighted total: 0.00020136\n",
      "Epoch 361. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005628381, boundary: 0.000088, weighted total: 0.00020378\n",
      "Epoch 362. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005588997, boundary: 0.000087, weighted total: 0.00020231\n",
      "Epoch 363. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005651984, boundary: 0.000087, weighted total: 0.00020395\n",
      "Epoch 364. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005452624, boundary: 0.000087, weighted total: 0.00019901\n",
      "Epoch 365. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005592940, boundary: 0.000087, weighted total: 0.00020254\n",
      "Epoch 366. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005429821, boundary: 0.000087, weighted total: 0.00019832\n",
      "Epoch 367. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005565999, boundary: 0.000088, weighted total: 0.00020240\n",
      "Epoch 368. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005387111, boundary: 0.000087, weighted total: 0.00019727\n",
      "Epoch 369. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005549524, boundary: 0.000087, weighted total: 0.00020138\n",
      "Epoch 370. Current alpha: 0.755829, lr: 0.0000025863, r_lower: 114.9. Training losses: pinn: 0.0005360933, boundary: 0.000087, weighted total: 0.00019666\n",
      "Epoch 371. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005354360, boundary: 0.000087, weighted total: 0.00019927\n",
      "Epoch 372. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005517926, boundary: 0.000088, weighted total: 0.00020385\n",
      "Epoch 373. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005286503, boundary: 0.000087, weighted total: 0.00019741\n",
      "Epoch 374. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005373848, boundary: 0.000088, weighted total: 0.00020011\n",
      "Epoch 375. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005370633, boundary: 0.000088, weighted total: 0.00020012\n",
      "Epoch 376. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005307018, boundary: 0.000087, weighted total: 0.00019779\n",
      "Epoch 377. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005242926, boundary: 0.000087, weighted total: 0.00019635\n",
      "Epoch 378. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005298567, boundary: 0.000087, weighted total: 0.00019781\n",
      "Epoch 379. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005294872, boundary: 0.000087, weighted total: 0.00019744\n",
      "Epoch 380. Current alpha: 0.750132, lr: 0.0000025863, r_lower: 114.7. Training losses: pinn: 0.0005383163, boundary: 0.000088, weighted total: 0.00020022\n",
      "Epoch 381. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005169207, boundary: 0.000085, weighted total: 0.00019563\n",
      "Epoch 382. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005080565, boundary: 0.000086, weighted total: 0.00019418\n",
      "Epoch 383. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005284467, boundary: 0.000087, weighted total: 0.00019964\n",
      "Epoch 384. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005250482, boundary: 0.000087, weighted total: 0.00019877\n",
      "Epoch 385. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005293815, boundary: 0.000086, weighted total: 0.00019917\n",
      "Epoch 386. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005111960, boundary: 0.000087, weighted total: 0.00019545\n",
      "Epoch 387. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005056660, boundary: 0.000085, weighted total: 0.00019231\n",
      "Epoch 388. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005128966, boundary: 0.000087, weighted total: 0.00019555\n",
      "Epoch 389. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005220072, boundary: 0.000085, weighted total: 0.00019668\n",
      "Epoch 390. Current alpha: 0.744478, lr: 0.0000024569, r_lower: 114.6. Training losses: pinn: 0.0005148015, boundary: 0.000087, weighted total: 0.00019606\n",
      "Epoch 391. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0005026516, boundary: 0.000085, weighted total: 0.00019401\n",
      "Epoch 392. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0005001812, boundary: 0.000086, weighted total: 0.00019418\n",
      "Epoch 393. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0005087835, boundary: 0.000085, weighted total: 0.00019556\n",
      "Epoch 394. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0005042802, boundary: 0.000086, weighted total: 0.00019535\n",
      "Epoch 395. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0004965956, boundary: 0.000085, weighted total: 0.00019216\n",
      "Epoch 396. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0004984592, boundary: 0.000085, weighted total: 0.00019333\n",
      "Epoch 397. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0004906376, boundary: 0.000085, weighted total: 0.00019085\n",
      "Epoch 398. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0005039057, boundary: 0.000085, weighted total: 0.00019420\n",
      "Epoch 399. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0005066791, boundary: 0.000086, weighted total: 0.00019589\n",
      "Epoch 400. Current alpha: 0.738867, lr: 0.0000024569, r_lower: 114.5. Training losses: pinn: 0.0004928580, boundary: 0.000085, weighted total: 0.00019133\n",
      "Epoch 401. Current alpha: 0.733298, lr: 0.0000024569, r_lower: 114.3. Training losses: pinn: 0.0005046547, boundary: 0.000086, weighted total: 0.00019758\n",
      "Epoch 402. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004860537, boundary: 0.000085, weighted total: 0.00019161\n",
      "Epoch 403. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004897922, boundary: 0.000084, weighted total: 0.00019238\n",
      "Epoch 404. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004876053, boundary: 0.000084, weighted total: 0.00019156\n",
      "Epoch 405. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004851593, boundary: 0.000083, weighted total: 0.00019055\n",
      "Epoch 406. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004950429, boundary: 0.000084, weighted total: 0.00019384\n",
      "Epoch 407. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004834702, boundary: 0.000083, weighted total: 0.00018984\n",
      "Epoch 408. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004933856, boundary: 0.000084, weighted total: 0.00019330\n",
      "Epoch 409. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004886193, boundary: 0.000083, weighted total: 0.00019122\n",
      "Epoch 410. Current alpha: 0.733298, lr: 0.0000023341, r_lower: 114.3. Training losses: pinn: 0.0004741871, boundary: 0.000083, weighted total: 0.00018746\n",
      "Epoch 411. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004801076, boundary: 0.000083, weighted total: 0.00019125\n",
      "Epoch 412. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004848052, boundary: 0.000084, weighted total: 0.00019287\n",
      "Epoch 413. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004655524, boundary: 0.000082, weighted total: 0.00018655\n",
      "Epoch 414. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004833898, boundary: 0.000083, weighted total: 0.00019227\n",
      "Epoch 415. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004772523, boundary: 0.000082, weighted total: 0.00018979\n",
      "Epoch 416. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004767963, boundary: 0.000083, weighted total: 0.00019011\n",
      "Epoch 417. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004663763, boundary: 0.000081, weighted total: 0.00018593\n",
      "Epoch 418. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004723679, boundary: 0.000083, weighted total: 0.00018874\n",
      "Epoch 419. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004796815, boundary: 0.000082, weighted total: 0.00018998\n",
      "Epoch 420. Current alpha: 0.727771, lr: 0.0000023341, r_lower: 114.2. Training losses: pinn: 0.0004760793, boundary: 0.000082, weighted total: 0.00018961\n",
      "Epoch 421. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004737024, boundary: 0.000081, weighted total: 0.00019028\n",
      "Epoch 422. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004575454, boundary: 0.000082, weighted total: 0.00018597\n",
      "Epoch 423. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004636820, boundary: 0.000080, weighted total: 0.00018682\n",
      "Epoch 424. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004701259, boundary: 0.000082, weighted total: 0.00018948\n",
      "Epoch 425. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004751559, boundary: 0.000081, weighted total: 0.00019042\n",
      "Epoch 426. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004639113, boundary: 0.000081, weighted total: 0.00018752\n",
      "Epoch 427. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004562210, boundary: 0.000080, weighted total: 0.00018434\n",
      "Epoch 428. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004566826, boundary: 0.000080, weighted total: 0.00018483\n",
      "Epoch 429. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004611468, boundary: 0.000080, weighted total: 0.00018572\n",
      "Epoch 430. Current alpha: 0.722285, lr: 0.0000022174, r_lower: 114.1. Training losses: pinn: 0.0004632556, boundary: 0.000080, weighted total: 0.00018633\n",
      "Epoch 431. Current alpha: 0.716841, lr: 0.0000022174, r_lower: 113.9. Training losses: pinn: 0.0004630163, boundary: 0.000080, weighted total: 0.00018857\n",
      "Epoch 432. Current alpha: 0.716841, lr: 0.0000022174, r_lower: 113.9. Training losses: pinn: 0.0004628566, boundary: 0.000080, weighted total: 0.00018867\n",
      "Epoch 433. Current alpha: 0.716841, lr: 0.0000021065, r_lower: 113.9. Training losses: pinn: 0.0004632451, boundary: 0.000080, weighted total: 0.00018858\n",
      "Epoch 434. Current alpha: 0.716841, lr: 0.0000021065, r_lower: 113.9. Training losses: pinn: 0.0004607207, boundary: 0.000080, weighted total: 0.00018767\n",
      "Epoch 435. Current alpha: 0.716841, lr: 0.0000021065, r_lower: 113.9. Training losses: pinn: 0.0004769125, boundary: 0.000081, weighted total: 0.00019308\n",
      "Epoch 436. Current alpha: 0.716841, lr: 0.0000020012, r_lower: 113.9. Training losses: pinn: 0.0004648306, boundary: 0.000080, weighted total: 0.00018861\n",
      "Epoch 437. Current alpha: 0.716841, lr: 0.0000020012, r_lower: 113.9. Training losses: pinn: 0.0004559760, boundary: 0.000080, weighted total: 0.00018613\n",
      "Epoch 438. Current alpha: 0.716841, lr: 0.0000020012, r_lower: 113.9. Training losses: pinn: 0.0004602973, boundary: 0.000079, weighted total: 0.00018702\n",
      "Epoch 439. Current alpha: 0.716841, lr: 0.0000019011, r_lower: 113.9. Training losses: pinn: 0.0004655767, boundary: 0.000079, weighted total: 0.00018845\n",
      "Epoch 440. Current alpha: 0.716841, lr: 0.0000018061, r_lower: 113.9. Training losses: pinn: 0.0004753877, boundary: 0.000078, weighted total: 0.00019069\n",
      "Epoch 441. Current alpha: 0.711438, lr: 0.0000017158, r_lower: 113.8. Training losses: pinn: 0.0004599972, boundary: 0.000078, weighted total: 0.00018842\n",
      "Epoch 442. Current alpha: 0.711438, lr: 0.0000017158, r_lower: 113.8. Training losses: pinn: 0.0004628470, boundary: 0.000078, weighted total: 0.00018934\n",
      "Epoch 443. Current alpha: 0.711438, lr: 0.0000017158, r_lower: 113.8. Training losses: pinn: 0.0004671236, boundary: 0.000078, weighted total: 0.00019028\n",
      "Epoch 444. Current alpha: 0.711438, lr: 0.0000016300, r_lower: 113.8. Training losses: pinn: 0.0004686820, boundary: 0.000078, weighted total: 0.00019099\n",
      "Epoch 445. Current alpha: 0.711438, lr: 0.0000015485, r_lower: 113.8. Training losses: pinn: 0.0004740448, boundary: 0.000078, weighted total: 0.00019215\n",
      "Epoch 446. Current alpha: 0.711438, lr: 0.0000015485, r_lower: 113.8. Training losses: pinn: 0.0004644979, boundary: 0.000078, weighted total: 0.00018926\n",
      "Epoch 447. Current alpha: 0.711438, lr: 0.0000015485, r_lower: 113.8. Training losses: pinn: 0.0004790395, boundary: 0.000078, weighted total: 0.00019364\n",
      "Epoch 448. Current alpha: 0.711438, lr: 0.0000014711, r_lower: 113.8. Training losses: pinn: 0.0004726367, boundary: 0.000077, weighted total: 0.00019131\n",
      "Epoch 449. Current alpha: 0.711438, lr: 0.0000013975, r_lower: 113.8. Training losses: pinn: 0.0004770351, boundary: 0.000076, weighted total: 0.00019138\n",
      "Epoch 450. Current alpha: 0.711438, lr: 0.0000013276, r_lower: 113.8. Training losses: pinn: 0.0004813790, boundary: 0.000076, weighted total: 0.00019293\n",
      "Epoch 451. Current alpha: 0.706075, lr: 0.0000012613, r_lower: 113.7. Training losses: pinn: 0.0004762392, boundary: 0.000077, weighted total: 0.00019414\n",
      "Epoch 452. Current alpha: 0.706075, lr: 0.0000011982, r_lower: 113.7. Training losses: pinn: 0.0004693730, boundary: 0.000077, weighted total: 0.00019217\n",
      "Epoch 453. Current alpha: 0.706075, lr: 0.0000011383, r_lower: 113.7. Training losses: pinn: 0.0004738403, boundary: 0.000077, weighted total: 0.00019351\n",
      "Epoch 454. Current alpha: 0.706075, lr: 0.0000010814, r_lower: 113.7. Training losses: pinn: 0.0004724550, boundary: 0.000077, weighted total: 0.00019292\n",
      "Epoch 455. Current alpha: 0.706075, lr: 0.0000010273, r_lower: 113.7. Training losses: pinn: 0.0004790047, boundary: 0.000076, weighted total: 0.00019427\n",
      "Epoch 456. Current alpha: 0.706075, lr: 0.0000009759, r_lower: 113.7. Training losses: pinn: 0.0004743668, boundary: 0.000076, weighted total: 0.00019302\n",
      "Epoch 457. Current alpha: 0.706075, lr: 0.0000009271, r_lower: 113.7. Training losses: pinn: 0.0004761128, boundary: 0.000075, weighted total: 0.00019322\n",
      "Epoch 458. Current alpha: 0.706075, lr: 0.0000009271, r_lower: 113.7. Training losses: pinn: 0.0004719151, boundary: 0.000075, weighted total: 0.00019192\n",
      "Epoch 459. Current alpha: 0.706075, lr: 0.0000009271, r_lower: 113.7. Training losses: pinn: 0.0004684922, boundary: 0.000075, weighted total: 0.00019037\n",
      "Epoch 460. Current alpha: 0.706075, lr: 0.0000009271, r_lower: 113.7. Training losses: pinn: 0.0004739442, boundary: 0.000074, weighted total: 0.00019168\n",
      "Epoch 461. Current alpha: 0.700753, lr: 0.0000009271, r_lower: 113.5. Training losses: pinn: 0.0004705930, boundary: 0.000075, weighted total: 0.00019353\n",
      "Epoch 462. Current alpha: 0.700753, lr: 0.0000009271, r_lower: 113.5. Training losses: pinn: 0.0004607291, boundary: 0.000075, weighted total: 0.00019019\n",
      "Epoch 463. Current alpha: 0.700753, lr: 0.0000009271, r_lower: 113.5. Training losses: pinn: 0.0004620087, boundary: 0.000075, weighted total: 0.00019060\n",
      "Epoch 464. Current alpha: 0.700753, lr: 0.0000009271, r_lower: 113.5. Training losses: pinn: 0.0004771274, boundary: 0.000075, weighted total: 0.00019549\n",
      "Epoch 465. Current alpha: 0.700753, lr: 0.0000008808, r_lower: 113.5. Training losses: pinn: 0.0004697498, boundary: 0.000075, weighted total: 0.00019306\n",
      "Epoch 466. Current alpha: 0.700753, lr: 0.0000008808, r_lower: 113.5. Training losses: pinn: 0.0004679189, boundary: 0.000075, weighted total: 0.00019249\n",
      "Epoch 467. Current alpha: 0.700753, lr: 0.0000008808, r_lower: 113.5. Training losses: pinn: 0.0004567069, boundary: 0.000074, weighted total: 0.00018835\n",
      "Epoch 468. Current alpha: 0.700753, lr: 0.0000008808, r_lower: 113.5. Training losses: pinn: 0.0004633328, boundary: 0.000074, weighted total: 0.00019069\n",
      "Epoch 469. Current alpha: 0.700753, lr: 0.0000008808, r_lower: 113.5. Training losses: pinn: 0.0004649360, boundary: 0.000074, weighted total: 0.00019076\n",
      "Epoch 470. Current alpha: 0.700753, lr: 0.0000008808, r_lower: 113.5. Training losses: pinn: 0.0004561564, boundary: 0.000073, weighted total: 0.00018800\n",
      "Epoch 471. Current alpha: 0.695472, lr: 0.0000008808, r_lower: 113.4. Training losses: pinn: 0.0004574100, boundary: 0.000074, weighted total: 0.00019061\n",
      "Epoch 472. Current alpha: 0.695472, lr: 0.0000008808, r_lower: 113.4. Training losses: pinn: 0.0004638191, boundary: 0.000074, weighted total: 0.00019267\n",
      "Epoch 473. Current alpha: 0.695472, lr: 0.0000008367, r_lower: 113.4. Training losses: pinn: 0.0004574279, boundary: 0.000073, weighted total: 0.00019039\n",
      "Epoch 474. Current alpha: 0.695472, lr: 0.0000008367, r_lower: 113.4. Training losses: pinn: 0.0004606001, boundary: 0.000073, weighted total: 0.00019127\n",
      "Epoch 475. Current alpha: 0.695472, lr: 0.0000008367, r_lower: 113.4. Training losses: pinn: 0.0004558671, boundary: 0.000073, weighted total: 0.00018930\n",
      "Epoch 476. Current alpha: 0.695472, lr: 0.0000008367, r_lower: 113.4. Training losses: pinn: 0.0004605176, boundary: 0.000073, weighted total: 0.00019099\n",
      "Epoch 477. Current alpha: 0.695472, lr: 0.0000008367, r_lower: 113.4. Training losses: pinn: 0.0004594714, boundary: 0.000072, weighted total: 0.00019022\n",
      "Epoch 478. Current alpha: 0.695472, lr: 0.0000007949, r_lower: 113.4. Training losses: pinn: 0.0004618903, boundary: 0.000072, weighted total: 0.00019093\n",
      "Epoch 479. Current alpha: 0.695472, lr: 0.0000007949, r_lower: 113.4. Training losses: pinn: 0.0004732198, boundary: 0.000073, weighted total: 0.00019472\n",
      "Epoch 480. Current alpha: 0.695472, lr: 0.0000007552, r_lower: 113.4. Training losses: pinn: 0.0004696872, boundary: 0.000072, weighted total: 0.00019323\n",
      "Epoch 481. Current alpha: 0.690230, lr: 0.0000007174, r_lower: 113.3. Training losses: pinn: 0.0004623389, boundary: 0.000072, weighted total: 0.00019322\n",
      "Epoch 482. Current alpha: 0.690230, lr: 0.0000006815, r_lower: 113.3. Training losses: pinn: 0.0004678728, boundary: 0.000072, weighted total: 0.00019477\n",
      "Epoch 483. Current alpha: 0.690230, lr: 0.0000006475, r_lower: 113.3. Training losses: pinn: 0.0004648228, boundary: 0.000072, weighted total: 0.00019337\n",
      "Epoch 484. Current alpha: 0.690230, lr: 0.0000006151, r_lower: 113.3. Training losses: pinn: 0.0004736820, boundary: 0.000072, weighted total: 0.00019654\n",
      "Epoch 485. Current alpha: 0.690230, lr: 0.0000005843, r_lower: 113.3. Training losses: pinn: 0.0004692353, boundary: 0.000072, weighted total: 0.00019522\n",
      "Epoch 486. Current alpha: 0.690230, lr: 0.0000005551, r_lower: 113.3. Training losses: pinn: 0.0004674428, boundary: 0.000072, weighted total: 0.00019435\n",
      "Epoch 487. Current alpha: 0.690230, lr: 0.0000005274, r_lower: 113.3. Training losses: pinn: 0.0004613325, boundary: 0.000072, weighted total: 0.00019254\n",
      "Epoch 488. Current alpha: 0.690230, lr: 0.0000005010, r_lower: 113.3. Training losses: pinn: 0.0004649457, boundary: 0.000072, weighted total: 0.00019351\n",
      "Epoch 489. Current alpha: 0.690230, lr: 0.0000004759, r_lower: 113.3. Training losses: pinn: 0.0004540170, boundary: 0.000071, weighted total: 0.00018984\n",
      "Epoch 490. Current alpha: 0.690230, lr: 0.0000004759, r_lower: 113.3. Training losses: pinn: 0.0004551446, boundary: 0.000071, weighted total: 0.00019021\n",
      "Epoch 491. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004618324, boundary: 0.000073, weighted total: 0.00019521\n",
      "Epoch 492. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004488113, boundary: 0.000071, weighted total: 0.00019024\n",
      "Epoch 493. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004486527, boundary: 0.000072, weighted total: 0.00019079\n",
      "Epoch 494. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004600665, boundary: 0.000073, weighted total: 0.00019474\n",
      "Epoch 495. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004553744, boundary: 0.000073, weighted total: 0.00019310\n",
      "Epoch 496. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004536391, boundary: 0.000073, weighted total: 0.00019291\n",
      "Epoch 497. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004555497, boundary: 0.000073, weighted total: 0.00019380\n",
      "Epoch 498. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004552552, boundary: 0.000073, weighted total: 0.00019370\n",
      "Epoch 499. Current alpha: 0.685027, lr: 0.0000004759, r_lower: 113.1. Training losses: pinn: 0.0004518781, boundary: 0.000074, weighted total: 0.00019274\n",
      "Epoch 500. Current alpha: 0.685027, lr: 0.0000004521, r_lower: 113.1. Training losses: pinn: 0.0004598321, boundary: 0.000074, weighted total: 0.00019559\n",
      "Epoch 501. Current alpha: 0.679864, lr: 0.0000004295, r_lower: 113.0. Training losses: pinn: 0.0004460351, boundary: 0.000074, weighted total: 0.00019321\n",
      "Epoch 502. Current alpha: 0.679864, lr: 0.0000004295, r_lower: 113.0. Training losses: pinn: 0.0004510957, boundary: 0.000074, weighted total: 0.00019505\n",
      "Epoch 503. Current alpha: 0.679864, lr: 0.0000004081, r_lower: 113.0. Training losses: pinn: 0.0004433656, boundary: 0.000074, weighted total: 0.00019239\n",
      "Epoch 504. Current alpha: 0.679864, lr: 0.0000004081, r_lower: 113.0. Training losses: pinn: 0.0004436872, boundary: 0.000075, weighted total: 0.00019280\n",
      "Epoch 505. Current alpha: 0.679864, lr: 0.0000004081, r_lower: 113.0. Training losses: pinn: 0.0004439528, boundary: 0.000075, weighted total: 0.00019299\n",
      "Epoch 506. Current alpha: 0.679864, lr: 0.0000004081, r_lower: 113.0. Training losses: pinn: 0.0004493945, boundary: 0.000075, weighted total: 0.00019480\n",
      "Epoch 507. Current alpha: 0.679864, lr: 0.0000004081, r_lower: 113.0. Training losses: pinn: 0.0004504649, boundary: 0.000075, weighted total: 0.00019544\n",
      "Epoch 508. Current alpha: 0.679864, lr: 0.0000004081, r_lower: 113.0. Training losses: pinn: 0.0004542642, boundary: 0.000076, weighted total: 0.00019681\n",
      "Epoch 509. Current alpha: 0.679864, lr: 0.0000003877, r_lower: 113.0. Training losses: pinn: 0.0004549348, boundary: 0.000076, weighted total: 0.00019738\n",
      "Epoch 510. Current alpha: 0.679864, lr: 0.0000003683, r_lower: 113.0. Training losses: pinn: 0.0004470542, boundary: 0.000076, weighted total: 0.00019459\n",
      "Epoch 511. Current alpha: 0.674739, lr: 0.0000003683, r_lower: 112.9. Training losses: pinn: 0.0004456316, boundary: 0.000076, weighted total: 0.00019648\n",
      "Epoch 512. Current alpha: 0.674739, lr: 0.0000003499, r_lower: 112.9. Training losses: pinn: 0.0004439815, boundary: 0.000076, weighted total: 0.00019590\n",
      "Epoch 513. Current alpha: 0.674739, lr: 0.0000003499, r_lower: 112.9. Training losses: pinn: 0.0004457194, boundary: 0.000077, weighted total: 0.00019660\n",
      "Epoch 514. Current alpha: 0.674739, lr: 0.0000003324, r_lower: 112.9. Training losses: pinn: 0.0004503244, boundary: 0.000077, weighted total: 0.00019871\n",
      "Epoch 515. Current alpha: 0.674739, lr: 0.0000003157, r_lower: 112.9. Training losses: pinn: 0.0004414802, boundary: 0.000077, weighted total: 0.00019541\n",
      "Epoch 516. Current alpha: 0.674739, lr: 0.0000003157, r_lower: 112.9. Training losses: pinn: 0.0004577122, boundary: 0.000078, weighted total: 0.00020147\n",
      "Epoch 517. Current alpha: 0.674739, lr: 0.0000003000, r_lower: 112.9. Training losses: pinn: 0.0004518173, boundary: 0.000078, weighted total: 0.00019954\n",
      "Epoch 518. Current alpha: 0.674739, lr: 0.0000002850, r_lower: 112.9. Training losses: pinn: 0.0004484675, boundary: 0.000078, weighted total: 0.00019841\n",
      "Epoch 519. Current alpha: 0.674739, lr: 0.0000002850, r_lower: 112.9. Training losses: pinn: 0.0004606712, boundary: 0.000078, weighted total: 0.00020275\n",
      "Epoch 520. Current alpha: 0.674739, lr: 0.0000002707, r_lower: 112.9. Training losses: pinn: 0.0004510799, boundary: 0.000078, weighted total: 0.00019942\n",
      "Epoch 521. Current alpha: 0.669654, lr: 0.0000002572, r_lower: 112.7. Training losses: pinn: 0.0004463773, boundary: 0.000079, weighted total: 0.00020016\n",
      "Epoch 522. Current alpha: 0.669654, lr: 0.0000002443, r_lower: 112.7. Training losses: pinn: 0.0004602507, boundary: 0.000080, weighted total: 0.00020536\n",
      "Epoch 523. Current alpha: 0.669654, lr: 0.0000002321, r_lower: 112.7. Training losses: pinn: 0.0004520368, boundary: 0.000079, weighted total: 0.00020251\n",
      "Epoch 524. Current alpha: 0.669654, lr: 0.0000002205, r_lower: 112.7. Training losses: pinn: 0.0004551741, boundary: 0.000080, weighted total: 0.00020365\n",
      "Epoch 525. Current alpha: 0.669654, lr: 0.0000002095, r_lower: 112.7. Training losses: pinn: 0.0004547498, boundary: 0.000080, weighted total: 0.00020350\n",
      "Epoch 526. Current alpha: 0.669654, lr: 0.0000001990, r_lower: 112.7. Training losses: pinn: 0.0004537542, boundary: 0.000079, weighted total: 0.00020297\n",
      "Epoch 527. Current alpha: 0.669654, lr: 0.0000001990, r_lower: 112.7. Training losses: pinn: 0.0004627597, boundary: 0.000080, weighted total: 0.00020664\n",
      "Epoch 528. Current alpha: 0.669654, lr: 0.0000001890, r_lower: 112.7. Training losses: pinn: 0.0004510093, boundary: 0.000080, weighted total: 0.00020246\n",
      "Epoch 529. Current alpha: 0.669654, lr: 0.0000001796, r_lower: 112.7. Training losses: pinn: 0.0004585688, boundary: 0.000080, weighted total: 0.00020501\n",
      "Epoch 530. Current alpha: 0.669654, lr: 0.0000001796, r_lower: 112.7. Training losses: pinn: 0.0004642605, boundary: 0.000080, weighted total: 0.00020725\n",
      "Epoch 531. Current alpha: 0.664606, lr: 0.0000001706, r_lower: 112.6. Training losses: pinn: 0.0004646257, boundary: 0.000082, weighted total: 0.00021007\n",
      "Epoch 532. Current alpha: 0.664606, lr: 0.0000001621, r_lower: 112.6. Training losses: pinn: 0.0004737057, boundary: 0.000082, weighted total: 0.00021341\n",
      "Epoch 533. Current alpha: 0.664606, lr: 0.0000001540, r_lower: 112.6. Training losses: pinn: 0.0004704661, boundary: 0.000083, weighted total: 0.00021263\n",
      "Epoch 534. Current alpha: 0.664606, lr: 0.0000001463, r_lower: 112.6. Training losses: pinn: 0.0004663172, boundary: 0.000082, weighted total: 0.00021103\n",
      "Epoch 535. Current alpha: 0.664606, lr: 0.0000001390, r_lower: 112.6. Training losses: pinn: 0.0004726735, boundary: 0.000082, weighted total: 0.00021328\n",
      "Epoch 536. Current alpha: 0.664606, lr: 0.0000001320, r_lower: 112.6. Training losses: pinn: 0.0004648320, boundary: 0.000082, weighted total: 0.00021067\n",
      "Epoch 537. Current alpha: 0.664606, lr: 0.0000001254, r_lower: 112.6. Training losses: pinn: 0.0004736893, boundary: 0.000083, weighted total: 0.00021382\n",
      "Epoch 538. Current alpha: 0.664606, lr: 0.0000001191, r_lower: 112.6. Training losses: pinn: 0.0004665793, boundary: 0.000083, weighted total: 0.00021133\n",
      "Epoch 539. Current alpha: 0.664606, lr: 0.0000001132, r_lower: 112.6. Training losses: pinn: 0.0004700544, boundary: 0.000083, weighted total: 0.00021251\n",
      "Epoch 540. Current alpha: 0.664606, lr: 0.0000001075, r_lower: 112.6. Training losses: pinn: 0.0004792019, boundary: 0.000083, weighted total: 0.00021616\n",
      "Epoch 541. Current alpha: 0.659597, lr: 0.0000001022, r_lower: 112.5. Training losses: pinn: 0.0004599003, boundary: 0.000083, weighted total: 0.00021150\n",
      "Epoch 542. Current alpha: 0.659597, lr: 0.0000001022, r_lower: 112.5. Training losses: pinn: 0.0004608379, boundary: 0.000083, weighted total: 0.00021173\n",
      "Epoch 543. Current alpha: 0.659597, lr: 0.0000001022, r_lower: 112.5. Training losses: pinn: 0.0004709438, boundary: 0.000084, weighted total: 0.00021549\n",
      "Epoch 544. Current alpha: 0.659597, lr: 0.0000000970, r_lower: 112.5. Training losses: pinn: 0.0004768391, boundary: 0.000084, weighted total: 0.00021793\n",
      "Epoch 545. Current alpha: 0.659597, lr: 0.0000000922, r_lower: 112.5. Training losses: pinn: 0.0004634308, boundary: 0.000084, weighted total: 0.00021340\n",
      "Epoch 546. Current alpha: 0.659597, lr: 0.0000000922, r_lower: 112.5. Training losses: pinn: 0.0004636671, boundary: 0.000084, weighted total: 0.00021322\n",
      "Epoch 547. Current alpha: 0.659597, lr: 0.0000000876, r_lower: 112.5. Training losses: pinn: 0.0004666480, boundary: 0.000084, weighted total: 0.00021434\n",
      "Epoch 548. Current alpha: 0.659597, lr: 0.0000000876, r_lower: 112.5. Training losses: pinn: 0.0004685589, boundary: 0.000085, weighted total: 0.00021524\n",
      "Epoch 549. Current alpha: 0.659597, lr: 0.0000000832, r_lower: 112.5. Training losses: pinn: 0.0004721403, boundary: 0.000084, weighted total: 0.00021621\n",
      "Epoch 550. Current alpha: 0.659597, lr: 0.0000000790, r_lower: 112.5. Training losses: pinn: 0.0004767997, boundary: 0.000085, weighted total: 0.00021839\n",
      "Epoch 551. Current alpha: 0.654625, lr: 0.0000000790, r_lower: 112.3. Training losses: pinn: 0.0004675853, boundary: 0.000086, weighted total: 0.00021754\n",
      "Epoch 552. Current alpha: 0.654625, lr: 0.0000000751, r_lower: 112.3. Training losses: pinn: 0.0004710587, boundary: 0.000086, weighted total: 0.00021867\n",
      "Epoch 553. Current alpha: 0.654625, lr: 0.0000000713, r_lower: 112.3. Training losses: pinn: 0.0004856300, boundary: 0.000087, weighted total: 0.00022444\n",
      "Epoch 554. Current alpha: 0.654625, lr: 0.0000000678, r_lower: 112.3. Training losses: pinn: 0.0004753249, boundary: 0.000086, weighted total: 0.00022073\n",
      "Epoch 555. Current alpha: 0.654625, lr: 0.0000000644, r_lower: 112.3. Training losses: pinn: 0.0004825319, boundary: 0.000086, weighted total: 0.00022306\n",
      "Epoch 556. Current alpha: 0.654625, lr: 0.0000000612, r_lower: 112.3. Training losses: pinn: 0.0004841539, boundary: 0.000087, weighted total: 0.00022408\n",
      "Epoch 557. Current alpha: 0.654625, lr: 0.0000000581, r_lower: 112.3. Training losses: pinn: 0.0004791649, boundary: 0.000087, weighted total: 0.00022243\n",
      "Epoch 558. Current alpha: 0.654625, lr: 0.0000000552, r_lower: 112.3. Training losses: pinn: 0.0004875941, boundary: 0.000087, weighted total: 0.00022547\n",
      "Epoch 559. Current alpha: 0.654625, lr: 0.0000000524, r_lower: 112.3. Training losses: pinn: 0.0004634050, boundary: 0.000087, weighted total: 0.00021674\n",
      "Epoch 560. Current alpha: 0.654625, lr: 0.0000000524, r_lower: 112.3. Training losses: pinn: 0.0004832002, boundary: 0.000086, weighted total: 0.00022349\n",
      "Epoch 561. Current alpha: 0.649691, lr: 0.0000000498, r_lower: 112.2. Training losses: pinn: 0.0004786378, boundary: 0.000087, weighted total: 0.00022436\n",
      "Epoch 562. Current alpha: 0.649691, lr: 0.0000000473, r_lower: 112.2. Training losses: pinn: 0.0004799742, boundary: 0.000088, weighted total: 0.00022510\n",
      "Epoch 563. Current alpha: 0.649691, lr: 0.0000000450, r_lower: 112.2. Training losses: pinn: 0.0004740462, boundary: 0.000088, weighted total: 0.00022309\n",
      "Epoch 564. Current alpha: 0.649691, lr: 0.0000000450, r_lower: 112.2. Training losses: pinn: 0.0004812748, boundary: 0.000088, weighted total: 0.00022550\n",
      "Epoch 565. Current alpha: 0.649691, lr: 0.0000000427, r_lower: 112.2. Training losses: pinn: 0.0004823431, boundary: 0.000088, weighted total: 0.00022619\n",
      "Epoch 566. Current alpha: 0.649691, lr: 0.0000000406, r_lower: 112.2. Training losses: pinn: 0.0004892365, boundary: 0.000089, weighted total: 0.00022898\n",
      "Epoch 567. Current alpha: 0.649691, lr: 0.0000000385, r_lower: 112.2. Training losses: pinn: 0.0004833890, boundary: 0.000089, weighted total: 0.00022695\n",
      "Epoch 568. Current alpha: 0.649691, lr: 0.0000000366, r_lower: 112.2. Training losses: pinn: 0.0004806057, boundary: 0.000089, weighted total: 0.00022595\n",
      "Epoch 569. Current alpha: 0.649691, lr: 0.0000000366, r_lower: 112.2. Training losses: pinn: 0.0004819300, boundary: 0.000089, weighted total: 0.00022641\n",
      "Epoch 570. Current alpha: 0.649691, lr: 0.0000000348, r_lower: 112.2. Training losses: pinn: 0.0004900555, boundary: 0.000089, weighted total: 0.00022955\n",
      "Epoch 571. Current alpha: 0.644794, lr: 0.0000000331, r_lower: 112.1. Training losses: pinn: 0.0004883982, boundary: 0.000089, weighted total: 0.00023108\n",
      "Epoch 572. Current alpha: 0.644794, lr: 0.0000000314, r_lower: 112.1. Training losses: pinn: 0.0004731700, boundary: 0.000089, weighted total: 0.00022561\n",
      "Epoch 573. Current alpha: 0.644794, lr: 0.0000000314, r_lower: 112.1. Training losses: pinn: 0.0004770375, boundary: 0.000089, weighted total: 0.00022705\n",
      "Epoch 574. Current alpha: 0.644794, lr: 0.0000000298, r_lower: 112.1. Training losses: pinn: 0.0004774341, boundary: 0.000090, weighted total: 0.00022740\n",
      "Epoch 575. Current alpha: 0.644794, lr: 0.0000000298, r_lower: 112.1. Training losses: pinn: 0.0004847118, boundary: 0.000090, weighted total: 0.00023016\n",
      "Epoch 576. Current alpha: 0.644794, lr: 0.0000000283, r_lower: 112.1. Training losses: pinn: 0.0004825222, boundary: 0.000090, weighted total: 0.00022944\n",
      "Epoch 577. Current alpha: 0.644794, lr: 0.0000000283, r_lower: 112.1. Training losses: pinn: 0.0004733650, boundary: 0.000090, weighted total: 0.00022611\n",
      "Epoch 578. Current alpha: 0.644794, lr: 0.0000000283, r_lower: 112.1. Training losses: pinn: 0.0004812705, boundary: 0.000090, weighted total: 0.00022887\n",
      "Epoch 579. Current alpha: 0.644794, lr: 0.0000000269, r_lower: 112.1. Training losses: pinn: 0.0004783653, boundary: 0.000090, weighted total: 0.00022781\n",
      "Epoch 580. Current alpha: 0.644794, lr: 0.0000000269, r_lower: 112.1. Training losses: pinn: 0.0004779675, boundary: 0.000090, weighted total: 0.00022763\n",
      "Epoch 581. Current alpha: 0.639934, lr: 0.0000000269, r_lower: 111.9. Training losses: pinn: 0.0004757802, boundary: 0.000090, weighted total: 0.00022888\n",
      "Epoch 582. Current alpha: 0.639934, lr: 0.0000000269, r_lower: 111.9. Training losses: pinn: 0.0004831256, boundary: 0.000090, weighted total: 0.00023178\n",
      "Epoch 583. Current alpha: 0.639934, lr: 0.0000000256, r_lower: 111.9. Training losses: pinn: 0.0004838883, boundary: 0.000091, weighted total: 0.00023234\n",
      "Epoch 584. Current alpha: 0.639934, lr: 0.0000000243, r_lower: 111.9. Training losses: pinn: 0.0004769254, boundary: 0.000091, weighted total: 0.00022966\n",
      "Epoch 585. Current alpha: 0.639934, lr: 0.0000000231, r_lower: 111.9. Training losses: pinn: 0.0004867784, boundary: 0.000091, weighted total: 0.00023344\n",
      "Epoch 586. Current alpha: 0.639934, lr: 0.0000000219, r_lower: 111.9. Training losses: pinn: 0.0004734901, boundary: 0.000091, weighted total: 0.00022856\n",
      "Epoch 587. Current alpha: 0.639934, lr: 0.0000000219, r_lower: 111.9. Training losses: pinn: 0.0004786757, boundary: 0.000091, weighted total: 0.00023050\n",
      "Epoch 588. Current alpha: 0.639934, lr: 0.0000000208, r_lower: 111.9. Training losses: pinn: 0.0004739666, boundary: 0.000091, weighted total: 0.00022872\n",
      "Epoch 589. Current alpha: 0.639934, lr: 0.0000000208, r_lower: 111.9. Training losses: pinn: 0.0004668009, boundary: 0.000091, weighted total: 0.00022605\n",
      "Epoch 590. Current alpha: 0.639934, lr: 0.0000000208, r_lower: 111.9. Training losses: pinn: 0.0004752653, boundary: 0.000090, weighted total: 0.00022895\n",
      "Epoch 591. Current alpha: 0.635110, lr: 0.0000000208, r_lower: 111.8. Training losses: pinn: 0.0004769633, boundary: 0.000091, weighted total: 0.00023172\n",
      "Epoch 592. Current alpha: 0.635110, lr: 0.0000000198, r_lower: 111.8. Training losses: pinn: 0.0004679636, boundary: 0.000091, weighted total: 0.00022856\n",
      "Epoch 593. Current alpha: 0.635110, lr: 0.0000000198, r_lower: 111.8. Training losses: pinn: 0.0004685863, boundary: 0.000091, weighted total: 0.00022889\n",
      "Epoch 594. Current alpha: 0.635110, lr: 0.0000000198, r_lower: 111.8. Training losses: pinn: 0.0004694094, boundary: 0.000091, weighted total: 0.00022920\n",
      "Epoch 595. Current alpha: 0.635110, lr: 0.0000000198, r_lower: 111.8. Training losses: pinn: 0.0004785994, boundary: 0.000092, weighted total: 0.00023288\n",
      "Epoch 596. Current alpha: 0.635110, lr: 0.0000000198, r_lower: 111.8. Training losses: pinn: 0.0004698076, boundary: 0.000091, weighted total: 0.00022946\n",
      "Epoch 597. Current alpha: 0.635110, lr: 0.0000000198, r_lower: 111.8. Training losses: pinn: 0.0004691679, boundary: 0.000091, weighted total: 0.00022923\n",
      "Epoch 598. Current alpha: 0.635110, lr: 0.0000000198, r_lower: 111.8. Training losses: pinn: 0.0004680612, boundary: 0.000091, weighted total: 0.00022889\n",
      "Epoch 599. Current alpha: 0.635110, lr: 0.0000000198, r_lower: 111.8. Training losses: pinn: 0.0004694872, boundary: 0.000091, weighted total: 0.00022930\n",
      "Epoch 600. Current alpha: 0.635110, lr: 0.0000000188, r_lower: 111.8. Training losses: pinn: 0.0004632369, boundary: 0.000091, weighted total: 0.00022713\n",
      "Epoch 601. Current alpha: 0.630323, lr: 0.0000000188, r_lower: 111.7. Training losses: pinn: 0.0004692751, boundary: 0.000092, weighted total: 0.00023139\n",
      "Epoch 602. Current alpha: 0.630323, lr: 0.0000000188, r_lower: 111.7. Training losses: pinn: 0.0004721765, boundary: 0.000092, weighted total: 0.00023258\n",
      "Epoch 603. Current alpha: 0.630323, lr: 0.0000000179, r_lower: 111.7. Training losses: pinn: 0.0004761222, boundary: 0.000093, weighted total: 0.00023435\n",
      "Epoch 604. Current alpha: 0.630323, lr: 0.0000000170, r_lower: 111.7. Training losses: pinn: 0.0004723055, boundary: 0.000093, weighted total: 0.00023295\n",
      "Epoch 605. Current alpha: 0.630323, lr: 0.0000000161, r_lower: 111.7. Training losses: pinn: 0.0004927464, boundary: 0.000093, weighted total: 0.00024086\n",
      "Epoch 606. Current alpha: 0.630323, lr: 0.0000000153, r_lower: 111.7. Training losses: pinn: 0.0004792293, boundary: 0.000093, weighted total: 0.00023605\n",
      "Epoch 607. Current alpha: 0.630323, lr: 0.0000000145, r_lower: 111.7. Training losses: pinn: 0.0004921274, boundary: 0.000094, weighted total: 0.00024097\n",
      "Epoch 608. Current alpha: 0.630323, lr: 0.0000000138, r_lower: 111.7. Training losses: pinn: 0.0004733105, boundary: 0.000094, weighted total: 0.00023408\n",
      "Epoch 609. Current alpha: 0.630323, lr: 0.0000000131, r_lower: 111.7. Training losses: pinn: 0.0004758683, boundary: 0.000094, weighted total: 0.00023487\n",
      "Epoch 610. Current alpha: 0.630323, lr: 0.0000000125, r_lower: 111.7. Training losses: pinn: 0.0004796917, boundary: 0.000094, weighted total: 0.00023651\n",
      "Epoch 611. Current alpha: 0.625572, lr: 0.0000000118, r_lower: 111.5. Training losses: pinn: 0.0004816650, boundary: 0.000094, weighted total: 0.00023918\n",
      "Epoch 612. Current alpha: 0.625572, lr: 0.0000000113, r_lower: 111.5. Training losses: pinn: 0.0004833524, boundary: 0.000094, weighted total: 0.00023981\n",
      "Epoch 613. Current alpha: 0.625572, lr: 0.0000000107, r_lower: 111.5. Training losses: pinn: 0.0004724791, boundary: 0.000094, weighted total: 0.00023587\n",
      "Epoch 614. Current alpha: 0.625572, lr: 0.0000000107, r_lower: 111.5. Training losses: pinn: 0.0004696021, boundary: 0.000094, weighted total: 0.00023464\n",
      "Epoch 615. Current alpha: 0.625572, lr: 0.0000000107, r_lower: 111.5. Training losses: pinn: 0.0004721183, boundary: 0.000094, weighted total: 0.00023569\n",
      "Epoch 616. Current alpha: 0.625572, lr: 0.0000000107, r_lower: 111.5. Training losses: pinn: 0.0004741786, boundary: 0.000094, weighted total: 0.00023652\n",
      "Epoch 617. Current alpha: 0.625572, lr: 0.0000000107, r_lower: 111.5. Training losses: pinn: 0.0004781144, boundary: 0.000094, weighted total: 0.00023800\n",
      "Epoch 618. Current alpha: 0.625572, lr: 0.0000000107, r_lower: 111.5. Training losses: pinn: 0.0004727215, boundary: 0.000095, weighted total: 0.00023613\n",
      "Epoch 619. Current alpha: 0.625572, lr: 0.0000000102, r_lower: 111.5. Training losses: pinn: 0.0004668161, boundary: 0.000094, weighted total: 0.00023377\n",
      "Epoch 620. Current alpha: 0.625572, lr: 0.0000000102, r_lower: 111.5. Training losses: pinn: 0.0004739339, boundary: 0.000094, weighted total: 0.00023643\n",
      "Epoch 621. Current alpha: 0.620857, lr: 0.0000000102, r_lower: 111.4. Training losses: pinn: 0.0004844201, boundary: 0.000095, weighted total: 0.00024245\n",
      "Epoch 622. Current alpha: 0.620857, lr: 0.0000000097, r_lower: 111.4. Training losses: pinn: 0.0004811844, boundary: 0.000095, weighted total: 0.00024151\n",
      "Epoch 623. Current alpha: 0.620857, lr: 0.0000000097, r_lower: 111.4. Training losses: pinn: 0.0004783563, boundary: 0.000095, weighted total: 0.00024040\n",
      "Epoch 624. Current alpha: 0.620857, lr: 0.0000000092, r_lower: 111.4. Training losses: pinn: 0.0004742800, boundary: 0.000095, weighted total: 0.00023899\n",
      "Epoch 625. Current alpha: 0.620857, lr: 0.0000000087, r_lower: 111.4. Training losses: pinn: 0.0004798573, boundary: 0.000095, weighted total: 0.00024106\n",
      "Epoch 626. Current alpha: 0.620857, lr: 0.0000000083, r_lower: 111.4. Training losses: pinn: 0.0004726298, boundary: 0.000095, weighted total: 0.00023838\n",
      "Epoch 627. Current alpha: 0.620857, lr: 0.0000000083, r_lower: 111.4. Training losses: pinn: 0.0004768316, boundary: 0.000095, weighted total: 0.00024001\n",
      "Epoch 628. Current alpha: 0.620857, lr: 0.0000000083, r_lower: 111.4. Training losses: pinn: 0.0004854871, boundary: 0.000096, weighted total: 0.00024345\n",
      "Epoch 629. Current alpha: 0.620857, lr: 0.0000000079, r_lower: 111.4. Training losses: pinn: 0.0004769781, boundary: 0.000096, weighted total: 0.00024023\n",
      "Epoch 630. Current alpha: 0.620857, lr: 0.0000000075, r_lower: 111.4. Training losses: pinn: 0.0004783039, boundary: 0.000096, weighted total: 0.00024081\n",
      "Epoch 631. Current alpha: 0.616178, lr: 0.0000000071, r_lower: 111.3. Training losses: pinn: 0.0004693663, boundary: 0.000096, weighted total: 0.00023917\n",
      "Epoch 632. Current alpha: 0.616178, lr: 0.0000000071, r_lower: 111.3. Training losses: pinn: 0.0004759045, boundary: 0.000096, weighted total: 0.00024180\n",
      "Epoch 633. Current alpha: 0.616178, lr: 0.0000000071, r_lower: 111.3. Training losses: pinn: 0.0004785602, boundary: 0.000096, weighted total: 0.00024290\n",
      "Epoch 634. Current alpha: 0.616178, lr: 0.0000000067, r_lower: 111.3. Training losses: pinn: 0.0004765625, boundary: 0.000096, weighted total: 0.00024221\n",
      "Epoch 635. Current alpha: 0.616178, lr: 0.0000000064, r_lower: 111.3. Training losses: pinn: 0.0004794697, boundary: 0.000096, weighted total: 0.00024339\n",
      "Epoch 636. Current alpha: 0.616178, lr: 0.0000000061, r_lower: 111.3. Training losses: pinn: 0.0004839002, boundary: 0.000096, weighted total: 0.00024506\n",
      "Epoch 637. Current alpha: 0.616178, lr: 0.0000000058, r_lower: 111.3. Training losses: pinn: 0.0004720814, boundary: 0.000096, weighted total: 0.00024058\n",
      "Epoch 638. Current alpha: 0.616178, lr: 0.0000000058, r_lower: 111.3. Training losses: pinn: 0.0004720958, boundary: 0.000096, weighted total: 0.00024058\n",
      "Epoch 639. Current alpha: 0.616178, lr: 0.0000000058, r_lower: 111.3. Training losses: pinn: 0.0004760364, boundary: 0.000096, weighted total: 0.00024207\n",
      "Epoch 640. Current alpha: 0.616178, lr: 0.0000000058, r_lower: 111.3. Training losses: pinn: 0.0004668940, boundary: 0.000096, weighted total: 0.00023856\n",
      "Epoch 641. Current alpha: 0.611533, lr: 0.0000000058, r_lower: 111.1. Training losses: pinn: 0.0004774571, boundary: 0.000096, weighted total: 0.00024444\n",
      "Epoch 642. Current alpha: 0.611533, lr: 0.0000000055, r_lower: 111.1. Training losses: pinn: 0.0004790338, boundary: 0.000097, weighted total: 0.00024519\n",
      "Epoch 643. Current alpha: 0.611533, lr: 0.0000000052, r_lower: 111.1. Training losses: pinn: 0.0004653038, boundary: 0.000097, weighted total: 0.00023986\n",
      "Epoch 644. Current alpha: 0.611533, lr: 0.0000000052, r_lower: 111.1. Training losses: pinn: 0.0004711554, boundary: 0.000097, weighted total: 0.00024216\n",
      "Epoch 645. Current alpha: 0.611533, lr: 0.0000000052, r_lower: 111.1. Training losses: pinn: 0.0004705816, boundary: 0.000097, weighted total: 0.00024199\n",
      "Epoch 646. Current alpha: 0.611533, lr: 0.0000000052, r_lower: 111.1. Training losses: pinn: 0.0004610913, boundary: 0.000097, weighted total: 0.00023832\n",
      "Epoch 647. Current alpha: 0.611533, lr: 0.0000000052, r_lower: 111.1. Training losses: pinn: 0.0004688058, boundary: 0.000097, weighted total: 0.00024127\n",
      "Epoch 648. Current alpha: 0.611533, lr: 0.0000000052, r_lower: 111.1. Training losses: pinn: 0.0004760285, boundary: 0.000097, weighted total: 0.00024416\n",
      "Epoch 649. Current alpha: 0.611533, lr: 0.0000000050, r_lower: 111.1. Training losses: pinn: 0.0004722037, boundary: 0.000097, weighted total: 0.00024273\n",
      "Epoch 650. Current alpha: 0.611533, lr: 0.0000000050, r_lower: 111.1. Training losses: pinn: 0.0004791353, boundary: 0.000097, weighted total: 0.00024545\n",
      "Epoch 651. Current alpha: 0.606924, lr: 0.0000000047, r_lower: 111.0. Training losses: pinn: 0.0004706186, boundary: 0.000097, weighted total: 0.00024393\n",
      "Epoch 652. Current alpha: 0.606924, lr: 0.0000000047, r_lower: 111.0. Training losses: pinn: 0.0004734138, boundary: 0.000097, weighted total: 0.00024507\n",
      "Epoch 653. Current alpha: 0.606924, lr: 0.0000000047, r_lower: 111.0. Training losses: pinn: 0.0004697007, boundary: 0.000097, weighted total: 0.00024371\n",
      "Epoch 654. Current alpha: 0.606924, lr: 0.0000000045, r_lower: 111.0. Training losses: pinn: 0.0004691864, boundary: 0.000097, weighted total: 0.00024351\n",
      "Epoch 655. Current alpha: 0.606924, lr: 0.0000000045, r_lower: 111.0. Training losses: pinn: 0.0004640370, boundary: 0.000097, weighted total: 0.00024151\n",
      "Epoch 656. Current alpha: 0.606924, lr: 0.0000000045, r_lower: 111.0. Training losses: pinn: 0.0004587972, boundary: 0.000097, weighted total: 0.00023943\n",
      "Epoch 657. Current alpha: 0.606924, lr: 0.0000000045, r_lower: 111.0. Training losses: pinn: 0.0004784145, boundary: 0.000097, weighted total: 0.00024715\n",
      "Epoch 658. Current alpha: 0.606924, lr: 0.0000000042, r_lower: 111.0. Training losses: pinn: 0.0004684863, boundary: 0.000098, weighted total: 0.00024334\n",
      "Epoch 659. Current alpha: 0.606924, lr: 0.0000000042, r_lower: 111.0. Training losses: pinn: 0.0004696526, boundary: 0.000098, weighted total: 0.00024385\n",
      "Epoch 660. Current alpha: 0.606924, lr: 0.0000000042, r_lower: 111.0. Training losses: pinn: 0.0004716220, boundary: 0.000098, weighted total: 0.00024464\n",
      "Epoch 661. Current alpha: 0.602349, lr: 0.0000000042, r_lower: 110.9. Training losses: pinn: 0.0004728751, boundary: 0.000098, weighted total: 0.00024687\n",
      "Epoch 662. Current alpha: 0.602349, lr: 0.0000000040, r_lower: 110.9. Training losses: pinn: 0.0004607727, boundary: 0.000098, weighted total: 0.00024212\n",
      "Epoch 663. Current alpha: 0.602349, lr: 0.0000000040, r_lower: 110.9. Training losses: pinn: 0.0004634081, boundary: 0.000098, weighted total: 0.00024317\n",
      "Epoch 664. Current alpha: 0.602349, lr: 0.0000000040, r_lower: 110.9. Training losses: pinn: 0.0004813648, boundary: 0.000098, weighted total: 0.00025038\n",
      "Epoch 665. Current alpha: 0.602349, lr: 0.0000000038, r_lower: 110.9. Training losses: pinn: 0.0004750179, boundary: 0.000098, weighted total: 0.00024792\n",
      "Epoch 666. Current alpha: 0.602349, lr: 0.0000000036, r_lower: 110.9. Training losses: pinn: 0.0004680743, boundary: 0.000098, weighted total: 0.00024519\n",
      "Epoch 667. Current alpha: 0.602349, lr: 0.0000000035, r_lower: 110.9. Training losses: pinn: 0.0004782486, boundary: 0.000098, weighted total: 0.00024927\n",
      "Epoch 668. Current alpha: 0.602349, lr: 0.0000000033, r_lower: 110.9. Training losses: pinn: 0.0004616181, boundary: 0.000098, weighted total: 0.00024269\n",
      "Epoch 669. Current alpha: 0.602349, lr: 0.0000000033, r_lower: 110.9. Training losses: pinn: 0.0004724322, boundary: 0.000098, weighted total: 0.00024700\n",
      "Epoch 670. Current alpha: 0.602349, lr: 0.0000000031, r_lower: 110.9. Training losses: pinn: 0.0004693909, boundary: 0.000098, weighted total: 0.00024580\n",
      "Epoch 671. Current alpha: 0.597809, lr: 0.0000000031, r_lower: 110.8. Training losses: pinn: 0.0004702526, boundary: 0.000098, weighted total: 0.00024787\n",
      "Epoch 672. Current alpha: 0.597809, lr: 0.0000000031, r_lower: 110.8. Training losses: pinn: 0.0004681028, boundary: 0.000098, weighted total: 0.00024705\n",
      "Epoch 673. Current alpha: 0.597809, lr: 0.0000000030, r_lower: 110.8. Training losses: pinn: 0.0004705921, boundary: 0.000098, weighted total: 0.00024809\n",
      "Epoch 674. Current alpha: 0.597809, lr: 0.0000000028, r_lower: 110.8. Training losses: pinn: 0.0004728200, boundary: 0.000098, weighted total: 0.00024902\n",
      "Epoch 675. Current alpha: 0.597809, lr: 0.0000000028, r_lower: 110.8. Training losses: pinn: 0.0004640336, boundary: 0.000099, weighted total: 0.00024552\n",
      "Epoch 676. Current alpha: 0.597809, lr: 0.0000000028, r_lower: 110.8. Training losses: pinn: 0.0004638859, boundary: 0.000099, weighted total: 0.00024546\n",
      "Epoch 677. Current alpha: 0.597809, lr: 0.0000000028, r_lower: 110.8. Training losses: pinn: 0.0004676099, boundary: 0.000099, weighted total: 0.00024697\n",
      "Epoch 678. Current alpha: 0.597809, lr: 0.0000000028, r_lower: 110.8. Training losses: pinn: 0.0004673511, boundary: 0.000099, weighted total: 0.00024688\n",
      "Epoch 679. Current alpha: 0.597809, lr: 0.0000000027, r_lower: 110.8. Training losses: pinn: 0.0004621710, boundary: 0.000099, weighted total: 0.00024482\n",
      "Epoch 680. Current alpha: 0.597809, lr: 0.0000000027, r_lower: 110.8. Training losses: pinn: 0.0004700379, boundary: 0.000099, weighted total: 0.00024800\n",
      "Epoch 681. Current alpha: 0.593303, lr: 0.0000000025, r_lower: 110.6. Training losses: pinn: 0.0004720736, boundary: 0.000099, weighted total: 0.00025054\n",
      "Epoch 682. Current alpha: 0.593303, lr: 0.0000000024, r_lower: 110.6. Training losses: pinn: 0.0004704529, boundary: 0.000099, weighted total: 0.00024992\n",
      "Epoch 683. Current alpha: 0.593303, lr: 0.0000000023, r_lower: 110.6. Training losses: pinn: 0.0004608462, boundary: 0.000099, weighted total: 0.00024603\n",
      "Epoch 684. Current alpha: 0.593303, lr: 0.0000000023, r_lower: 110.6. Training losses: pinn: 0.0004627197, boundary: 0.000099, weighted total: 0.00024681\n",
      "Epoch 685. Current alpha: 0.593303, lr: 0.0000000023, r_lower: 110.6. Training losses: pinn: 0.0004652706, boundary: 0.000099, weighted total: 0.00024787\n",
      "Epoch 686. Current alpha: 0.593303, lr: 0.0000000022, r_lower: 110.6. Training losses: pinn: 0.0004591626, boundary: 0.000099, weighted total: 0.00024541\n",
      "Epoch 687. Current alpha: 0.593303, lr: 0.0000000022, r_lower: 110.6. Training losses: pinn: 0.0004625332, boundary: 0.000099, weighted total: 0.00024679\n",
      "Epoch 688. Current alpha: 0.593303, lr: 0.0000000022, r_lower: 110.6. Training losses: pinn: 0.0004771904, boundary: 0.000099, weighted total: 0.00025278\n",
      "Epoch 689. Current alpha: 0.593303, lr: 0.0000000021, r_lower: 110.6. Training losses: pinn: 0.0004583818, boundary: 0.000099, weighted total: 0.00024515\n",
      "Epoch 690. Current alpha: 0.593303, lr: 0.0000000021, r_lower: 110.6. Training losses: pinn: 0.0004658305, boundary: 0.000099, weighted total: 0.00024818\n",
      "Epoch 691. Current alpha: 0.588831, lr: 0.0000000021, r_lower: 110.5. Training losses: pinn: 0.0004758256, boundary: 0.000099, weighted total: 0.00025396\n",
      "Epoch 692. Current alpha: 0.588831, lr: 0.0000000020, r_lower: 110.5. Training losses: pinn: 0.0004685963, boundary: 0.000099, weighted total: 0.00025103\n",
      "Epoch 693. Current alpha: 0.588831, lr: 0.0000000020, r_lower: 110.5. Training losses: pinn: 0.0004529889, boundary: 0.000099, weighted total: 0.00024464\n",
      "Epoch 694. Current alpha: 0.588831, lr: 0.0000000020, r_lower: 110.5. Training losses: pinn: 0.0004587877, boundary: 0.000099, weighted total: 0.00024703\n",
      "Epoch 695. Current alpha: 0.588831, lr: 0.0000000020, r_lower: 110.5. Training losses: pinn: 0.0004677640, boundary: 0.000099, weighted total: 0.00025074\n",
      "Epoch 696. Current alpha: 0.588831, lr: 0.0000000019, r_lower: 110.5. Training losses: pinn: 0.0004693500, boundary: 0.000099, weighted total: 0.00025142\n",
      "Epoch 697. Current alpha: 0.588831, lr: 0.0000000018, r_lower: 110.5. Training losses: pinn: 0.0004693195, boundary: 0.000099, weighted total: 0.00025143\n",
      "Epoch 698. Current alpha: 0.588831, lr: 0.0000000017, r_lower: 110.5. Training losses: pinn: 0.0004638658, boundary: 0.000099, weighted total: 0.00024921\n",
      "Epoch 699. Current alpha: 0.588831, lr: 0.0000000017, r_lower: 110.5. Training losses: pinn: 0.0004565787, boundary: 0.000099, weighted total: 0.00024622\n",
      "Epoch 700. Current alpha: 0.588831, lr: 0.0000000017, r_lower: 110.5. Training losses: pinn: 0.0004657517, boundary: 0.000099, weighted total: 0.00025000\n",
      "Epoch 701. Current alpha: 0.584393, lr: 0.0000000016, r_lower: 110.4. Training losses: pinn: 0.0004632763, boundary: 0.000099, weighted total: 0.00025062\n",
      "Epoch 702. Current alpha: 0.584393, lr: 0.0000000016, r_lower: 110.4. Training losses: pinn: 0.0004711686, boundary: 0.000099, weighted total: 0.00025393\n",
      "Epoch 703. Current alpha: 0.584393, lr: 0.0000000015, r_lower: 110.4. Training losses: pinn: 0.0004722131, boundary: 0.000099, weighted total: 0.00025438\n",
      "Epoch 704. Current alpha: 0.584393, lr: 0.0000000014, r_lower: 110.4. Training losses: pinn: 0.0004631636, boundary: 0.000099, weighted total: 0.00025064\n",
      "Epoch 705. Current alpha: 0.584393, lr: 0.0000000014, r_lower: 110.4. Training losses: pinn: 0.0004706495, boundary: 0.000100, weighted total: 0.00025377\n",
      "Epoch 706. Current alpha: 0.584393, lr: 0.0000000013, r_lower: 110.4. Training losses: pinn: 0.0004598990, boundary: 0.000100, weighted total: 0.00024932\n",
      "Epoch 707. Current alpha: 0.584393, lr: 0.0000000013, r_lower: 110.4. Training losses: pinn: 0.0004775903, boundary: 0.000100, weighted total: 0.00025669\n",
      "Epoch 708. Current alpha: 0.584393, lr: 0.0000000012, r_lower: 110.4. Training losses: pinn: 0.0004588240, boundary: 0.000100, weighted total: 0.00024891\n",
      "Epoch 709. Current alpha: 0.584393, lr: 0.0000000012, r_lower: 110.4. Training losses: pinn: 0.0004617983, boundary: 0.000100, weighted total: 0.00025015\n",
      "Epoch 710. Current alpha: 0.584393, lr: 0.0000000012, r_lower: 110.4. Training losses: pinn: 0.0004619002, boundary: 0.000100, weighted total: 0.00025020\n",
      "Epoch 711. Current alpha: 0.579988, lr: 0.0000000012, r_lower: 110.2. Training losses: pinn: 0.0004643687, boundary: 0.000100, weighted total: 0.00025285\n",
      "Epoch 712. Current alpha: 0.579988, lr: 0.0000000011, r_lower: 110.2. Training losses: pinn: 0.0004684954, boundary: 0.000100, weighted total: 0.00025460\n",
      "Epoch 713. Current alpha: 0.579988, lr: 0.0000000011, r_lower: 110.2. Training losses: pinn: 0.0004660153, boundary: 0.000100, weighted total: 0.00025358\n",
      "Epoch 714. Current alpha: 0.579988, lr: 0.0000000011, r_lower: 110.2. Training losses: pinn: 0.0004672876, boundary: 0.000100, weighted total: 0.00025413\n",
      "Epoch 715. Current alpha: 0.579988, lr: 0.0000000011, r_lower: 110.2. Training losses: pinn: 0.0004521451, boundary: 0.000100, weighted total: 0.00024778\n",
      "Epoch 716. Current alpha: 0.579988, lr: 0.0000000011, r_lower: 110.2. Training losses: pinn: 0.0004724242, boundary: 0.000100, weighted total: 0.00025631\n",
      "Epoch 717. Current alpha: 0.579988, lr: 0.0000000010, r_lower: 110.2. Training losses: pinn: 0.0004602839, boundary: 0.000100, weighted total: 0.00025122\n",
      "Epoch 718. Current alpha: 0.579988, lr: 0.0000000010, r_lower: 110.2. Training losses: pinn: 0.0004685376, boundary: 0.000100, weighted total: 0.00025470\n",
      "Epoch 719. Current alpha: 0.579988, lr: 0.0000000010, r_lower: 110.2. Training losses: pinn: 0.0004760552, boundary: 0.000100, weighted total: 0.00025787\n",
      "Epoch 720. Current alpha: 0.579988, lr: 0.0000000009, r_lower: 110.2. Training losses: pinn: 0.0004647483, boundary: 0.000100, weighted total: 0.00025314\n",
      "Epoch 721. Current alpha: 0.575617, lr: 0.0000000009, r_lower: 110.1. Training losses: pinn: 0.0004725847, boundary: 0.000100, weighted total: 0.00025807\n",
      "Epoch 722. Current alpha: 0.575617, lr: 0.0000000008, r_lower: 110.1. Training losses: pinn: 0.0004673756, boundary: 0.000100, weighted total: 0.00025587\n",
      "Epoch 723. Current alpha: 0.575617, lr: 0.0000000008, r_lower: 110.1. Training losses: pinn: 0.0004576583, boundary: 0.000100, weighted total: 0.00025176\n",
      "Epoch 724. Current alpha: 0.575617, lr: 0.0000000008, r_lower: 110.1. Training losses: pinn: 0.0004563937, boundary: 0.000100, weighted total: 0.00025123\n",
      "Epoch 725. Current alpha: 0.575617, lr: 0.0000000008, r_lower: 110.1. Training losses: pinn: 0.0004625994, boundary: 0.000100, weighted total: 0.00025387\n",
      "Epoch 726. Current alpha: 0.575617, lr: 0.0000000008, r_lower: 110.1. Training losses: pinn: 0.0004663328, boundary: 0.000100, weighted total: 0.00025546\n",
      "Epoch 727. Current alpha: 0.575617, lr: 0.0000000008, r_lower: 110.1. Training losses: pinn: 0.0004786521, boundary: 0.000100, weighted total: 0.00026070\n",
      "Epoch 728. Current alpha: 0.575617, lr: 0.0000000007, r_lower: 110.1. Training losses: pinn: 0.0004700071, boundary: 0.000100, weighted total: 0.00025705\n",
      "Epoch 729. Current alpha: 0.575617, lr: 0.0000000007, r_lower: 110.1. Training losses: pinn: 0.0004677811, boundary: 0.000100, weighted total: 0.00025611\n",
      "Epoch 730. Current alpha: 0.575617, lr: 0.0000000007, r_lower: 110.1. Training losses: pinn: 0.0004714188, boundary: 0.000100, weighted total: 0.00025767\n",
      "Epoch 731. Current alpha: 0.571278, lr: 0.0000000007, r_lower: 110.0. Training losses: pinn: 0.0004649999, boundary: 0.000100, weighted total: 0.00025653\n",
      "Epoch 732. Current alpha: 0.571278, lr: 0.0000000007, r_lower: 110.0. Training losses: pinn: 0.0004636496, boundary: 0.000100, weighted total: 0.00025596\n",
      "Epoch 733. Current alpha: 0.571278, lr: 0.0000000007, r_lower: 110.0. Training losses: pinn: 0.0004692253, boundary: 0.000100, weighted total: 0.00025837\n",
      "Epoch 734. Current alpha: 0.571278, lr: 0.0000000006, r_lower: 110.0. Training losses: pinn: 0.0004538472, boundary: 0.000100, weighted total: 0.00025178\n",
      "Epoch 735. Current alpha: 0.571278, lr: 0.0000000006, r_lower: 110.0. Training losses: pinn: 0.0004685584, boundary: 0.000100, weighted total: 0.00025810\n",
      "Epoch 736. Current alpha: 0.571278, lr: 0.0000000006, r_lower: 110.0. Training losses: pinn: 0.0004682236, boundary: 0.000100, weighted total: 0.00025796\n",
      "Epoch 737. Current alpha: 0.571278, lr: 0.0000000006, r_lower: 110.0. Training losses: pinn: 0.0004587142, boundary: 0.000100, weighted total: 0.00025389\n",
      "Epoch 738. Current alpha: 0.571278, lr: 0.0000000006, r_lower: 110.0. Training losses: pinn: 0.0004603074, boundary: 0.000100, weighted total: 0.00025458\n",
      "Epoch 739. Current alpha: 0.571278, lr: 0.0000000006, r_lower: 110.0. Training losses: pinn: 0.0004673332, boundary: 0.000100, weighted total: 0.00025760\n",
      "Epoch 740. Current alpha: 0.571278, lr: 0.0000000006, r_lower: 110.0. Training losses: pinn: 0.0004642314, boundary: 0.000100, weighted total: 0.00025628\n",
      "Epoch 741. Current alpha: 0.566972, lr: 0.0000000006, r_lower: 109.8. Training losses: pinn: 0.0004657676, boundary: 0.000100, weighted total: 0.00025852\n",
      "Epoch 742. Current alpha: 0.566972, lr: 0.0000000005, r_lower: 109.8. Training losses: pinn: 0.0004581211, boundary: 0.000100, weighted total: 0.00025522\n",
      "Epoch 743. Current alpha: 0.566972, lr: 0.0000000005, r_lower: 109.8. Training losses: pinn: 0.0004588259, boundary: 0.000100, weighted total: 0.00025553\n",
      "Epoch 744. Current alpha: 0.566972, lr: 0.0000000005, r_lower: 109.8. Training losses: pinn: 0.0004663517, boundary: 0.000100, weighted total: 0.00025880\n",
      "Epoch 745. Current alpha: 0.566972, lr: 0.0000000005, r_lower: 109.8. Training losses: pinn: 0.0004562539, boundary: 0.000100, weighted total: 0.00025443\n",
      "Epoch 746. Current alpha: 0.566972, lr: 0.0000000005, r_lower: 109.8. Training losses: pinn: 0.0004762024, boundary: 0.000100, weighted total: 0.00026308\n",
      "Epoch 747. Current alpha: 0.566972, lr: 0.0000000005, r_lower: 109.8. Training losses: pinn: 0.0004590298, boundary: 0.000100, weighted total: 0.00025565\n",
      "Epoch 748. Current alpha: 0.566972, lr: 0.0000000005, r_lower: 109.8. Training losses: pinn: 0.0004685213, boundary: 0.000100, weighted total: 0.00025977\n",
      "Epoch 749. Current alpha: 0.566972, lr: 0.0000000004, r_lower: 109.8. Training losses: pinn: 0.0004595448, boundary: 0.000100, weighted total: 0.00025589\n",
      "Epoch 750. Current alpha: 0.566972, lr: 0.0000000004, r_lower: 109.8. Training losses: pinn: 0.0004594791, boundary: 0.000100, weighted total: 0.00025587\n",
      "Epoch 751. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004618991, boundary: 0.000100, weighted total: 0.00025847\n",
      "Epoch 752. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004540402, boundary: 0.000100, weighted total: 0.00025503\n",
      "Epoch 753. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004563865, boundary: 0.000100, weighted total: 0.00025607\n",
      "Epoch 754. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004685864, boundary: 0.000100, weighted total: 0.00026141\n",
      "Epoch 755. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004659640, boundary: 0.000100, weighted total: 0.00026027\n",
      "Epoch 756. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004691342, boundary: 0.000100, weighted total: 0.00026166\n",
      "Epoch 757. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004661298, boundary: 0.000100, weighted total: 0.00026036\n",
      "Epoch 758. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004571335, boundary: 0.000100, weighted total: 0.00025643\n",
      "Epoch 759. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004581636, boundary: 0.000100, weighted total: 0.00025688\n",
      "Epoch 760. Current alpha: 0.562698, lr: 0.0000000004, r_lower: 109.7. Training losses: pinn: 0.0004560931, boundary: 0.000100, weighted total: 0.00025598\n",
      "Epoch 761. Current alpha: 0.558457, lr: 0.0000000004, r_lower: 109.6. Training losses: pinn: 0.0004570138, boundary: 0.000100, weighted total: 0.00025790\n",
      "Epoch 762. Current alpha: 0.558457, lr: 0.0000000004, r_lower: 109.6. Training losses: pinn: 0.0004666976, boundary: 0.000100, weighted total: 0.00026219\n",
      "Epoch 763. Current alpha: 0.558457, lr: 0.0000000004, r_lower: 109.6. Training losses: pinn: 0.0004595353, boundary: 0.000101, weighted total: 0.00025903\n",
      "Epoch 764. Current alpha: 0.558457, lr: 0.0000000003, r_lower: 109.6. Training losses: pinn: 0.0004643064, boundary: 0.000101, weighted total: 0.00026114\n",
      "Epoch 765. Current alpha: 0.558457, lr: 0.0000000003, r_lower: 109.6. Training losses: pinn: 0.0004542447, boundary: 0.000101, weighted total: 0.00025670\n",
      "Epoch 766. Current alpha: 0.558457, lr: 0.0000000003, r_lower: 109.6. Training losses: pinn: 0.0004802850, boundary: 0.000101, weighted total: 0.00026821\n",
      "Epoch 767. Current alpha: 0.558457, lr: 0.0000000003, r_lower: 109.6. Training losses: pinn: 0.0004685388, boundary: 0.000101, weighted total: 0.00026303\n",
      "Epoch 768. Current alpha: 0.558457, lr: 0.0000000003, r_lower: 109.6. Training losses: pinn: 0.0004527231, boundary: 0.000101, weighted total: 0.00025605\n",
      "Epoch 769. Current alpha: 0.558457, lr: 0.0000000003, r_lower: 109.6. Training losses: pinn: 0.0004561235, boundary: 0.000101, weighted total: 0.00025755\n",
      "Epoch 770. Current alpha: 0.558457, lr: 0.0000000003, r_lower: 109.6. Training losses: pinn: 0.0004573058, boundary: 0.000101, weighted total: 0.00025808\n",
      "Epoch 771. Current alpha: 0.554248, lr: 0.0000000003, r_lower: 109.5. Training losses: pinn: 0.0004701980, boundary: 0.000101, weighted total: 0.00026533\n",
      "Epoch 772. Current alpha: 0.554248, lr: 0.0000000003, r_lower: 109.5. Training losses: pinn: 0.0004599713, boundary: 0.000101, weighted total: 0.00026078\n",
      "Epoch 773. Current alpha: 0.554248, lr: 0.0000000003, r_lower: 109.5. Training losses: pinn: 0.0004730903, boundary: 0.000101, weighted total: 0.00026663\n",
      "Epoch 774. Current alpha: 0.554248, lr: 0.0000000003, r_lower: 109.5. Training losses: pinn: 0.0004660599, boundary: 0.000101, weighted total: 0.00026350\n",
      "Epoch 775. Current alpha: 0.554248, lr: 0.0000000003, r_lower: 109.5. Training losses: pinn: 0.0004616919, boundary: 0.000101, weighted total: 0.00026156\n",
      "Epoch 776. Current alpha: 0.554248, lr: 0.0000000002, r_lower: 109.5. Training losses: pinn: 0.0004648843, boundary: 0.000101, weighted total: 0.00026299\n",
      "Epoch 777. Current alpha: 0.554248, lr: 0.0000000002, r_lower: 109.5. Training losses: pinn: 0.0004625894, boundary: 0.000101, weighted total: 0.00026197\n",
      "Epoch 778. Current alpha: 0.554248, lr: 0.0000000002, r_lower: 109.5. Training losses: pinn: 0.0004672962, boundary: 0.000101, weighted total: 0.00026407\n",
      "Epoch 779. Current alpha: 0.554248, lr: 0.0000000002, r_lower: 109.5. Training losses: pinn: 0.0004531652, boundary: 0.000101, weighted total: 0.00025777\n",
      "Epoch 780. Current alpha: 0.554248, lr: 0.0000000002, r_lower: 109.5. Training losses: pinn: 0.0004623918, boundary: 0.000101, weighted total: 0.00026189\n",
      "Epoch 781. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004761472, boundary: 0.000101, weighted total: 0.00026959\n",
      "Epoch 782. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004765318, boundary: 0.000101, weighted total: 0.00026977\n",
      "Epoch 783. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004668292, boundary: 0.000101, weighted total: 0.00026541\n",
      "Epoch 784. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004687276, boundary: 0.000101, weighted total: 0.00026626\n",
      "Epoch 785. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004735707, boundary: 0.000101, weighted total: 0.00026845\n",
      "Epoch 786. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004699064, boundary: 0.000101, weighted total: 0.00026680\n",
      "Epoch 787. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004663645, boundary: 0.000101, weighted total: 0.00026521\n",
      "Epoch 788. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004539750, boundary: 0.000101, weighted total: 0.00025964\n",
      "Epoch 789. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004505550, boundary: 0.000101, weighted total: 0.00025810\n",
      "Epoch 790. Current alpha: 0.550070, lr: 0.0000000002, r_lower: 109.3. Training losses: pinn: 0.0004695468, boundary: 0.000101, weighted total: 0.00026665\n",
      "Epoch 791. Current alpha: 0.545924, lr: 0.0000000002, r_lower: 109.2. Training losses: pinn: 0.0004672936, boundary: 0.000101, weighted total: 0.00026716\n",
      "Epoch 792. Current alpha: 0.545924, lr: 0.0000000002, r_lower: 109.2. Training losses: pinn: 0.0004656403, boundary: 0.000101, weighted total: 0.00026641\n",
      "Epoch 793. Current alpha: 0.545924, lr: 0.0000000002, r_lower: 109.2. Training losses: pinn: 0.0004643787, boundary: 0.000101, weighted total: 0.00026584\n",
      "Epoch 794. Current alpha: 0.545924, lr: 0.0000000002, r_lower: 109.2. Training losses: pinn: 0.0004625610, boundary: 0.000101, weighted total: 0.00026502\n",
      "Epoch 795. Current alpha: 0.545924, lr: 0.0000000002, r_lower: 109.2. Training losses: pinn: 0.0004562113, boundary: 0.000101, weighted total: 0.00026214\n",
      "Epoch 796. Current alpha: 0.545924, lr: 0.0000000002, r_lower: 109.2. Training losses: pinn: 0.0004582756, boundary: 0.000101, weighted total: 0.00026308\n",
      "Epoch 797. Current alpha: 0.545924, lr: 0.0000000002, r_lower: 109.2. Training losses: pinn: 0.0004612690, boundary: 0.000101, weighted total: 0.00026444\n",
      "Epoch 798. Current alpha: 0.545924, lr: 0.0000000002, r_lower: 109.2. Training losses: pinn: 0.0004623236, boundary: 0.000101, weighted total: 0.00026492\n",
      "Epoch 799. Current alpha: 0.545924, lr: 0.0000000001, r_lower: 109.2. Training losses: pinn: 0.0004614648, boundary: 0.000101, weighted total: 0.00026453\n",
      "Epoch 800. Current alpha: 0.545924, lr: 0.0000000001, r_lower: 109.2. Training losses: pinn: 0.0004659610, boundary: 0.000101, weighted total: 0.00026658\n",
      "Epoch 801. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004558880, boundary: 0.000101, weighted total: 0.00026347\n",
      "Epoch 802. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004728295, boundary: 0.000101, weighted total: 0.00027123\n",
      "Epoch 803. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004548188, boundary: 0.000101, weighted total: 0.00026298\n",
      "Epoch 804. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004609391, boundary: 0.000101, weighted total: 0.00026579\n",
      "Epoch 805. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004625294, boundary: 0.000101, weighted total: 0.00026652\n",
      "Epoch 806. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004457783, boundary: 0.000101, weighted total: 0.00025884\n",
      "Epoch 807. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004596949, boundary: 0.000101, weighted total: 0.00026522\n",
      "Epoch 808. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004664974, boundary: 0.000101, weighted total: 0.00026834\n",
      "Epoch 809. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004544369, boundary: 0.000101, weighted total: 0.00026282\n",
      "Epoch 810. Current alpha: 0.541809, lr: 0.0000000001, r_lower: 109.1. Training losses: pinn: 0.0004622466, boundary: 0.000101, weighted total: 0.00026640\n",
      "Epoch 811. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004703414, boundary: 0.000101, weighted total: 0.00027161\n",
      "Epoch 812. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004542352, boundary: 0.000101, weighted total: 0.00026417\n",
      "Epoch 813. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004509149, boundary: 0.000101, weighted total: 0.00026264\n",
      "Epoch 814. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004624990, boundary: 0.000101, weighted total: 0.00026799\n",
      "Epoch 815. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004611359, boundary: 0.000101, weighted total: 0.00026736\n",
      "Epoch 816. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004580539, boundary: 0.000101, weighted total: 0.00026594\n",
      "Epoch 817. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004510752, boundary: 0.000101, weighted total: 0.00026271\n",
      "Epoch 818. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004636986, boundary: 0.000101, weighted total: 0.00026855\n",
      "Epoch 819. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004629465, boundary: 0.000101, weighted total: 0.00026820\n",
      "Epoch 820. Current alpha: 0.537726, lr: 0.0000000001, r_lower: 108.9. Training losses: pinn: 0.0004481236, boundary: 0.000101, weighted total: 0.00026135\n",
      "Epoch 821. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004503027, boundary: 0.000101, weighted total: 0.00026378\n",
      "Epoch 822. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004643594, boundary: 0.000101, weighted total: 0.00027033\n",
      "Epoch 823. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004741723, boundary: 0.000101, weighted total: 0.00027491\n",
      "Epoch 824. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004672346, boundary: 0.000101, weighted total: 0.00027167\n",
      "Epoch 825. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004657726, boundary: 0.000101, weighted total: 0.00027099\n",
      "Epoch 826. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004691959, boundary: 0.000101, weighted total: 0.00027259\n",
      "Epoch 827. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004693887, boundary: 0.000101, weighted total: 0.00027268\n",
      "Epoch 828. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004484674, boundary: 0.000101, weighted total: 0.00026293\n",
      "Epoch 829. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004520894, boundary: 0.000101, weighted total: 0.00026462\n",
      "Epoch 830. Current alpha: 0.533672, lr: 0.0000000001, r_lower: 108.8. Training losses: pinn: 0.0004634440, boundary: 0.000101, weighted total: 0.00026991\n",
      "Epoch 831. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004596287, boundary: 0.000101, weighted total: 0.00026958\n",
      "Epoch 832. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004462114, boundary: 0.000101, weighted total: 0.00026327\n",
      "Epoch 833. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004673728, boundary: 0.000101, weighted total: 0.00027322\n",
      "Epoch 834. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004583136, boundary: 0.000101, weighted total: 0.00026896\n",
      "Epoch 835. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004569699, boundary: 0.000101, weighted total: 0.00026833\n",
      "Epoch 836. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004587929, boundary: 0.000101, weighted total: 0.00026919\n",
      "Epoch 837. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004690757, boundary: 0.000101, weighted total: 0.00027402\n",
      "Epoch 838. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004519408, boundary: 0.000101, weighted total: 0.00026596\n",
      "Epoch 839. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004724465, boundary: 0.000101, weighted total: 0.00027561\n",
      "Epoch 840. Current alpha: 0.529650, lr: 0.0000000001, r_lower: 108.7. Training losses: pinn: 0.0004751654, boundary: 0.000101, weighted total: 0.00027689\n",
      "Epoch 841. Current alpha: 0.525658, lr: 0.0000000001, r_lower: 108.6. Training losses: pinn: 0.0004517148, boundary: 0.000101, weighted total: 0.00026726\n",
      "Epoch 842. Current alpha: 0.525658, lr: 0.0000000001, r_lower: 108.6. Training losses: pinn: 0.0004710881, boundary: 0.000101, weighted total: 0.00027645\n",
      "Epoch 843. Current alpha: 0.525658, lr: 0.0000000001, r_lower: 108.6. Training losses: pinn: 0.0004628503, boundary: 0.000101, weighted total: 0.00027254\n",
      "Epoch 844. Current alpha: 0.525658, lr: 0.0000000001, r_lower: 108.6. Training losses: pinn: 0.0004672531, boundary: 0.000101, weighted total: 0.00027463\n",
      "Epoch 845. Current alpha: 0.525658, lr: 0.0000000000, r_lower: 108.6. Training losses: pinn: 0.0004549256, boundary: 0.000101, weighted total: 0.00026878\n",
      "Epoch 846. Current alpha: 0.525658, lr: 0.0000000000, r_lower: 108.6. Training losses: pinn: 0.0004640140, boundary: 0.000101, weighted total: 0.00027309\n",
      "Epoch 847. Current alpha: 0.525658, lr: 0.0000000000, r_lower: 108.6. Training losses: pinn: 0.0004672755, boundary: 0.000101, weighted total: 0.00027464\n",
      "Epoch 848. Current alpha: 0.525658, lr: 0.0000000000, r_lower: 108.6. Training losses: pinn: 0.0004627306, boundary: 0.000101, weighted total: 0.00027249\n",
      "Epoch 849. Current alpha: 0.525658, lr: 0.0000000000, r_lower: 108.6. Training losses: pinn: 0.0004611274, boundary: 0.000101, weighted total: 0.00027173\n",
      "Epoch 850. Current alpha: 0.525658, lr: 0.0000000000, r_lower: 108.6. Training losses: pinn: 0.0004628925, boundary: 0.000101, weighted total: 0.00027256\n",
      "Epoch 851. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004669460, boundary: 0.000101, weighted total: 0.00027594\n",
      "Epoch 852. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004675492, boundary: 0.000101, weighted total: 0.00027622\n",
      "Epoch 853. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004649265, boundary: 0.000101, weighted total: 0.00027497\n",
      "Epoch 854. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004647115, boundary: 0.000101, weighted total: 0.00027487\n",
      "Epoch 855. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004611789, boundary: 0.000101, weighted total: 0.00027318\n",
      "Epoch 856. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004550893, boundary: 0.000101, weighted total: 0.00027026\n",
      "Epoch 857. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004545353, boundary: 0.000101, weighted total: 0.00027000\n",
      "Epoch 858. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004457642, boundary: 0.000101, weighted total: 0.00026581\n",
      "Epoch 859. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004597351, boundary: 0.000101, weighted total: 0.00027249\n",
      "Epoch 860. Current alpha: 0.521696, lr: 0.0000000000, r_lower: 108.4. Training losses: pinn: 0.0004701044, boundary: 0.000101, weighted total: 0.00027745\n",
      "Epoch 861. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004482834, boundary: 0.000101, weighted total: 0.00026838\n",
      "Epoch 862. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004677270, boundary: 0.000101, weighted total: 0.00027775\n",
      "Epoch 863. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004517568, boundary: 0.000101, weighted total: 0.00027005\n",
      "Epoch 864. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004580463, boundary: 0.000101, weighted total: 0.00027309\n",
      "Epoch 865. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004593392, boundary: 0.000101, weighted total: 0.00027371\n",
      "Epoch 866. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004658174, boundary: 0.000101, weighted total: 0.00027683\n",
      "Epoch 867. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004602819, boundary: 0.000101, weighted total: 0.00027416\n",
      "Epoch 868. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004761752, boundary: 0.000101, weighted total: 0.00028183\n",
      "Epoch 869. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004574539, boundary: 0.000101, weighted total: 0.00027280\n",
      "Epoch 870. Current alpha: 0.517764, lr: 0.0000000000, r_lower: 108.3. Training losses: pinn: 0.0004686571, boundary: 0.000101, weighted total: 0.00027820\n",
      "Epoch 871. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004636644, boundary: 0.000101, weighted total: 0.00027721\n",
      "Epoch 872. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004604538, boundary: 0.000101, weighted total: 0.00027565\n",
      "Epoch 873. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004650986, boundary: 0.000101, weighted total: 0.00027791\n",
      "Epoch 874. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004567758, boundary: 0.000101, weighted total: 0.00027386\n",
      "Epoch 875. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004719623, boundary: 0.000101, weighted total: 0.00028125\n",
      "Epoch 876. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004666700, boundary: 0.000101, weighted total: 0.00027867\n",
      "Epoch 877. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004670462, boundary: 0.000101, weighted total: 0.00027886\n",
      "Epoch 878. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004708559, boundary: 0.000101, weighted total: 0.00028071\n",
      "Epoch 879. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004656234, boundary: 0.000101, weighted total: 0.00027816\n",
      "Epoch 880. Current alpha: 0.513861, lr: 0.0000000000, r_lower: 108.2. Training losses: pinn: 0.0004606737, boundary: 0.000101, weighted total: 0.00027576\n",
      "Epoch 881. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004672821, boundary: 0.000101, weighted total: 0.00028039\n",
      "Epoch 882. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004600354, boundary: 0.000101, weighted total: 0.00027684\n",
      "Epoch 883. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004566729, boundary: 0.000101, weighted total: 0.00027519\n",
      "Epoch 884. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004558790, boundary: 0.000101, weighted total: 0.00027480\n",
      "Epoch 885. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004534328, boundary: 0.000101, weighted total: 0.00027360\n",
      "Epoch 886. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004665992, boundary: 0.000101, weighted total: 0.00028006\n",
      "Epoch 887. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004563957, boundary: 0.000101, weighted total: 0.00027506\n",
      "Epoch 888. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004683460, boundary: 0.000101, weighted total: 0.00028091\n",
      "Epoch 889. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004725626, boundary: 0.000101, weighted total: 0.00028298\n",
      "Epoch 890. Current alpha: 0.509988, lr: 0.0000000000, r_lower: 108.1. Training losses: pinn: 0.0004535944, boundary: 0.000101, weighted total: 0.00027368\n",
      "Epoch 891. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004732437, boundary: 0.000101, weighted total: 0.00028474\n",
      "Epoch 892. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004626851, boundary: 0.000101, weighted total: 0.00027953\n",
      "Epoch 893. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004453032, boundary: 0.000101, weighted total: 0.00027094\n",
      "Epoch 894. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004577498, boundary: 0.000101, weighted total: 0.00027709\n",
      "Epoch 895. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004635371, boundary: 0.000101, weighted total: 0.00027995\n",
      "Epoch 896. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004591778, boundary: 0.000101, weighted total: 0.00027780\n",
      "Epoch 897. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004591542, boundary: 0.000101, weighted total: 0.00027779\n",
      "Epoch 898. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004694896, boundary: 0.000101, weighted total: 0.00028289\n",
      "Epoch 899. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004638508, boundary: 0.000101, weighted total: 0.00028010\n",
      "Epoch 900. Current alpha: 0.506144, lr: 0.0000000000, r_lower: 107.9. Training losses: pinn: 0.0004602536, boundary: 0.000101, weighted total: 0.00027833\n",
      "Epoch 901. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004587202, boundary: 0.000101, weighted total: 0.00027894\n",
      "Epoch 902. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004726633, boundary: 0.000101, weighted total: 0.00028588\n",
      "Epoch 903. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004505330, boundary: 0.000101, weighted total: 0.00027486\n",
      "Epoch 904. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004645332, boundary: 0.000101, weighted total: 0.00028183\n",
      "Epoch 905. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004673190, boundary: 0.000101, weighted total: 0.00028322\n",
      "Epoch 906. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004656718, boundary: 0.000101, weighted total: 0.00028240\n",
      "Epoch 907. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004640903, boundary: 0.000101, weighted total: 0.00028161\n",
      "Epoch 908. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004623604, boundary: 0.000101, weighted total: 0.00028075\n",
      "Epoch 909. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004631876, boundary: 0.000101, weighted total: 0.00028116\n",
      "Epoch 910. Current alpha: 0.502329, lr: 0.0000000000, r_lower: 107.8. Training losses: pinn: 0.0004690305, boundary: 0.000101, weighted total: 0.00028407\n",
      "Epoch 911. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004625686, boundary: 0.000101, weighted total: 0.00028222\n",
      "Epoch 912. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004528112, boundary: 0.000101, weighted total: 0.00027733\n",
      "Epoch 913. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004533880, boundary: 0.000101, weighted total: 0.00027762\n",
      "Epoch 914. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004735725, boundary: 0.000101, weighted total: 0.00028774\n",
      "Epoch 915. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004538038, boundary: 0.000101, weighted total: 0.00027783\n",
      "Epoch 916. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004614214, boundary: 0.000101, weighted total: 0.00028165\n",
      "Epoch 917. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004624124, boundary: 0.000101, weighted total: 0.00028214\n",
      "Epoch 918. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004613739, boundary: 0.000101, weighted total: 0.00028162\n",
      "Epoch 919. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004581485, boundary: 0.000101, weighted total: 0.00028000\n",
      "Epoch 920. Current alpha: 0.498543, lr: 0.0000000000, r_lower: 107.7. Training losses: pinn: 0.0004643296, boundary: 0.000101, weighted total: 0.00028310\n",
      "Epoch 921. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004619946, boundary: 0.000101, weighted total: 0.00028329\n",
      "Epoch 922. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004575253, boundary: 0.000101, weighted total: 0.00028103\n",
      "Epoch 923. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004662873, boundary: 0.000101, weighted total: 0.00028546\n",
      "Epoch 924. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004481305, boundary: 0.000101, weighted total: 0.00027629\n",
      "Epoch 925. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004579759, boundary: 0.000101, weighted total: 0.00028126\n",
      "Epoch 926. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004567030, boundary: 0.000101, weighted total: 0.00028062\n",
      "Epoch 927. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004653969, boundary: 0.000101, weighted total: 0.00028501\n",
      "Epoch 928. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004597972, boundary: 0.000101, weighted total: 0.00028218\n",
      "Epoch 929. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004678556, boundary: 0.000101, weighted total: 0.00028625\n",
      "Epoch 930. Current alpha: 0.494785, lr: 0.0000000000, r_lower: 107.5. Training losses: pinn: 0.0004548128, boundary: 0.000101, weighted total: 0.00027966\n",
      "Epoch 931. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004560543, boundary: 0.000101, weighted total: 0.00028161\n",
      "Epoch 932. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004546435, boundary: 0.000101, weighted total: 0.00028090\n",
      "Epoch 933. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004630876, boundary: 0.000101, weighted total: 0.00028519\n",
      "Epoch 934. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004651377, boundary: 0.000101, weighted total: 0.00028624\n",
      "Epoch 935. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004652237, boundary: 0.000101, weighted total: 0.00028628\n",
      "Epoch 936. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004552898, boundary: 0.000101, weighted total: 0.00028122\n",
      "Epoch 937. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004642067, boundary: 0.000101, weighted total: 0.00028576\n",
      "Epoch 938. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004592528, boundary: 0.000101, weighted total: 0.00028324\n",
      "Epoch 939. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004678110, boundary: 0.000101, weighted total: 0.00028760\n",
      "Epoch 940. Current alpha: 0.491055, lr: 0.0000000000, r_lower: 107.4. Training losses: pinn: 0.0004548483, boundary: 0.000101, weighted total: 0.00028100\n",
      "Epoch 941. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004641915, boundary: 0.000101, weighted total: 0.00028710\n",
      "Epoch 942. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004558289, boundary: 0.000101, weighted total: 0.00028281\n",
      "Epoch 943. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004589456, boundary: 0.000101, weighted total: 0.00028441\n",
      "Epoch 944. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004708527, boundary: 0.000101, weighted total: 0.00029051\n",
      "Epoch 945. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004613387, boundary: 0.000101, weighted total: 0.00028564\n",
      "Epoch 946. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004492736, boundary: 0.000101, weighted total: 0.00027945\n",
      "Epoch 947. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004613098, boundary: 0.000101, weighted total: 0.00028562\n",
      "Epoch 948. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004644394, boundary: 0.000101, weighted total: 0.00028723\n",
      "Epoch 949. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004597651, boundary: 0.000101, weighted total: 0.00028483\n",
      "Epoch 950. Current alpha: 0.487354, lr: 0.0000000000, r_lower: 107.3. Training losses: pinn: 0.0004655938, boundary: 0.000101, weighted total: 0.00028782\n",
      "Epoch 951. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004573231, boundary: 0.000101, weighted total: 0.00028489\n",
      "Epoch 952. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004709242, boundary: 0.000101, weighted total: 0.00029191\n",
      "Epoch 953. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004659366, boundary: 0.000101, weighted total: 0.00028934\n",
      "Epoch 954. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004630882, boundary: 0.000101, weighted total: 0.00028787\n",
      "Epoch 955. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004604672, boundary: 0.000101, weighted total: 0.00028651\n",
      "Epoch 956. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004624335, boundary: 0.000101, weighted total: 0.00028753\n",
      "Epoch 957. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004597666, boundary: 0.000101, weighted total: 0.00028615\n",
      "Epoch 958. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004607895, boundary: 0.000101, weighted total: 0.00028668\n",
      "Epoch 959. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004608627, boundary: 0.000101, weighted total: 0.00028672\n",
      "Epoch 960. Current alpha: 0.483681, lr: 0.0000000000, r_lower: 107.2. Training losses: pinn: 0.0004530878, boundary: 0.000101, weighted total: 0.00028270\n",
      "Epoch 961. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004566289, boundary: 0.000101, weighted total: 0.00028583\n",
      "Epoch 962. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004593176, boundary: 0.000101, weighted total: 0.00028723\n",
      "Epoch 963. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004561882, boundary: 0.000101, weighted total: 0.00028560\n",
      "Epoch 964. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004581038, boundary: 0.000101, weighted total: 0.00028659\n",
      "Epoch 965. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004595384, boundary: 0.000101, weighted total: 0.00028734\n",
      "Epoch 966. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004482922, boundary: 0.000101, weighted total: 0.00028149\n",
      "Epoch 967. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004603533, boundary: 0.000101, weighted total: 0.00028776\n",
      "Epoch 968. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004591570, boundary: 0.000101, weighted total: 0.00028714\n",
      "Epoch 969. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004576464, boundary: 0.000101, weighted total: 0.00028636\n",
      "Epoch 970. Current alpha: 0.480035, lr: 0.0000000000, r_lower: 107.0. Training losses: pinn: 0.0004611095, boundary: 0.000101, weighted total: 0.00028816\n",
      "Epoch 971. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004595003, boundary: 0.000101, weighted total: 0.00028862\n",
      "Epoch 972. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004630202, boundary: 0.000101, weighted total: 0.00029046\n",
      "Epoch 973. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004582644, boundary: 0.000101, weighted total: 0.00028797\n",
      "Epoch 974. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004584938, boundary: 0.000101, weighted total: 0.00028809\n",
      "Epoch 975. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004634359, boundary: 0.000101, weighted total: 0.00029068\n",
      "Epoch 976. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004554095, boundary: 0.000101, weighted total: 0.00028648\n",
      "Epoch 977. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004566694, boundary: 0.000101, weighted total: 0.00028714\n",
      "Epoch 978. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004596081, boundary: 0.000101, weighted total: 0.00028867\n",
      "Epoch 979. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004475716, boundary: 0.000101, weighted total: 0.00028237\n",
      "Epoch 980. Current alpha: 0.476417, lr: 0.0000000000, r_lower: 106.9. Training losses: pinn: 0.0004682264, boundary: 0.000101, weighted total: 0.00029319\n",
      "Epoch 981. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004542684, boundary: 0.000101, weighted total: 0.00028715\n",
      "Epoch 982. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004577985, boundary: 0.000101, weighted total: 0.00028901\n",
      "Epoch 983. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004531249, boundary: 0.000101, weighted total: 0.00028655\n",
      "Epoch 984. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004588188, boundary: 0.000101, weighted total: 0.00028955\n",
      "Epoch 985. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004514071, boundary: 0.000101, weighted total: 0.00028564\n",
      "Epoch 986. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004604767, boundary: 0.000101, weighted total: 0.00029042\n",
      "Epoch 987. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004496968, boundary: 0.000101, weighted total: 0.00028474\n",
      "Epoch 988. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004593099, boundary: 0.000101, weighted total: 0.00028981\n",
      "Epoch 989. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004618383, boundary: 0.000101, weighted total: 0.00029114\n",
      "Epoch 990. Current alpha: 0.472826, lr: 0.0000000000, r_lower: 106.8. Training losses: pinn: 0.0004681761, boundary: 0.000101, weighted total: 0.00029448\n",
      "Epoch 991. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004580617, boundary: 0.000101, weighted total: 0.00029042\n",
      "Epoch 992. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004489114, boundary: 0.000101, weighted total: 0.00028556\n",
      "Epoch 993. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004618818, boundary: 0.000101, weighted total: 0.00029245\n",
      "Epoch 994. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004537635, boundary: 0.000101, weighted total: 0.00028814\n",
      "Epoch 995. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004581108, boundary: 0.000101, weighted total: 0.00029045\n",
      "Epoch 996. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004638065, boundary: 0.000101, weighted total: 0.00029347\n",
      "Epoch 997. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004632559, boundary: 0.000101, weighted total: 0.00029318\n",
      "Epoch 998. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004656115, boundary: 0.000101, weighted total: 0.00029443\n",
      "Epoch 999. Current alpha: 0.469262, lr: 0.0000000000, r_lower: 106.7. Training losses: pinn: 0.0004511548, boundary: 0.000101, weighted total: 0.00028676\n"
     ]
    }
   ],
   "source": [
    "# Train the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], f_boundary=f_boundary[:, 0], f_bound=f_bound, size=size, n_samples=n_samples)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_predict, client=None, trial=None, alpha=alpha, beta=beta, batchsize=batchsize, \n",
    "                                                 boundary_batchsize=boundary_batchsize, epochs=epochs, lr=lr, size=size, save=save, load_epoch=load_epoch, \n",
    "                                                 lr_decay=lr_decay, alpha_decay=alpha_decay, r_lower_change=r_lower_change, alpha_limit=alpha_limit, \n",
    "                                                 patience=patience, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1efedbc-5843-44e8-981d-b48ca8bf011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open(OUTPUTS_PATH + '/pinn_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open(OUTPUTS_PATH + '/boundary_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "     \n",
    "with open(OUTPUTS_PATH + '/predictions_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b77f7-94b5-4e71-a474-83faf92d8618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
