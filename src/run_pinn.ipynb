{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7e6e69-8377-4874-aee5-22f99dbf95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 08:56:45.329566: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-20 08:56:45.329633: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gpu-0009\n",
      "2022-12-20 08:56:45.329643: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gpu-0009\n",
      "2022-12-20 08:56:45.329764: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.43.4\n",
      "2022-12-20 08:56:45.329813: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.43.4\n",
      "2022-12-20 08:56:45.329821: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.43.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfm = tf.math\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8fd5e8-fb2d-4ec5-a104-5f606294015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "CURRENT_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"data\"))\n",
    "OUTPUTS_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \"..\", \"outputs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4ede82-5306-4443-be72-353fbda27c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "size = 1024\n",
    "\n",
    "with open(DATA_PATH + '/f_boundary_' + str(size) + '.pkl', 'rb') as file:\n",
    "    f_boundary = pkl.load(file)\n",
    "\n",
    "with open(DATA_PATH + '/p_' + str(size) + '.pkl', 'rb') as file:\n",
    "    p = pkl.load(file)\n",
    "\n",
    "with open(DATA_PATH + '/T_' + str(size) + '.pkl', 'rb') as file:\n",
    "    T = pkl.load(file)\n",
    "\n",
    "with open(DATA_PATH + '/r_' + str(size) + '.pkl', 'rb') as file:\n",
    "    r = pkl.load(file)\n",
    "\n",
    "with open(DATA_PATH + '/P_predict_' + str(size) + '.pkl', 'rb') as file:\n",
    "    P_predict = pkl.load(file)\n",
    "\n",
    "# Get upper and lower bounds\n",
    "lb = np.log(np.array([p[0], r[0]], dtype='float32'))\n",
    "ub = np.log(np.array([p[-1], r[-1]], dtype='float32'))\n",
    "min_f_log_space = -34.54346331847909\n",
    "max_f_log_space = 6.466899920699378\n",
    "f_bound = np.array([min_f_log_space, max_f_log_space], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7dab597-dc24-4b1c-a440-00e78daca9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, f_boundary, f_bound, size, num_samples=20_000):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound # In log space\n",
    "        self.upper_bound = upper_bound # In log space\n",
    "        self.p = p # In real space\n",
    "        self.f_boundary = f_boundary # In scaled space (0 to 1)\n",
    "        self.num_samples = num_samples\n",
    "        self.size = size\n",
    "        self.f_bound = f_bound # In log space\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and f_boundary.\n",
    "        \n",
    "        g_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs. G_boundary is the\n",
    "                    scaled version of f_boundary, and relates to f_boundary via g = (log(f) - min(log(f)))/(max(log(f)) - min(log(f)))\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "        beta: weight on pinn_loss to scale it to the same order of magnitude as boundary_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha, beta):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                t1.watch(p)\n",
    "                t1.watch(r)\n",
    "                t1.watch(p_boundary)\n",
    "                t1.watch(r_boundary)\n",
    "                \n",
    "                # PINN loss data\n",
    "                p_scaled = self.scale(p, self.upper_bound[0], self.lower_bound[0], should_take_log=True)\n",
    "                r_scaled = self.scale(r, self.upper_bound[1], self.lower_bound[1], should_take_log=True)\n",
    "                P = tf.concat((p_scaled, r_scaled), axis=1)\n",
    "                g = self.tf_call(P)\n",
    "\n",
    "                # Boundary loss data\n",
    "                p_boundary_scaled = self.scale(p_boundary, self.upper_bound[0], self.lower_bound[0], should_take_log=True)\n",
    "                r_boundary_scaled = self.scale(r_boundary, self.upper_bound[1], self.lower_bound[1], should_take_log=True)\n",
    "                P_boundary = tf.concat((p_boundary_scaled, r_boundary_scaled), axis=1)\n",
    "                g_pred_boundary = self.tf_call(P_boundary)\n",
    "                \n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tfm.reduce_mean(tfm.square(g_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order gradients\n",
    "            dg_dp = t1.gradient(g, p)\n",
    "            dg_dr = t1.gradient(g, r)\n",
    "            \n",
    "            dg_dp_boundary = t1.gradient(g_pred_boundary, p_boundary)\n",
    "            dg_dr_boundary = t1.gradient(g_pred_boundary, r_boundary)\n",
    "            \n",
    "            # Calculate f (real space) from g (scaled sapce) and get df/dg\n",
    "            with tf.GradientTape(persistent=True) as t3: \n",
    "                t3.watch(g)\n",
    "                t3.watch(g_pred_boundary)\n",
    "                \n",
    "                diff = tfm.abs(self.f_bound[1] - self.f_bound[0])\n",
    "\n",
    "                f = tfm.exp(g*diff + self.f_bound[0])\n",
    "                f_pred_boundary = tfm.exp(g_pred_boundary*diff + self.f_bound[0])\n",
    "\n",
    "            df_dg = t3.gradient(f, g)\n",
    "            df_dg_boundary = t3.gradient(f_pred_boundary, g_pred_boundary)\n",
    "            \n",
    "            # Use chain rule to calculate df/dp and df/dr\n",
    "            df_dp = df_dg*dg_dp\n",
    "            df_dr = df_dg*dg_dr\n",
    "            \n",
    "            df_dp_boundary = df_dg_boundary*dg_dp_boundary\n",
    "            df_dr_boundary = df_dg_boundary*dg_dr_boundary\n",
    "            \n",
    "            # Calculate PINN loss and total loss\n",
    "            pinn_loss = beta*(self.pinn_loss(p, r, df_dp, df_dr)[1] + self.pinn_loss(p_boundary, r_boundary, df_dp_boundary, df_dr_boundary)[1])\n",
    "            total_loss = (1-alpha)*pinn_loss + alpha*boundary_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Get mask of values to be re-sampled during evolutionary sampling\n",
    "        residuals = tfm.abs(self.pinn_loss(p, r, df_dp, df_dr)[0])\n",
    "        threshold = tfm.reduce_mean(residuals)\n",
    "        mask = tfm.greater(residuals, threshold)\n",
    "        \n",
    "        # Return losses and the mask\n",
    "        return pinn_loss.numpy(), boundary_loss.numpy(), p, r, mask\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        client, trial: Sherpa client and trial\n",
    "        \n",
    "        beta: weight on pinn_loss to scale it to the same order of magnitude as boundary_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_schedule: Determines the schedule lr will be on. Options include 'decay' and 'oscillate', else lr will remain constant\n",
    "\n",
    "        alpha_schedule: Determines the schedule alpha will be on. Choices include 'decay', 'grow', and 'oscillate'\n",
    "        \n",
    "        patience: Number of epochs to check whether loss has decreased before decaying lr, if lr_schedule='decay'\n",
    "        \n",
    "        num_cycles = Number of cycles to oscillate lr for, if lr_schedule='oscillate'\n",
    "        \n",
    "        filename: Name for the checkpoint file\n",
    "        \n",
    "        adam_beta1 = Value of beta1 for the Adam optimizer. Changes emphasis on momentum during training\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, client=None, trial=None, beta=1, batchsize=64, boundary_batchsize=16, epochs=20, \n",
    "            lr=3e-3, size=256, save=False, load_epoch=-1, lr_schedule='', alpha_schedule='', patience=10, num_cycles=10, \n",
    "            filename='', adam_beta1=0.9):\n",
    "        \n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(load_epoch)\n",
    "            self.load_weights(name)\n",
    "        \n",
    "        # Initialize variables for oscillating lr schedule and evolutionary sampling\n",
    "        just_decreased = False\n",
    "        max_lr = lr\n",
    "        min_lr = max_lr/1000\n",
    "        stepsize = (max_lr-min_lr)/(epochs/(num_cycles/2))\n",
    "        mask = None\n",
    "        \n",
    "        # Initialize\n",
    "        steps_per_epoch = np.ceil(self.num_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs,))\n",
    "        total_boundary_loss = np.zeros((epochs,))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # Initialize alpha based on alpha_schedule\n",
    "        if (alpha_schedule == 'decay'): alpha = 1.0\n",
    "        elif (alpha_schedule == 'grow'): alpha = 0.001\n",
    "        else: alpha = 0.5\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=adam_beta1)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, sample data and pass to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                uniform_dist = tfd.Uniform(0, 1)\n",
    "                p_new = tfm.exp((uniform_dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[0] - self.lower_bound[0])) + self.lower_bound[0])\n",
    "                r_new = (uniform_dist.sample((batchsize, 1))*tfm.abs(tfm.exp(self.upper_bound[1]) - tfm.exp(self.lower_bound[1]))) + tfm.exp(self.lower_bound[1])\n",
    "\n",
    "                # If there is a mask, update p and r with evolutionary sampling\n",
    "                if mask != None:\n",
    "                    p_update = tf.where(mask, p_old, p_new).numpy()\n",
    "                    p = tf.convert_to_tensor(p_update, dtype=tf.float32)\n",
    "                    \n",
    "                    r_update = tf.where(mask, r_old, r_new).numpy()\n",
    "                    r = tf.convert_to_tensor(r_update, dtype=tf.float32)\n",
    "                \n",
    "                # If there isn't a mask, keep p_new and r_new\n",
    "                else:\n",
    "                    p = p_new\n",
    "                    r = r_new\n",
    "                    \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = tf.convert_to_tensor(self.p[p_idx], dtype=tf.float32)\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_boundary = np.zeros((boundary_batchsize, 1))\n",
    "                upper_boundary[:] = tfm.exp(self.upper_bound[1])\n",
    "                r_boundary = tf.convert_to_tensor(upper_boundary, dtype=tf.float32)\n",
    "                \n",
    "                # Train and get loss and new p and r\n",
    "                step_pinn_loss, step_boundary_loss, p_old, r_old, mask = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\n",
    "                pinn_loss[step] = step_pinn_loss\n",
    "                boundary_loss[step] = step_boundary_loss\n",
    "            \n",
    "            # Sum losses\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Epoch {epoch}. Current alpha: {alpha:.6f}, lr: {lr:.10f}, ' + \n",
    "                  f'Training losses: pinn: {total_pinn_loss[epoch]:.10f}, boundary: {total_boundary_loss[epoch]:.10f}, ' +\n",
    "                  f'weighted total: {((alpha*total_boundary_loss[epoch])+((1-alpha)*total_pinn_loss[epoch])):.10f}')\n",
    "            \n",
    "            # Predict\n",
    "            predictions[:, :, epoch] = self.predict(P_predict, batchsize)\n",
    "            \n",
    "            # Adjust alpha based on the schedule, only every 10 epochs\n",
    "            if (epoch%10 == 0):\n",
    "                if alpha_schedule=='decay': alpha = alpha*0.995\n",
    "                elif (alpha_schedule=='grow') & (alpha <= 1): alpha = alpha*1.015\n",
    "\n",
    "            # Check if loss has decreased\n",
    "            hasnt_decreased = False\n",
    "            if (epoch > patience):\n",
    "                if (total_pinn_loss[epoch] + total_boundary_loss[epoch]) > (total_pinn_loss[epoch-patience] + total_boundary_loss[epoch-patience]):\n",
    "                    hasnt_decreased = True\n",
    "                        \n",
    "            # If loss hasn't decreased, adjust lr based on the assigned schedule\n",
    "            if ((lr_schedule == 'decay') & hasnt_decreased): lr = lr*0.95\n",
    "            elif (lr_schedule == 'oscillate'): \n",
    "                lr, just_decreased = self.oscillate_lr(just_decreased, lr, min_lr, max_lr, stepsize)\n",
    "\n",
    "            # Save the model to a checkpoint\n",
    "            should_save = (epoch%100 == 0) & (save == True)\n",
    "            if should_save:\n",
    "                name = './outputs/ckpts/pinn_' + filename + '_epoch_' + str(epoch)\n",
    "                self.save_weights(name, overwrite=True, save_format=None, options=None)\n",
    "                \n",
    "            # Send metrics if running Sherpa optimization\n",
    "            if client:\n",
    "                if (np.isnan(total_pinn_loss[epoch]) and np.isnan(total_boundary_loss[epoch])):\n",
    "                    obj = np.inf\n",
    "                else:\n",
    "                    obj = total_pinn_loss[epoch] + total_boundary_loss[epoch]\n",
    "                client.send_metrics(\n",
    "                         trial=trial,\n",
    "                         iteration=epoch,\n",
    "                         objective=obj)\n",
    "        \n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, batchsize):\n",
    "        P_size = P.shape[0]\n",
    "        steps_per_epoch = np.ceil(P_size / batchsize).astype(int)\n",
    "        predictions = np.zeros((P_size, 1))\n",
    "        \n",
    "        # For each step predict on data between start and end indices\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # Calculate end_idx\n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P_size - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Predict\n",
    "            predictions[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, df_dp, df_dr):\n",
    "        V = 400 # 400 km/s\n",
    "        M = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tfm.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tfm.divide(p, tfm.sqrt(tfm.square(p) + tfm.square(M))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        residuals = df_dr + (tfm.divide(R*V, 3*k) * df_dp)\n",
    "        mse = tfm.reduce_mean(tfm.square(residuals))\n",
    "        \n",
    "        return residuals, mse\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)\n",
    "    \n",
    "    # Scales input data and returns scaled version\n",
    "    def scale(self, data, upper_bound, lower_bound, should_take_log=True):\n",
    "        if should_take_log: scaled_data = (tfm.log(data) - lower_bound)/tfm.abs(upper_bound - lower_bound)   \n",
    "        else: scaled_data = (data - lower_bound)/tfm.abs(upper_bound - lower_bound)\n",
    "            \n",
    "        return scaled_data\n",
    "    \n",
    "    f'''\n",
    "    Description: Implements an oscillating lr according to the triangular CLR schedule: decrease lr linearly to min_lr, then increase linearly to max_lr, and repeat\n",
    "    \n",
    "    Inputs: \n",
    "        just_decreased: Boolean, True if lr decreased last epoch and False if not\n",
    "        \n",
    "        lr: float, Current learning rate\n",
    "        \n",
    "        min_lr: float, minimum learning rate to decrease to. In literature defined as 1/R smaller than max_lr\n",
    "        \n",
    "        max_lr: float, maximum learning rate to increase to\n",
    "        \n",
    "        stepsize: float, value to increase/decrease lr by\n",
    "        \n",
    "    Outputs: \n",
    "        just_decreased and new lr\n",
    "    '''\n",
    "    def oscillate_lr(self, just_decreased, lr, min_lr, max_lr, stepsize):\n",
    "        decrease_lr = (just_decreased and (lr - stepsize >= min_lr)) or ((not just_decreased) and (lr + stepsize > max_lr))\n",
    "        increase_lr = ((not just_decreased) and (lr + stepsize <= max_lr)) or (just_decreased and (lr - stepsize < min_lr))\n",
    "\n",
    "        if decrease_lr: \n",
    "            lr = lr - stepsize\n",
    "            just_decreased = True\n",
    "        elif increase_lr:\n",
    "            lr = lr + stepsize\n",
    "            just_decreased = False\n",
    "\n",
    "        return lr, just_decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442d0c7b-4143-45df-95fc-a534727e3664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 08:56:45.468340: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 100\n",
    "beta = 1e13\n",
    "adam_beta1 = 0.9\n",
    "lr_schedule = 'decay'\n",
    "alpha_schedule = 'static'\n",
    "patience = 30\n",
    "num_cycles = 2\n",
    "batchsize = 1024\n",
    "boundary_batchsize = 512\n",
    "activation = 'selu'\n",
    "save = True\n",
    "load_epoch = -1\n",
    "num_samples = 20_000\n",
    "lr = 3e-3\n",
    "num_layers = 2\n",
    "num_hidden_units = 150\n",
    "final_activation = 'sigmoid'\n",
    "filename = 'test'\n",
    "\n",
    "\n",
    "# Create model\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(inputs)\n",
    "for _ in range(num_layers-1):\n",
    "    x_ = tf.keras.layers.Dense(num_hidden_units, activation=activation)(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation=final_activation)(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a9cf11-b7a4-4914-866b-6fa66a51ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0636048930, boundary: 0.5587629387, weighted total: 0.3111839159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 08:56:50.261653: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 589861800 exceeds 10% of free system memory.\n",
      "2022-12-20 08:56:51.497886: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 589861800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.1184786848, boundary: 0.8408425506, weighted total: 0.4796606177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 08:56:57.349109: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 589861800 exceeds 10% of free system memory.\n",
      "2022-12-20 08:56:58.499868: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 589861800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0437052744, boundary: 0.2682093284, weighted total: 0.1559573014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 08:57:03.641977: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 589861800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0306853272, boundary: 0.1959406717, weighted total: 0.1133129994\n",
      "Epoch 4. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0842694866, boundary: 0.3931190055, weighted total: 0.2386942460\n",
      "Epoch 5. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0470049044, boundary: 0.2757047275, weighted total: 0.1613548160\n",
      "Epoch 6. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0278922888, boundary: 0.1587222260, weighted total: 0.0933072574\n",
      "Epoch 7. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0197168262, boundary: 0.1271760538, weighted total: 0.0734464400\n",
      "Epoch 8. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0166087149, boundary: 0.1207678230, weighted total: 0.0686882689\n",
      "Epoch 9. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0100854562, boundary: 0.0991298379, weighted total: 0.0546076471\n",
      "Epoch 10. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0081653085, boundary: 0.0922997991, weighted total: 0.0502325538\n",
      "Epoch 11. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0047297413, boundary: 0.0723800987, weighted total: 0.0385549200\n",
      "Epoch 12. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0065404970, boundary: 0.0805069004, weighted total: 0.0435236987\n",
      "Epoch 13. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0053501312, boundary: 0.0623902398, weighted total: 0.0338701855\n",
      "Epoch 14. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0061520113, boundary: 0.0415907291, weighted total: 0.0238713702\n",
      "Epoch 15. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0060696587, boundary: 0.0595227232, weighted total: 0.0327961909\n",
      "Epoch 16. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0056185561, boundary: 0.0675960841, weighted total: 0.0366073201\n",
      "Epoch 17. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0055766023, boundary: 0.0487949761, weighted total: 0.0271857892\n",
      "Epoch 18. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0062936291, boundary: 0.0323992359, weighted total: 0.0193464325\n",
      "Epoch 19. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0110962340, boundary: 0.0395169526, weighted total: 0.0253065933\n",
      "Epoch 20. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0044048025, boundary: 0.0543232264, weighted total: 0.0293640145\n",
      "Epoch 21. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0033941344, boundary: 0.0434215397, weighted total: 0.0234078371\n",
      "Epoch 22. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0040341419, boundary: 0.0384495056, weighted total: 0.0212418237\n",
      "Epoch 23. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0037118336, boundary: 0.0415841413, weighted total: 0.0226479874\n",
      "Epoch 24. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0039878552, boundary: 0.0389647160, weighted total: 0.0214762856\n",
      "Epoch 25. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0066814258, boundary: 0.0294678001, weighted total: 0.0180746129\n",
      "Epoch 26. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0065816214, boundary: 0.0447836766, weighted total: 0.0256826490\n",
      "Epoch 27. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0039637083, boundary: 0.0339978812, weighted total: 0.0189807947\n",
      "Epoch 28. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0051683981, boundary: 0.0245338032, weighted total: 0.0148511007\n",
      "Epoch 29. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0065819110, boundary: 0.0276645217, weighted total: 0.0171232163\n",
      "Epoch 30. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0050382728, boundary: 0.0331828768, weighted total: 0.0191105748\n",
      "Epoch 31. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0054075393, boundary: 0.0353888322, weighted total: 0.0203981857\n",
      "Epoch 32. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0062806481, boundary: 0.0262977452, weighted total: 0.0162891966\n",
      "Epoch 33. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0039772327, boundary: 0.0275190116, weighted total: 0.0157481222\n",
      "Epoch 34. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0035639357, boundary: 0.0332401705, weighted total: 0.0184020531\n",
      "Epoch 35. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0046473406, boundary: 0.0394906657, weighted total: 0.0220690031\n",
      "Epoch 36. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0038522237, boundary: 0.0329371232, weighted total: 0.0183946735\n",
      "Epoch 37. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0052307979, boundary: 0.0328662870, weighted total: 0.0190485425\n",
      "Epoch 38. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0057831818, boundary: 0.0282427103, weighted total: 0.0170129460\n",
      "Epoch 39. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0046572846, boundary: 0.0316637066, weighted total: 0.0181604956\n",
      "Epoch 40. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0039948604, boundary: 0.0339078229, weighted total: 0.0189513416\n",
      "Epoch 41. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0049185589, boundary: 0.0309253715, weighted total: 0.0179219652\n",
      "Epoch 42. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0038544598, boundary: 0.0316926952, weighted total: 0.0177735775\n",
      "Epoch 43. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0038202429, boundary: 0.0354606492, weighted total: 0.0196404460\n",
      "Epoch 44. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0035716799, boundary: 0.0263904561, weighted total: 0.0149810680\n",
      "Epoch 45. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0030842336, boundary: 0.0307503936, weighted total: 0.0169173136\n",
      "Epoch 46. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0037149126, boundary: 0.0305948072, weighted total: 0.0171548599\n",
      "Epoch 47. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0042712132, boundary: 0.0323726749, weighted total: 0.0183219441\n",
      "Epoch 48. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0042957898, boundary: 0.0278475471, weighted total: 0.0160716684\n",
      "Epoch 49. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0032195075, boundary: 0.0302400257, weighted total: 0.0167297666\n",
      "Epoch 50. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0036152051, boundary: 0.0299502189, weighted total: 0.0167827120\n",
      "Epoch 51. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0045703055, boundary: 0.0305399750, weighted total: 0.0175551403\n",
      "Epoch 52. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0038393431, boundary: 0.0280843959, weighted total: 0.0159618695\n",
      "Epoch 53. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0054840696, boundary: 0.0314393703, weighted total: 0.0184617199\n",
      "Epoch 54. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0037361933, boundary: 0.0243564267, weighted total: 0.0140463100\n",
      "Epoch 55. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0040045696, boundary: 0.0280507751, weighted total: 0.0160276723\n",
      "Epoch 56. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0068018656, boundary: 0.0254723572, weighted total: 0.0161371114\n",
      "Epoch 57. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0051131802, boundary: 0.0271792411, weighted total: 0.0161462107\n",
      "Epoch 58. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0046566098, boundary: 0.0201689317, weighted total: 0.0124127708\n",
      "Epoch 59. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0050030212, boundary: 0.0180150619, weighted total: 0.0115090415\n",
      "Epoch 60. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0053340545, boundary: 0.0233383169, weighted total: 0.0143361857\n",
      "Epoch 61. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0044410189, boundary: 0.0240432504, weighted total: 0.0142421346\n",
      "Epoch 62. Current alpha: 0.500000, lr: 0.0030000000, Training losses: pinn: 0.0058683100, boundary: 0.0280286956, weighted total: 0.0169485028\n",
      "Epoch 63. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0046705045, boundary: 0.0243567975, weighted total: 0.0145136510\n",
      "Epoch 64. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0057001474, boundary: 0.0188740276, weighted total: 0.0122870875\n",
      "Epoch 65. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0057094590, boundary: 0.0240137777, weighted total: 0.0148616183\n",
      "Epoch 66. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0048505781, boundary: 0.0224915891, weighted total: 0.0136710836\n",
      "Epoch 67. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0052140791, boundary: 0.0241841641, weighted total: 0.0146991216\n",
      "Epoch 68. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0046463856, boundary: 0.0248232697, weighted total: 0.0147348276\n",
      "Epoch 69. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0073326046, boundary: 0.0197702622, weighted total: 0.0135514334\n",
      "Epoch 70. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0060289744, boundary: 0.0227852119, weighted total: 0.0144070931\n",
      "Epoch 71. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0047865848, boundary: 0.0254421565, weighted total: 0.0151143706\n",
      "Epoch 72. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0059538654, boundary: 0.0211307928, weighted total: 0.0135423291\n",
      "Epoch 73. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0050942736, boundary: 0.0189622873, weighted total: 0.0120282804\n",
      "Epoch 74. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0048949407, boundary: 0.0194667199, weighted total: 0.0121808303\n",
      "Epoch 75. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0042773041, boundary: 0.0178902661, weighted total: 0.0110837851\n",
      "Epoch 76. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0037934437, boundary: 0.0178889600, weighted total: 0.0108412018\n",
      "Epoch 77. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0032001747, boundary: 0.0267978320, weighted total: 0.0149990034\n",
      "Epoch 78. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0027486668, boundary: 0.0235424411, weighted total: 0.0131455539\n",
      "Epoch 79. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0037138926, boundary: 0.0248114418, weighted total: 0.0142626672\n",
      "Epoch 80. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0040039975, boundary: 0.0243250525, weighted total: 0.0141645250\n",
      "Epoch 81. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0030186436, boundary: 0.0225610448, weighted total: 0.0127898442\n",
      "Epoch 82. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0040095604, boundary: 0.0187168378, weighted total: 0.0113631991\n",
      "Epoch 83. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0039149209, boundary: 0.0202556663, weighted total: 0.0120852936\n",
      "Epoch 84. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0036328670, boundary: 0.0217508093, weighted total: 0.0126918382\n",
      "Epoch 85. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0040230075, boundary: 0.0214675666, weighted total: 0.0127452871\n",
      "Epoch 86. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0040446652, boundary: 0.0162629990, weighted total: 0.0101538321\n",
      "Epoch 87. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0034184100, boundary: 0.0171447071, weighted total: 0.0102815585\n",
      "Epoch 88. Current alpha: 0.500000, lr: 0.0028500000, Training losses: pinn: 0.0029631716, boundary: 0.0231397480, weighted total: 0.0130514598\n",
      "Epoch 89. Current alpha: 0.500000, lr: 0.0027075000, Training losses: pinn: 0.0032915799, boundary: 0.0216172913, weighted total: 0.0124544356\n",
      "Epoch 90. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0027539690, boundary: 0.0201689905, weighted total: 0.0114614798\n",
      "Epoch 91. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0031094078, boundary: 0.0213096911, weighted total: 0.0122095495\n",
      "Epoch 92. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0033261395, boundary: 0.0187579698, weighted total: 0.0110420546\n",
      "Epoch 93. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0037334248, boundary: 0.0153980847, weighted total: 0.0095657548\n",
      "Epoch 94. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0032353859, boundary: 0.0186432902, weighted total: 0.0109393381\n",
      "Epoch 95. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0027602672, boundary: 0.0161574954, weighted total: 0.0094588813\n",
      "Epoch 96. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0033986099, boundary: 0.0159530812, weighted total: 0.0096758456\n",
      "Epoch 97. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0030166718, boundary: 0.0180055900, weighted total: 0.0105111309\n",
      "Epoch 98. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0026497170, boundary: 0.0141710573, weighted total: 0.0084103871\n",
      "Epoch 99. Current alpha: 0.500000, lr: 0.0025721250, Training losses: pinn: 0.0027318818, boundary: 0.0174879218, weighted total: 0.0101099018\n"
     ]
    }
   ],
   "source": [
    "# Train the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], f_boundary=f_boundary[:, 0], f_bound=f_bound, size=size, num_samples=num_samples)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_predict, client=None, trial=None, beta=beta, batchsize=batchsize, boundary_batchsize=boundary_batchsize, epochs=epochs, \n",
    "                                                 lr=lr, size=size, save=save, load_epoch=load_epoch, lr_schedule=lr_schedule, alpha_schedule=alpha_schedule, patience=patience, \n",
    "                                                 num_cycles=num_cycles, adam_beta1=adam_beta1, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1efedbc-5843-44e8-981d-b48ca8bf011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open(OUTPUTS_PATH + '/pinn_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open(OUTPUTS_PATH + '/boundary_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "     \n",
    "with open(OUTPUTS_PATH + '/predictions_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b77f7-94b5-4e71-a474-83faf92d8618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
