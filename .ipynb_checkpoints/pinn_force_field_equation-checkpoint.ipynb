{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Physics Informed Neural Networks\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook adapts the code implementation of Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs, and the code by Michael Ito, to solve the force-field equation for solar modulation of cosmic rays. Michael Ito's code adaption use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure. \n",
    "\n",
    "In this application, our PINN is $h(r, p) = \\frac{\\partial f}{\\partial r} + \\frac{RV}{3k} \\frac{\\partial f}{\\partial p}$ where $k=\\beta(p)k_1(r)k_2(r)$ and $\\beta = \\frac{p}{\\sqrt{p^2 + M^2}}$. We will approximate $f(r, p)$ using a neural network.\n",
    "\n",
    "We have no initial data, but our boundary data will be given by $f(r_{HP}, p) = \\frac{J(r_{HP}, T)}{p^2} = \\frac{(T+M)^\\gamma}{p^2}$, where $r_{HP} = 120$ AU (i.e. the radius of Heliopause), $M=0.938$ GeV, $\\gamma$ is between $-2$ and $-3$, and $T = \\sqrt{p^2 + M^2} - M$. Or, vice versa, $p = \\sqrt{T^2 + 2TM}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sherpa\n",
    "import pickle as pkl\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: (512, 1), p: (512, 1), T: (512, 1), f_boundary: (512, 1), P_star: (262144, 2), lb: [[-3.13904026]\n",
      " [18.82614585]], ub:[[ 6.9086924 ]\n",
      " [23.61363759]]\n"
     ]
    }
   ],
   "source": [
    "# Constants  \n",
    "m = 0.938 # GeV/c^2\n",
    "gamma = -3 # Between -2 and -3\n",
    "size = 512 # size of r, T, p, and f_boundary\n",
    "\n",
    "# Create intial r, p, and T predict data\n",
    "T = np.logspace(np.log10(0.001), np.log10(1000), size).flatten()[:,None] # GeV\n",
    "p = (np.sqrt((T+m)**2-m**2)).flatten()[:,None] # GeV/c\n",
    "r = (np.logspace(np.log10(1), np.log10(120), size)*150e6).flatten()[:,None] # km\n",
    "\n",
    "# Create boundary f data (f at r_HP) for boundary loss\n",
    "f_boundary = ((T + m)**gamma)/(p**2) # particles/(m^3 (GeV/c)^3)\n",
    "\n",
    "# Plot\n",
    "# plt.loglog(T, f_boundary*(p**2))\n",
    "# plt.xlabel(\"T (GeV)\")\n",
    "# plt.ylabel(\"J(r, T) = f(r, p)*p^2\")\n",
    "\n",
    "# Take the log of all input data\n",
    "r = np.log(r)\n",
    "T = np.log(T)\n",
    "p = np.log(p)\n",
    "f_boundary = np.log(f_boundary)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([p[0], r[0]]) # (p, r) in (GeV, AU)\n",
    "ub = np.array([p[-1], r[-1]]) # (p, r) in (GeV, AU)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "P, R = np.meshgrid(p, r)\n",
    "P_star = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))\n",
    "\n",
    "# Check inputs\n",
    "print(f'r: {r.shape}, p: {p.shape}, T: {T.shape}, f_boundary: {f_boundary.shape}, P_star: {P_star.shape}, lb: {lb}, ub:{ub}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, r, f_boundary, size, n_samples=20000, n_boundary=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.r = r\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.n_boundary = n_boundary\n",
    "        self.size = size\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and boundary_f.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "        alpha = weight on pinn_loss\n",
    "        \n",
    "        beta = weight on boundary_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha=1, beta=1):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                # Forward pass P (PINN data)\n",
    "                P = tf.concat((p, r), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Forward pass P_boundary (boundary condition data)\n",
    "                P_boundary = tf.concat((p_boundary, r_boundary), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "\n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tf.math.reduce_mean(tf.math.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            f_p = t1.gradient(f, p)\n",
    "            f_r = t1.gradient(f, r)\n",
    "\n",
    "            # Calculate resulting loss = PINN loss + boundary loss\n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r)\n",
    "            sum_loss = alpha*pinn_loss + beta*boundary_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(sum_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Return losses\n",
    "        return sum_loss.numpy(), pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        alpha = weight on pinn_loss\n",
    "        \n",
    "        beta = weight on boundary_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be loaded from the provided epoch\n",
    "        \n",
    "        threshold: If boundary and pinn loss fall below thrshold, quit training\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, size, alpha=1, beta=1, batchsize=64, boundary_batchsize=16, epochs=20, save=False, load_epoch=-1, threshold=1)#, lr_decay=-1):\n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            self.load_weights('./ckpts/pinn_epoch_' + str(load_epoch))\n",
    "        \n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        total_loss = np.zeros((epochs, ))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Reset loss variables\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN and boundary variables and pass them to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN p and r variables via uniform distribution sampling between lower and upper bounds\n",
    "                p = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                r = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = self.p[p_idx]\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound[:] = self.upper_bound[1]\n",
    "                r_boundary = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                \n",
    "                # Pass variables through the model via train_step and get losses\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\n",
    "                sum_loss[step] = losses[0]\n",
    "                pinn_loss[step] = losses[1]\n",
    "                boundary_loss[step] = losses[2]\n",
    "            \n",
    "            # Calculate and print total losses for the epoch\n",
    "            total_loss[epoch] = np.sum(sum_loss)\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Training loss for epoch {epoch}: pinn: {total_pinn_loss[epoch]:.4f}, boundary: {total_boundary_loss[epoch]:.4f}, total: {total_loss[epoch]:.4f}')\n",
    "            \n",
    "            # Predict\n",
    "            predictions[:, :, epoch] = self.predict(P_predict, size)\n",
    "            \n",
    "            # If the epoch is a multiple of 10, save to a checkpoint\n",
    "            if (epoch%10 == 0) & (save == True):\n",
    "                self.save_weights('./ckpts/pinn_epoch_' + str(epoch), overwrite=True, save_format=None, options=None)\n",
    "                \n",
    "            # If boundary_loss falls below certain threshold, break out the loop\n",
    "            if (total_boundary_loss[epoch] < threshold) & (total_pinn_loss[epoch] < threshold):\n",
    "                break\n",
    "        \n",
    "        # Return epoch losses\n",
    "        return total_loss, total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, size, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(P.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((size**2, 1))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Pass prediction data through the model\n",
    "            preds[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        # Return f\n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r): \n",
    "        # Note: p and r are taken out of logspace for the PINN calculation\n",
    "        p = tf.math.exp(p) # GeV/c\n",
    "        r = tf.math.exp(r) # km\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tf.math.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tf.math.divide(p, tf.math.sqrt(tf.math.square(p) + tf.math.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tf.math.reduce_mean(tf.math.abs(f_r + (tf.math.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network regularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 0: pinn: 0.4118, boundary: 388.7675, total: 392.8854\n",
      "Training loss for epoch 1: pinn: 0.3538, boundary: 347.3234, total: 350.8617\n",
      "Training loss for epoch 2: pinn: 0.5341, boundary: 298.2415, total: 303.5823\n",
      "Training loss for epoch 3: pinn: 0.8497, boundary: 241.1384, total: 249.6359\n",
      "Training loss for epoch 4: pinn: 1.3574, boundary: 167.9850, total: 181.5589\n",
      "Training loss for epoch 5: pinn: 2.0175, boundary: 107.9615, total: 128.1370\n",
      "Training loss for epoch 6: pinn: 2.2968, boundary: 66.2051, total: 89.1733\n",
      "Training loss for epoch 7: pinn: 2.2762, boundary: 45.7898, total: 68.5515\n",
      "Training loss for epoch 8: pinn: 1.8594, boundary: 39.0745, total: 57.6684\n",
      "Training loss for epoch 9: pinn: 1.5241, boundary: 34.8674, total: 50.1081\n",
      "Training loss for epoch 10: pinn: 1.3904, boundary: 30.6005, total: 44.5048\n",
      "Training loss for epoch 11: pinn: 1.1891, boundary: 26.7802, total: 38.6711\n",
      "Training loss for epoch 12: pinn: 1.0588, boundary: 22.4274, total: 33.0153\n",
      "Training loss for epoch 13: pinn: 0.9678, boundary: 17.3958, total: 27.0741\n",
      "Training loss for epoch 14: pinn: 0.8145, boundary: 13.2608, total: 21.4054\n",
      "Training loss for epoch 15: pinn: 0.7783, boundary: 9.3661, total: 17.1492\n",
      "Training loss for epoch 16: pinn: 0.7173, boundary: 6.6369, total: 13.8097\n",
      "Training loss for epoch 17: pinn: 0.6997, boundary: 4.9927, total: 11.9898\n",
      "Training loss for epoch 18: pinn: 0.7907, boundary: 5.1295, total: 13.0364\n",
      "Training loss for epoch 19: pinn: 0.8068, boundary: 4.8211, total: 12.8895\n",
      "Training loss for epoch 20: pinn: 0.8271, boundary: 3.4106, total: 11.6817\n",
      "Training loss for epoch 21: pinn: 0.7934, boundary: 3.0976, total: 11.0319\n",
      "Training loss for epoch 22: pinn: 0.7465, boundary: 2.3312, total: 9.7964\n",
      "Training loss for epoch 23: pinn: 0.7257, boundary: 2.2053, total: 9.4618\n",
      "Training loss for epoch 24: pinn: 0.6868, boundary: 2.3367, total: 9.2052\n",
      "Training loss for epoch 25: pinn: 0.6819, boundary: 2.1164, total: 8.9355\n",
      "Training loss for epoch 26: pinn: 0.6841, boundary: 1.7581, total: 8.5993\n",
      "Training loss for epoch 27: pinn: 0.6510, boundary: 1.9339, total: 8.4440\n",
      "Training loss for epoch 28: pinn: 0.6565, boundary: 1.9204, total: 8.4855\n",
      "Training loss for epoch 29: pinn: 0.6276, boundary: 1.7839, total: 8.0600\n",
      "Training loss for epoch 30: pinn: 0.6362, boundary: 1.6238, total: 7.9858\n",
      "Training loss for epoch 31: pinn: 0.6093, boundary: 1.3663, total: 7.4591\n",
      "Training loss for epoch 32: pinn: 0.6094, boundary: 1.5279, total: 7.6217\n",
      "Training loss for epoch 33: pinn: 0.5928, boundary: 0.9186, total: 6.8463\n",
      "Training loss for epoch 34: pinn: 0.5707, boundary: 1.6340, total: 7.3409\n",
      "Training loss for epoch 35: pinn: 0.5495, boundary: 1.7624, total: 7.2572\n",
      "Training loss for epoch 36: pinn: 0.5481, boundary: 1.2527, total: 6.7339\n",
      "Training loss for epoch 37: pinn: 0.5547, boundary: 1.1457, total: 6.6922\n",
      "Training loss for epoch 38: pinn: 0.5207, boundary: 1.3497, total: 6.5564\n",
      "Training loss for epoch 39: pinn: 0.5113, boundary: 1.0033, total: 6.1168\n",
      "Training loss for epoch 40: pinn: 0.4899, boundary: 1.2929, total: 6.1918\n",
      "Training loss for epoch 41: pinn: 0.4818, boundary: 1.7178, total: 6.5355\n",
      "Training loss for epoch 42: pinn: 0.4734, boundary: 0.8059, total: 5.5398\n",
      "Training loss for epoch 43: pinn: 0.4582, boundary: 0.6186, total: 5.2008\n",
      "Training loss for epoch 44: pinn: 0.4493, boundary: 1.0426, total: 5.5353\n",
      "Training loss for epoch 45: pinn: 0.4289, boundary: 0.5928, total: 4.8815\n",
      "Training loss for epoch 46: pinn: 0.4368, boundary: 1.4592, total: 5.8270\n",
      "Training loss for epoch 47: pinn: 0.4257, boundary: 0.8823, total: 5.1389\n",
      "Training loss for epoch 48: pinn: 0.4006, boundary: 0.6317, total: 4.6375\n",
      "Training loss for epoch 49: pinn: 0.3841, boundary: 0.5917, total: 4.4328\n",
      "Training loss for epoch 50: pinn: 0.3854, boundary: 0.7659, total: 4.6202\n",
      "Training loss for epoch 51: pinn: 0.3839, boundary: 1.1486, total: 4.9875\n",
      "Training loss for epoch 52: pinn: 0.3692, boundary: 1.3310, total: 5.0226\n",
      "Training loss for epoch 53: pinn: 0.3730, boundary: 1.2573, total: 4.9877\n",
      "Training loss for epoch 54: pinn: 0.3565, boundary: 0.8089, total: 4.3741\n",
      "Training loss for epoch 55: pinn: 0.3603, boundary: 0.9219, total: 4.5248\n",
      "Training loss for epoch 56: pinn: 0.3576, boundary: 0.8258, total: 4.4015\n",
      "Training loss for epoch 57: pinn: 0.3514, boundary: 0.8121, total: 4.3261\n",
      "Training loss for epoch 58: pinn: 0.3435, boundary: 0.8522, total: 4.2876\n",
      "Training loss for epoch 59: pinn: 0.3557, boundary: 1.9020, total: 5.4593\n",
      "Training loss for epoch 60: pinn: 0.3438, boundary: 0.6584, total: 4.0963\n",
      "Training loss for epoch 61: pinn: 0.3317, boundary: 0.7176, total: 4.0343\n",
      "Training loss for epoch 62: pinn: 0.3254, boundary: 0.7142, total: 3.9682\n",
      "Training loss for epoch 63: pinn: 0.3128, boundary: 0.5709, total: 3.6984\n",
      "Training loss for epoch 64: pinn: 0.3126, boundary: 0.4799, total: 3.6055\n",
      "Training loss for epoch 65: pinn: 0.3115, boundary: 0.9877, total: 4.1024\n",
      "Training loss for epoch 66: pinn: 0.3010, boundary: 0.7888, total: 3.7985\n",
      "Training loss for epoch 67: pinn: 0.2920, boundary: 0.5483, total: 3.4680\n",
      "Training loss for epoch 68: pinn: 0.2773, boundary: 0.7266, total: 3.4998\n",
      "Training loss for epoch 69: pinn: 0.2765, boundary: 0.6167, total: 3.3818\n",
      "Training loss for epoch 70: pinn: 0.2803, boundary: 1.5914, total: 4.3940\n",
      "Training loss for epoch 71: pinn: 0.2668, boundary: 0.9960, total: 3.6637\n",
      "Training loss for epoch 72: pinn: 0.2580, boundary: 0.7666, total: 3.3463\n",
      "Training loss for epoch 73: pinn: 0.2491, boundary: 0.7578, total: 3.2492\n",
      "Training loss for epoch 74: pinn: 0.2440, boundary: 0.5976, total: 3.0379\n",
      "Training loss for epoch 75: pinn: 0.2341, boundary: 0.8194, total: 3.1604\n",
      "Training loss for epoch 76: pinn: 0.2312, boundary: 0.7496, total: 3.0611\n",
      "Training loss for epoch 77: pinn: 0.2258, boundary: 0.9171, total: 3.1756\n",
      "Training loss for epoch 78: pinn: 0.2223, boundary: 0.7758, total: 2.9990\n",
      "Training loss for epoch 79: pinn: 0.2136, boundary: 0.5525, total: 2.6884\n",
      "Training loss for epoch 80: pinn: 0.2071, boundary: 0.5589, total: 2.6301\n",
      "Training loss for epoch 81: pinn: 0.2099, boundary: 1.2142, total: 3.3136\n",
      "Training loss for epoch 82: pinn: 0.2012, boundary: 0.6153, total: 2.6278\n",
      "Training loss for epoch 83: pinn: 0.1976, boundary: 0.6142, total: 2.5907\n",
      "Training loss for epoch 84: pinn: 0.1918, boundary: 0.7650, total: 2.6828\n",
      "Training loss for epoch 85: pinn: 0.1921, boundary: 0.6198, total: 2.5410\n",
      "Training loss for epoch 86: pinn: 0.1922, boundary: 0.8879, total: 2.8103\n",
      "Training loss for epoch 87: pinn: 0.1932, boundary: 1.1462, total: 3.0783\n",
      "Training loss for epoch 88: pinn: 0.1815, boundary: 0.6632, total: 2.4787\n",
      "Training loss for epoch 89: pinn: 0.1749, boundary: 0.9902, total: 2.7393\n",
      "Training loss for epoch 90: pinn: 0.1716, boundary: 0.6120, total: 2.3280\n",
      "Training loss for epoch 91: pinn: 0.1716, boundary: 1.0905, total: 2.8062\n",
      "Training loss for epoch 92: pinn: 0.1631, boundary: 0.8650, total: 2.4964\n",
      "Training loss for epoch 93: pinn: 0.1634, boundary: 0.7318, total: 2.3658\n",
      "Training loss for epoch 94: pinn: 0.1522, boundary: 0.5736, total: 2.0951\n",
      "Training loss for epoch 95: pinn: 0.1636, boundary: 1.3654, total: 3.0013\n",
      "Training loss for epoch 96: pinn: 0.1534, boundary: 0.6225, total: 2.1562\n",
      "Training loss for epoch 97: pinn: 0.1523, boundary: 0.4959, total: 2.0187\n",
      "Training loss for epoch 98: pinn: 0.1483, boundary: 0.5062, total: 1.9888\n",
      "Training loss for epoch 99: pinn: 0.1588, boundary: 1.0293, total: 2.6177\n",
      "Training loss for epoch 100: pinn: 0.1564, boundary: 1.5036, total: 3.0673\n",
      "Training loss for epoch 101: pinn: 0.1409, boundary: 0.5175, total: 1.9265\n",
      "Training loss for epoch 102: pinn: 0.1384, boundary: 0.6732, total: 2.0577\n",
      "Training loss for epoch 103: pinn: 0.1364, boundary: 0.7170, total: 2.0807\n",
      "Training loss for epoch 104: pinn: 0.1337, boundary: 0.7620, total: 2.0989\n",
      "Training loss for epoch 105: pinn: 0.1273, boundary: 0.8144, total: 2.0871\n",
      "Training loss for epoch 106: pinn: 0.1226, boundary: 0.7933, total: 2.0190\n",
      "Training loss for epoch 107: pinn: 0.1356, boundary: 1.2380, total: 2.5940\n",
      "Training loss for epoch 108: pinn: 0.1306, boundary: 1.3612, total: 2.6671\n",
      "Training loss for epoch 109: pinn: 0.1166, boundary: 0.5544, total: 1.7209\n",
      "Training loss for epoch 110: pinn: 0.1192, boundary: 1.0040, total: 2.1961\n",
      "Training loss for epoch 111: pinn: 0.1177, boundary: 0.9691, total: 2.1461\n",
      "Training loss for epoch 112: pinn: 0.1100, boundary: 0.5802, total: 1.6805\n",
      "Training loss for epoch 113: pinn: 0.1107, boundary: 0.6237, total: 1.7311\n",
      "Training loss for epoch 114: pinn: 0.1097, boundary: 0.4618, total: 1.5591\n",
      "Training loss for epoch 115: pinn: 0.1125, boundary: 0.8303, total: 1.9554\n",
      "Training loss for epoch 116: pinn: 0.1118, boundary: 0.9707, total: 2.0883\n",
      "Training loss for epoch 117: pinn: 0.1033, boundary: 0.4780, total: 1.5106\n",
      "Training loss for epoch 118: pinn: 0.1020, boundary: 0.6592, total: 1.6797\n",
      "Training loss for epoch 119: pinn: 0.0992, boundary: 0.8088, total: 1.8012\n",
      "Training loss for epoch 120: pinn: 0.0981, boundary: 0.7926, total: 1.7734\n",
      "Training loss for epoch 121: pinn: 0.1050, boundary: 0.8809, total: 1.9304\n",
      "Training loss for epoch 122: pinn: 0.1226, boundary: 1.8440, total: 3.0700\n",
      "Training loss for epoch 123: pinn: 0.0999, boundary: 0.9452, total: 1.9438\n",
      "Training loss for epoch 124: pinn: 0.0916, boundary: 0.5009, total: 1.4169\n",
      "Training loss for epoch 125: pinn: 0.1052, boundary: 1.1451, total: 2.1968\n",
      "Training loss for epoch 126: pinn: 0.1089, boundary: 1.1841, total: 2.2734\n",
      "Training loss for epoch 127: pinn: 0.1264, boundary: 2.1930, total: 3.4575\n",
      "Training loss for epoch 128: pinn: 0.0925, boundary: 0.7798, total: 1.7046\n",
      "Training loss for epoch 129: pinn: 0.1073, boundary: 1.5085, total: 2.5815\n",
      "Training loss for epoch 130: pinn: 0.0943, boundary: 0.9337, total: 1.8765\n",
      "Training loss for epoch 131: pinn: 0.1040, boundary: 1.4117, total: 2.4518\n",
      "Training loss for epoch 132: pinn: 0.0897, boundary: 0.9370, total: 1.8345\n",
      "Training loss for epoch 133: pinn: 0.0885, boundary: 0.7008, total: 1.5863\n",
      "Training loss for epoch 134: pinn: 0.0916, boundary: 1.0173, total: 1.9333\n",
      "Training loss for epoch 135: pinn: 0.0882, boundary: 0.7309, total: 1.6128\n",
      "Training loss for epoch 136: pinn: 0.0889, boundary: 0.7740, total: 1.6634\n",
      "Training loss for epoch 137: pinn: 0.0903, boundary: 0.7761, total: 1.6791\n",
      "Training loss for epoch 138: pinn: 0.1049, boundary: 1.5128, total: 2.5614\n",
      "Training loss for epoch 139: pinn: 0.0854, boundary: 0.6192, total: 1.4735\n",
      "Training loss for epoch 140: pinn: 0.0924, boundary: 0.8184, total: 1.7422\n",
      "Training loss for epoch 141: pinn: 0.0994, boundary: 1.3313, total: 2.3256\n",
      "Training loss for epoch 142: pinn: 0.0887, boundary: 0.9550, total: 1.8421\n",
      "Training loss for epoch 143: pinn: 0.0924, boundary: 0.9641, total: 1.8879\n",
      "Training loss for epoch 144: pinn: 0.0910, boundary: 0.8398, total: 1.7503\n",
      "Training loss for epoch 145: pinn: 0.0913, boundary: 0.8933, total: 1.8066\n",
      "Training loss for epoch 146: pinn: 0.0932, boundary: 1.0015, total: 1.9331\n",
      "Training loss for epoch 147: pinn: 0.0877, boundary: 0.8759, total: 1.7528\n",
      "Training loss for epoch 148: pinn: 0.0905, boundary: 0.8741, total: 1.7790\n",
      "Training loss for epoch 149: pinn: 0.0861, boundary: 0.6302, total: 1.4908\n",
      "Training loss for epoch 150: pinn: 0.0893, boundary: 0.4287, total: 1.3215\n",
      "Training loss for epoch 151: pinn: 0.0925, boundary: 0.6848, total: 1.6099\n",
      "Training loss for epoch 152: pinn: 0.1032, boundary: 1.3900, total: 2.4222\n",
      "Training loss for epoch 153: pinn: 0.0905, boundary: 0.7853, total: 1.6906\n",
      "Training loss for epoch 154: pinn: 0.0895, boundary: 0.4182, total: 1.3127\n",
      "Training loss for epoch 155: pinn: 0.0947, boundary: 0.9148, total: 1.8615\n",
      "Training loss for epoch 156: pinn: 0.0908, boundary: 0.8418, total: 1.7494\n",
      "Training loss for epoch 157: pinn: 0.0889, boundary: 0.7497, total: 1.6389\n",
      "Training loss for epoch 158: pinn: 0.0978, boundary: 0.8683, total: 1.8461\n",
      "Training loss for epoch 159: pinn: 0.1136, boundary: 1.6588, total: 2.7949\n",
      "Training loss for epoch 160: pinn: 0.1020, boundary: 1.4880, total: 2.5081\n",
      "Training loss for epoch 161: pinn: 0.0852, boundary: 0.4709, total: 1.3229\n",
      "Training loss for epoch 162: pinn: 0.0873, boundary: 0.4904, total: 1.3635\n",
      "Training loss for epoch 163: pinn: 0.0915, boundary: 0.9648, total: 1.8794\n",
      "Training loss for epoch 164: pinn: 0.0850, boundary: 0.7352, total: 1.5848\n",
      "Training loss for epoch 165: pinn: 0.0847, boundary: 0.6325, total: 1.4794\n",
      "Training loss for epoch 166: pinn: 0.0849, boundary: 0.7742, total: 1.6228\n",
      "Training loss for epoch 167: pinn: 0.0784, boundary: 0.5764, total: 1.3603\n",
      "Training loss for epoch 168: pinn: 0.0944, boundary: 1.1802, total: 2.1239\n",
      "Training loss for epoch 169: pinn: 0.0897, boundary: 1.0354, total: 1.9327\n",
      "Training loss for epoch 170: pinn: 0.0826, boundary: 0.8087, total: 1.6345\n",
      "Training loss for epoch 171: pinn: 0.0951, boundary: 1.3640, total: 2.3146\n",
      "Training loss for epoch 172: pinn: 0.0713, boundary: 0.5473, total: 1.2606\n",
      "Training loss for epoch 173: pinn: 0.0957, boundary: 1.3376, total: 2.2941\n",
      "Training loss for epoch 174: pinn: 0.0777, boundary: 0.9150, total: 1.6923\n",
      "Training loss for epoch 175: pinn: 0.0764, boundary: 0.9116, total: 1.6760\n",
      "Training loss for epoch 176: pinn: 0.0714, boundary: 0.8083, total: 1.5224\n",
      "Training loss for epoch 177: pinn: 0.0690, boundary: 0.5573, total: 1.2475\n",
      "Training loss for epoch 178: pinn: 0.0978, boundary: 1.5620, total: 2.5399\n",
      "Training loss for epoch 179: pinn: 0.1002, boundary: 1.7147, total: 2.7163\n",
      "Training loss for epoch 180: pinn: 0.0953, boundary: 1.7416, total: 2.6945\n",
      "Training loss for epoch 181: pinn: 0.0918, boundary: 1.6521, total: 2.5699\n",
      "Training loss for epoch 182: pinn: 0.0677, boundary: 0.8912, total: 1.5682\n",
      "Training loss for epoch 183: pinn: 0.0634, boundary: 0.6593, total: 1.2936\n",
      "Training loss for epoch 184: pinn: 0.0684, boundary: 0.8653, total: 1.5495\n",
      "Training loss for epoch 185: pinn: 0.0671, boundary: 0.8443, total: 1.5157\n",
      "Training loss for epoch 186: pinn: 0.0628, boundary: 0.7388, total: 1.3672\n",
      "Training loss for epoch 187: pinn: 0.0604, boundary: 0.4265, total: 1.0306\n",
      "Training loss for epoch 188: pinn: 0.0657, boundary: 0.4218, total: 1.0792\n",
      "Training loss for epoch 189: pinn: 0.0708, boundary: 0.5894, total: 1.2978\n",
      "Training loss for epoch 190: pinn: 0.0708, boundary: 0.5958, total: 1.3035\n",
      "Training loss for epoch 191: pinn: 0.0795, boundary: 1.0374, total: 1.8322\n",
      "Training loss for epoch 192: pinn: 0.0658, boundary: 0.5926, total: 1.2510\n",
      "Training loss for epoch 193: pinn: 0.0749, boundary: 0.8986, total: 1.6475\n",
      "Training loss for epoch 194: pinn: 0.0707, boundary: 0.8766, total: 1.5836\n",
      "Training loss for epoch 195: pinn: 0.0642, boundary: 0.5281, total: 1.1696\n",
      "Training loss for epoch 196: pinn: 0.0723, boundary: 0.8861, total: 1.6095\n",
      "Training loss for epoch 197: pinn: 0.0644, boundary: 0.5659, total: 1.2096\n",
      "Training loss for epoch 198: pinn: 0.0697, boundary: 0.7310, total: 1.4276\n",
      "Training loss for epoch 199: pinn: 0.0645, boundary: 0.4265, total: 1.0720\n",
      "Training loss for epoch 200: pinn: 0.0926, boundary: 1.5379, total: 2.4640\n",
      "Training loss for epoch 201: pinn: 0.0710, boundary: 0.9397, total: 1.6494\n",
      "Training loss for epoch 202: pinn: 0.0643, boundary: 0.7641, total: 1.4067\n",
      "Training loss for epoch 203: pinn: 0.0656, boundary: 0.7289, total: 1.3845\n",
      "Training loss for epoch 204: pinn: 0.0659, boundary: 0.8464, total: 1.5057\n",
      "Training loss for epoch 205: pinn: 0.0645, boundary: 0.8255, total: 1.4701\n",
      "Training loss for epoch 206: pinn: 0.0686, boundary: 0.8964, total: 1.5827\n",
      "Training loss for epoch 207: pinn: 0.0590, boundary: 0.4975, total: 1.0873\n",
      "Training loss for epoch 208: pinn: 0.0687, boundary: 0.8619, total: 1.5484\n",
      "Training loss for epoch 209: pinn: 0.0650, boundary: 0.8613, total: 1.5109\n",
      "Training loss for epoch 210: pinn: 0.0593, boundary: 0.7173, total: 1.3098\n",
      "Training loss for epoch 211: pinn: 0.0704, boundary: 1.0590, total: 1.7629\n",
      "Training loss for epoch 212: pinn: 0.0629, boundary: 0.7510, total: 1.3803\n",
      "Training loss for epoch 213: pinn: 0.0724, boundary: 1.0920, total: 1.8159\n",
      "Training loss for epoch 214: pinn: 0.0610, boundary: 0.6734, total: 1.2837\n",
      "Training loss for epoch 215: pinn: 0.0661, boundary: 0.8480, total: 1.5086\n",
      "Training loss for epoch 216: pinn: 0.0645, boundary: 0.7015, total: 1.3463\n",
      "Training loss for epoch 217: pinn: 0.0628, boundary: 0.5747, total: 1.2029\n",
      "Training loss for epoch 218: pinn: 0.0673, boundary: 0.8387, total: 1.5120\n",
      "Training loss for epoch 219: pinn: 0.0715, boundary: 1.0593, total: 1.7746\n",
      "Training loss for epoch 220: pinn: 0.0808, boundary: 1.3857, total: 2.1938\n",
      "Training loss for epoch 221: pinn: 0.0613, boundary: 0.7305, total: 1.3430\n",
      "Training loss for epoch 222: pinn: 0.0637, boundary: 0.7222, total: 1.3593\n",
      "Training loss for epoch 223: pinn: 0.0740, boundary: 1.0548, total: 1.7945\n",
      "Training loss for epoch 224: pinn: 0.0641, boundary: 0.8141, total: 1.4551\n",
      "Training loss for epoch 225: pinn: 0.0792, boundary: 1.3194, total: 2.1116\n",
      "Training loss for epoch 226: pinn: 0.0808, boundary: 1.3915, total: 2.1998\n",
      "Training loss for epoch 227: pinn: 0.0582, boundary: 0.5982, total: 1.1806\n",
      "Training loss for epoch 228: pinn: 0.0649, boundary: 0.7088, total: 1.3575\n",
      "Training loss for epoch 229: pinn: 0.0646, boundary: 0.8017, total: 1.4473\n",
      "Training loss for epoch 230: pinn: 0.0642, boundary: 0.8004, total: 1.4422\n",
      "Training loss for epoch 231: pinn: 0.0609, boundary: 0.7484, total: 1.3578\n",
      "Training loss for epoch 232: pinn: 0.0601, boundary: 0.5903, total: 1.1914\n",
      "Training loss for epoch 233: pinn: 0.0602, boundary: 0.5603, total: 1.1628\n",
      "Training loss for epoch 234: pinn: 0.0680, boundary: 0.8147, total: 1.4952\n",
      "Training loss for epoch 235: pinn: 0.0637, boundary: 0.7319, total: 1.3688\n",
      "Training loss for epoch 236: pinn: 0.0612, boundary: 0.5018, total: 1.1133\n",
      "Training loss for epoch 237: pinn: 0.0670, boundary: 0.6737, total: 1.3435\n",
      "Training loss for epoch 238: pinn: 0.0672, boundary: 0.7955, total: 1.4678\n",
      "Training loss for epoch 239: pinn: 0.0661, boundary: 0.7877, total: 1.4488\n",
      "Training loss for epoch 240: pinn: 0.0687, boundary: 0.9230, total: 1.6100\n",
      "Training loss for epoch 241: pinn: 0.0698, boundary: 0.9011, total: 1.5988\n",
      "Training loss for epoch 242: pinn: 0.0667, boundary: 0.7822, total: 1.4493\n",
      "Training loss for epoch 243: pinn: 0.0641, boundary: 0.7686, total: 1.4095\n",
      "Training loss for epoch 244: pinn: 0.0609, boundary: 0.6296, total: 1.2386\n",
      "Training loss for epoch 245: pinn: 0.0624, boundary: 0.5455, total: 1.1697\n",
      "Training loss for epoch 246: pinn: 0.0627, boundary: 0.4107, total: 1.0381\n",
      "Training loss for epoch 247: pinn: 0.0742, boundary: 0.8770, total: 1.6190\n",
      "Training loss for epoch 248: pinn: 0.0780, boundary: 1.0724, total: 1.8527\n",
      "Training loss for epoch 249: pinn: 0.0612, boundary: 0.4444, total: 1.0562\n",
      "Training loss for epoch 250: pinn: 0.0695, boundary: 0.6804, total: 1.3750\n",
      "Training loss for epoch 251: pinn: 0.0760, boundary: 0.9107, total: 1.6707\n",
      "Training loss for epoch 252: pinn: 0.0729, boundary: 0.8485, total: 1.5772\n",
      "Training loss for epoch 253: pinn: 0.0643, boundary: 0.4224, total: 1.0650\n",
      "Training loss for epoch 254: pinn: 0.0733, boundary: 0.8224, total: 1.5556\n",
      "Training loss for epoch 255: pinn: 0.0724, boundary: 0.8098, total: 1.5337\n",
      "Training loss for epoch 256: pinn: 0.0658, boundary: 0.4911, total: 1.1486\n",
      "Training loss for epoch 257: pinn: 0.0718, boundary: 0.8535, total: 1.5712\n",
      "Training loss for epoch 258: pinn: 0.0680, boundary: 0.7452, total: 1.4252\n",
      "Training loss for epoch 259: pinn: 0.0679, boundary: 0.8002, total: 1.4789\n",
      "Training loss for epoch 260: pinn: 0.0629, boundary: 0.7358, total: 1.3651\n",
      "Training loss for epoch 261: pinn: 0.0635, boundary: 0.6591, total: 1.2938\n",
      "Training loss for epoch 262: pinn: 0.0602, boundary: 0.3823, total: 0.9842\n",
      "Training loss for epoch 263: pinn: 0.0649, boundary: 0.5065, total: 1.1553\n",
      "Training loss for epoch 264: pinn: 0.0660, boundary: 0.6657, total: 1.3262\n",
      "Training loss for epoch 265: pinn: 0.0886, boundary: 1.6181, total: 2.5045\n",
      "Training loss for epoch 266: pinn: 0.0666, boundary: 0.9058, total: 1.5715\n",
      "Training loss for epoch 267: pinn: 0.0599, boundary: 0.5954, total: 1.1943\n",
      "Training loss for epoch 268: pinn: 0.0635, boundary: 0.7227, total: 1.3577\n",
      "Training loss for epoch 269: pinn: 0.0602, boundary: 0.6198, total: 1.2223\n",
      "Training loss for epoch 270: pinn: 0.0605, boundary: 0.5182, total: 1.1233\n",
      "Training loss for epoch 271: pinn: 0.0615, boundary: 0.8179, total: 1.4332\n",
      "Training loss for epoch 272: pinn: 0.0585, boundary: 0.7262, total: 1.3117\n",
      "Training loss for epoch 273: pinn: 0.0764, boundary: 1.1619, total: 1.9255\n",
      "Training loss for epoch 274: pinn: 0.0631, boundary: 0.8919, total: 1.5231\n",
      "Training loss for epoch 275: pinn: 0.0533, boundary: 0.4378, total: 0.9704\n",
      "Training loss for epoch 276: pinn: 0.0616, boundary: 0.8404, total: 1.4565\n",
      "Training loss for epoch 277: pinn: 0.0561, boundary: 0.7246, total: 1.2861\n",
      "Training loss for epoch 278: pinn: 0.0554, boundary: 0.5921, total: 1.1456\n",
      "Training loss for epoch 279: pinn: 0.0718, boundary: 1.1030, total: 1.8210\n",
      "Training loss for epoch 280: pinn: 0.0551, boundary: 0.5465, total: 1.0980\n",
      "Training loss for epoch 281: pinn: 0.0547, boundary: 0.6244, total: 1.1713\n",
      "Training loss for epoch 282: pinn: 0.0590, boundary: 0.6851, total: 1.2750\n",
      "Training loss for epoch 283: pinn: 0.0579, boundary: 0.6924, total: 1.2718\n",
      "Training loss for epoch 284: pinn: 0.0609, boundary: 0.7388, total: 1.3474\n",
      "Training loss for epoch 285: pinn: 0.0647, boundary: 0.9094, total: 1.5567\n",
      "Training loss for epoch 286: pinn: 0.0603, boundary: 0.8122, total: 1.4149\n",
      "Training loss for epoch 287: pinn: 0.0613, boundary: 0.8576, total: 1.4710\n",
      "Training loss for epoch 288: pinn: 0.0590, boundary: 0.7962, total: 1.3858\n",
      "Training loss for epoch 289: pinn: 0.0597, boundary: 0.8543, total: 1.4511\n",
      "Training loss for epoch 290: pinn: 0.0519, boundary: 0.5617, total: 1.0806\n",
      "Training loss for epoch 291: pinn: 0.0530, boundary: 0.3685, total: 0.8983\n",
      "Training loss for epoch 292: pinn: 0.0578, boundary: 0.4086, total: 0.9868\n",
      "Training loss for epoch 293: pinn: 0.0663, boundary: 0.8438, total: 1.5068\n",
      "Training loss for epoch 294: pinn: 0.0605, boundary: 0.7926, total: 1.3980\n",
      "Training loss for epoch 295: pinn: 0.0579, boundary: 0.6773, total: 1.2567\n",
      "Training loss for epoch 296: pinn: 0.0621, boundary: 0.8124, total: 1.4337\n",
      "Training loss for epoch 297: pinn: 0.0584, boundary: 0.7560, total: 1.3396\n",
      "Training loss for epoch 298: pinn: 0.0617, boundary: 0.8052, total: 1.4224\n",
      "Training loss for epoch 299: pinn: 0.0658, boundary: 0.9544, total: 1.6122\n",
      "Training loss for epoch 300: pinn: 0.0752, boundary: 1.2582, total: 2.0105\n",
      "Training loss for epoch 301: pinn: 0.0562, boundary: 0.3787, total: 0.9404\n",
      "Training loss for epoch 302: pinn: 0.0661, boundary: 0.7848, total: 1.4459\n",
      "Training loss for epoch 303: pinn: 0.0636, boundary: 0.8078, total: 1.4439\n",
      "Training loss for epoch 304: pinn: 0.0597, boundary: 0.7747, total: 1.3715\n",
      "Training loss for epoch 305: pinn: 0.0572, boundary: 0.6015, total: 1.1734\n",
      "Training loss for epoch 306: pinn: 0.0604, boundary: 0.6850, total: 1.2886\n",
      "Training loss for epoch 307: pinn: 0.0576, boundary: 0.5626, total: 1.1387\n",
      "Training loss for epoch 308: pinn: 0.0658, boundary: 0.7411, total: 1.3991\n",
      "Training loss for epoch 309: pinn: 0.0626, boundary: 0.7345, total: 1.3606\n",
      "Training loss for epoch 310: pinn: 0.0619, boundary: 0.5757, total: 1.1947\n",
      "Training loss for epoch 311: pinn: 0.0622, boundary: 0.6262, total: 1.2479\n",
      "Training loss for epoch 312: pinn: 0.0598, boundary: 0.4792, total: 1.0768\n",
      "Training loss for epoch 313: pinn: 0.0608, boundary: 0.4255, total: 1.0339\n",
      "Training loss for epoch 314: pinn: 0.0596, boundary: 0.3621, total: 0.9582\n",
      "Training loss for epoch 315: pinn: 0.0650, boundary: 0.6603, total: 1.3105\n",
      "Training loss for epoch 316: pinn: 0.0661, boundary: 0.8345, total: 1.4957\n",
      "Training loss for epoch 317: pinn: 0.0651, boundary: 0.7777, total: 1.4290\n",
      "Training loss for epoch 318: pinn: 0.0581, boundary: 0.3492, total: 0.9304\n",
      "Training loss for epoch 319: pinn: 0.0695, boundary: 0.7998, total: 1.4945\n",
      "Training loss for epoch 320: pinn: 0.0719, boundary: 0.9889, total: 1.7075\n",
      "Training loss for epoch 321: pinn: 0.0695, boundary: 0.8069, total: 1.5021\n",
      "Training loss for epoch 322: pinn: 0.0688, boundary: 0.9098, total: 1.5979\n",
      "Training loss for epoch 323: pinn: 0.0664, boundary: 0.7801, total: 1.4444\n",
      "Training loss for epoch 324: pinn: 0.0679, boundary: 0.8227, total: 1.5013\n",
      "Training loss for epoch 325: pinn: 0.0598, boundary: 0.6023, total: 1.2005\n",
      "Training loss for epoch 326: pinn: 0.0635, boundary: 0.5329, total: 1.1679\n",
      "Training loss for epoch 327: pinn: 0.0712, boundary: 0.8274, total: 1.5395\n",
      "Training loss for epoch 328: pinn: 0.0664, boundary: 0.8062, total: 1.4705\n",
      "Training loss for epoch 329: pinn: 0.0601, boundary: 0.5045, total: 1.1055\n",
      "Training loss for epoch 330: pinn: 0.0779, boundary: 0.9700, total: 1.7487\n",
      "Training loss for epoch 331: pinn: 0.0725, boundary: 0.8457, total: 1.5711\n",
      "Training loss for epoch 332: pinn: 0.0634, boundary: 0.4925, total: 1.1261\n",
      "Training loss for epoch 333: pinn: 0.0707, boundary: 0.8111, total: 1.5182\n",
      "Training loss for epoch 334: pinn: 0.0611, boundary: 0.4517, total: 1.0624\n",
      "Training loss for epoch 335: pinn: 0.0678, boundary: 0.5634, total: 1.2418\n",
      "Training loss for epoch 336: pinn: 0.0789, boundary: 0.9046, total: 1.6932\n",
      "Training loss for epoch 337: pinn: 0.0730, boundary: 0.8382, total: 1.5681\n",
      "Training loss for epoch 338: pinn: 0.0700, boundary: 0.7602, total: 1.4598\n",
      "Training loss for epoch 339: pinn: 0.0650, boundary: 0.5074, total: 1.1572\n",
      "Training loss for epoch 340: pinn: 0.0837, boundary: 1.0477, total: 1.8851\n",
      "Training loss for epoch 341: pinn: 0.0677, boundary: 0.5377, total: 1.2151\n",
      "Training loss for epoch 342: pinn: 0.0871, boundary: 1.0845, total: 1.9557\n",
      "Training loss for epoch 343: pinn: 0.0937, boundary: 1.4624, total: 2.3997\n",
      "Training loss for epoch 344: pinn: 0.0774, boundary: 0.9109, total: 1.6846\n",
      "Training loss for epoch 345: pinn: 0.0831, boundary: 0.9628, total: 1.7936\n",
      "Training loss for epoch 346: pinn: 0.0795, boundary: 0.8970, total: 1.6916\n",
      "Training loss for epoch 347: pinn: 0.0836, boundary: 1.0499, total: 1.8859\n",
      "Training loss for epoch 348: pinn: 0.0795, boundary: 0.9109, total: 1.7061\n",
      "Training loss for epoch 349: pinn: 0.0787, boundary: 0.6621, total: 1.4494\n",
      "Training loss for epoch 350: pinn: 0.0791, boundary: 0.7065, total: 1.4972\n",
      "Training loss for epoch 351: pinn: 0.0712, boundary: 0.3690, total: 1.0815\n",
      "Training loss for epoch 352: pinn: 0.0809, boundary: 0.5343, total: 1.3430\n",
      "Training loss for epoch 353: pinn: 0.0802, boundary: 0.5563, total: 1.3584\n",
      "Training loss for epoch 354: pinn: 0.0872, boundary: 0.7815, total: 1.6539\n",
      "Training loss for epoch 355: pinn: 0.0861, boundary: 0.8688, total: 1.7299\n",
      "Training loss for epoch 356: pinn: 0.0919, boundary: 1.0565, total: 1.9755\n",
      "Training loss for epoch 357: pinn: 0.0836, boundary: 0.5452, total: 1.3815\n",
      "Training loss for epoch 358: pinn: 0.0908, boundary: 0.8723, total: 1.7799\n",
      "Training loss for epoch 359: pinn: 0.0984, boundary: 1.1808, total: 2.1646\n",
      "Training loss for epoch 360: pinn: 0.1066, boundary: 1.5038, total: 2.5698\n",
      "Training loss for epoch 361: pinn: 0.0907, boundary: 1.0862, total: 1.9931\n",
      "Training loss for epoch 362: pinn: 0.0876, boundary: 0.7048, total: 1.5809\n",
      "Training loss for epoch 363: pinn: 0.0846, boundary: 0.4850, total: 1.3313\n",
      "Training loss for epoch 364: pinn: 0.0924, boundary: 0.5112, total: 1.4348\n",
      "Training loss for epoch 365: pinn: 0.0942, boundary: 0.6077, total: 1.5496\n",
      "Training loss for epoch 366: pinn: 0.0966, boundary: 0.6691, total: 1.6347\n",
      "Training loss for epoch 367: pinn: 0.0956, boundary: 0.5416, total: 1.4973\n",
      "Training loss for epoch 368: pinn: 0.0994, boundary: 0.7238, total: 1.7182\n",
      "Training loss for epoch 369: pinn: 0.0962, boundary: 0.7437, total: 1.7055\n",
      "Training loss for epoch 370: pinn: 0.0962, boundary: 0.7152, total: 1.6769\n",
      "Training loss for epoch 371: pinn: 0.0956, boundary: 0.7744, total: 1.7304\n",
      "Training loss for epoch 372: pinn: 0.0964, boundary: 0.7478, total: 1.7122\n",
      "Training loss for epoch 373: pinn: 0.0956, boundary: 0.3378, total: 1.2934\n",
      "Training loss for epoch 374: pinn: 0.1039, boundary: 0.7672, total: 1.8057\n",
      "Training loss for epoch 375: pinn: 0.0989, boundary: 0.5811, total: 1.5702\n",
      "Training loss for epoch 376: pinn: 0.1005, boundary: 0.6601, total: 1.6650\n",
      "Training loss for epoch 377: pinn: 0.0988, boundary: 0.5447, total: 1.5325\n",
      "Training loss for epoch 378: pinn: 0.1025, boundary: 0.6035, total: 1.6288\n",
      "Training loss for epoch 379: pinn: 0.1008, boundary: 0.3950, total: 1.4033\n",
      "Training loss for epoch 380: pinn: 0.1070, boundary: 0.5641, total: 1.6345\n",
      "Training loss for epoch 381: pinn: 0.1106, boundary: 0.7910, total: 1.8967\n",
      "Training loss for epoch 382: pinn: 0.1064, boundary: 0.7238, total: 1.7875\n",
      "Training loss for epoch 383: pinn: 0.1053, boundary: 0.6786, total: 1.7321\n"
     ]
    }
   ],
   "source": [
    "# Define neural network. Note: 2 inputs- (p, r), 1 output- f(r, p)\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(1000, activation='relu')(inputs)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "# Define hyperparameters\n",
    "alpha = 10 # pinn_loss weight\n",
    "beta = 1 # boundary_loss weight\n",
    "lr = 3e-4\n",
    "# lr_decay = 0.9\n",
    "batchsize = 512\n",
    "boundary_batchsize = 256\n",
    "epochs = 500\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "save = True\n",
    "load_epoch = -1\n",
    "threshold = 0.1\n",
    "    \n",
    "# Initialize and compile and fit the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "            f_boundary=f_boundary[:, 0], size=size)\n",
    "pinn.compile(optimizer=optimizer)\n",
    "total_loss, pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, beta=beta, batchsize=batchsize, \n",
    "                                                             boundary_batchsize=boundary_batchsize, epochs=epochs, size=size, \n",
    "                                                             save=save, load_epoch=load_epoch, threshold=threshold)#, lr_decay=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the outputs to a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open('./figures/pinn_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/boundary_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/predictions.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "with open('./figures/f_boundary.pkl', 'wb') as file:\n",
    "    pkl.dump(f_boundary, file)\n",
    "    \n",
    "with open('./figures/p.pkl', 'wb') as file:\n",
    "    pkl.dump(p, file)\n",
    "    \n",
    "with open('./figures/T.pkl', 'wb') as file:\n",
    "    pkl.dump(T, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network with Sherpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set-up sherpa parameters, algorithm, and study\n",
    "# parameters = [sherpa.Ordinal(name='beta', range=[5, 10, 15, 20, 30]),\n",
    "#               sherpa.Continuous(name='lr', range=[3e-5, 3e-1], scale='log'),\n",
    "#               sherpa.Ordinal(name='batchsize', range=[256, 512, 1032, 2048]),\n",
    "#               sherpa.Ordinal(name='boundary_batchsize', range=[64, 128, 256]),\n",
    "#               sherpa.Ordinal(name='num_hidden_units', range=[100, 500, 1000]),\n",
    "#               sherpa.Choice(name='activation', range=['relu', 'tanh'])]\n",
    "\n",
    "# algorithm = sherpa.algorithms.RandomSearch(max_num_trials=2)\n",
    "\n",
    "# study = sherpa.Study(parameters=parameters,\n",
    "#                  algorithm=algorithm,\n",
    "#                  lower_is_better=True)\n",
    "\n",
    "# num_iterations = 1\n",
    "\n",
    "# # For each trial in the study, fit the model on the parameters and add the observation to the study\n",
    "# for trial in study:\n",
    "#     # Define neural network\n",
    "#     inputs = tf.keras.Input((2))\n",
    "#     x_ = tf.keras.layers.Dense(trial.parameters, activation=trial.parameters[5])(inputs)\n",
    "#     x_ = tf.keras.layers.Dense(trial.parameters[4], activation=trial.parameters[5])(x_)\n",
    "#     outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "#     # Initialize PINN and compile\n",
    "#     pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "#             f_boundary=f_boundary[:, 0], size=size)\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=trial.parameters[1])\n",
    "#     pinn.compile(optimizer=optimizer)\n",
    "    \n",
    "#     # For each iteration, fit the PINN and add the observation to the study\n",
    "#     for iteration in range(num_iterations):\n",
    "#         total_loss, pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, beta=trial.parameters[0], batchsize=trial.parameters[2], \n",
    "#                                                              boundary_batchsize=trial.parameters[3], epochs=100, size=size)\n",
    "#         study.add_observation(trial=trial,\n",
    "#                               iteration=iteration,\n",
    "#                               objective=boundary_loss,\n",
    "#                               context={'boundary_loss': boundary_loss})\n",
    "#     study.finalize(trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
