{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PINN Implementation\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook implements Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs. The implementation is not the same as Raissi as the original PINN was implemented in Tensorflow v1. Here, we use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 00:16:52.151804: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-19 00:16:52.151841: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-08-19 00:16:54.924964: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-08-19 00:16:54.925023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gpu-0008\n",
      "2022-08-19 00:16:54.925036: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gpu-0008\n",
      "2022-08-19 00:16:54.925160: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.43.4\n",
      "2022-08-19 00:16:54.925210: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.43.4\n",
      "2022-08-19 00:16:54.925221: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.43.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The data was gathered from https://github.com/maziarraissi/PINNs/tree/master/main/Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 256 256\n",
      "201 201\n",
      "[[-5.        -4.9609375 -4.921875  ...  4.8828125  4.921875   4.9609375]\n",
      " [-5.        -4.9609375 -4.921875  ...  4.8828125  4.921875   4.9609375]\n",
      " [-5.        -4.9609375 -4.921875  ...  4.8828125  4.921875   4.9609375]\n",
      " ...\n",
      " [-5.        -4.9609375 -4.921875  ...  4.8828125  4.921875   4.9609375]\n",
      " [-5.        -4.9609375 -4.921875  ...  4.8828125  4.921875   4.9609375]\n",
      " [-5.        -4.9609375 -4.921875  ...  4.8828125  4.921875   4.9609375]] [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00785398 0.00785398 0.00785398 ... 0.00785398 0.00785398 0.00785398]\n",
      " [0.01570796 0.01570796 0.01570796 ... 0.01570796 0.01570796 0.01570796]\n",
      " ...\n",
      " [1.55508836 1.55508836 1.55508836 ... 1.55508836 1.55508836 1.55508836]\n",
      " [1.56294235 1.56294235 1.56294235 ... 1.56294235 1.56294235 1.56294235]\n",
      " [1.57079633 1.57079633 1.57079633 ... 1.57079633 1.57079633 1.57079633]]\n"
     ]
    }
   ],
   "source": [
    "noise = 0.0        \n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([-5.0, 0.0]) # (x, t)\n",
    "ub = np.array([5.0, np.pi/2]) # (x, t)\n",
    "\n",
    "# Assign number of data points\n",
    "N0 = 50 # number of initial data points\n",
    "N_b = 50 # number of boundary data points\n",
    "N_f = 20000 # number of collocation data points\n",
    "layers = [2, 100, 100, 100, 100, 2]\n",
    "\n",
    "# Load data from NLS.mat\n",
    "# Note: this is the data used to compare the neural network to for losses.\n",
    "data = scipy.io.loadmat('./NLS.mat')\n",
    "\n",
    "t = data['tt'].flatten()[:,None] # time data\n",
    "x = data['x'].flatten()[:,None] # position data\n",
    "Exact = data['uu'] \n",
    "print(len(t), len(x), len(Exact))\n",
    "# print(t, x)\n",
    "\n",
    "Exact_u = np.real(Exact) # real\n",
    "Exact_v = np.imag(Exact) # imaginary\n",
    "# Exact_h = np.sqrt(Exact_u**2 + Exact_v**2) # Linnea: Commented this out because it is not used. Copied over from Github original\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "print(len(X), len(T))\n",
    "print(X, T)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "# Linnea: Commented this all out because it is not used. Copied over from Github original\n",
    "# u_star = Exact_u.T.flatten()[:,None]\n",
    "# v_star = Exact_v.T.flatten()[:,None]\n",
    "# h_star = Exact_h.T.flatten()[:,None]\n",
    "\n",
    "###########################\n",
    "# Linnea: Commented this all out because it is not used. Copied over from Github original\n",
    "# # Get initial condition data\n",
    "# idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "# x0 = x[idx_x,:]\n",
    "# u0 = Exact_u[idx_x,0:1]\n",
    "# v0 = Exact_v[idx_x,0:1]\n",
    "\n",
    "# # Get boundary data\n",
    "# idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "# tb = t[idx_t,:]\n",
    "\n",
    "# # Get PINN data\n",
    "# X_f = lb + (ub-lb)*lhs(2, N_f)\n",
    "\n",
    "# # model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions.\n",
    "\n",
    "Michael advises working through the TF2 API introduction to Gradients and Autodifferentiation in tensorflow https://www.tensorflow.org/guide/autodiff and Advanced Autodifferentiation in tensorflow https://www.tensorflow.org/guide/advanced_autodiff. These are the main data structures for the PINN used in the train_step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves Schrodingers equations. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, x, t, initial_u, initial_v, n_samples=20000, n_initial=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.initial_u = initial_u\n",
    "        self.initial_v = initial_v\n",
    "        self.n_samples = n_samples\n",
    "        self.n_initial = n_initial\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 3 types of equations: the main partial differential equations, the \n",
    "    initial value equations, and the boundary value equations. These three equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy ALL 3 equations, the system is solved. Since there are 3 types of \n",
    "    equations (PDE, Initial Value, Boundary Value), we will need 3 types of inputs. Each input is composed of a spatial \n",
    "    variable 'x' and a time variable 't'. The different types of (x, t) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        x, t: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (x, t) through the PINN for the forward propagation. This expression is PINN(x, t) = (u, v). \n",
    "        Next, the partials u_x, v_x, u_t, and v_t are obtained. We also need u_xx and v_xx. We utilize TF2s GradientTape\n",
    "        data structure to obtain all partials. Once we obtain these partials, we can compute the main PDE loss \n",
    "        and optimize weights wrt. to the loss. \n",
    "        \n",
    "        x_initial, t_initial : (initial_batchsize, 1) shaped arrays : These inputs are used to derive the initial value\n",
    "        equations. The initial value loss relies on target data (**not an equation**), so we can just measure the MSE of \n",
    "        PINN(x_initial, t_initial) = (u_pred_initial, v_pred_initial) and (u_initial, v_initial).\n",
    "        \n",
    "        u_initial, v_initial: (initial_batchsize, 1) shaped arrays : These are the target data for the initial value inputs\n",
    "        \n",
    "        x_lower, x_upper, t_boundary: (boundary_batchsize, 1) shaped arrays: These are simply (batchsize, 1) arrays where \n",
    "        x values are spatial boundaries and t values span the entire temporal domain. The boundary value equation specifies \n",
    "        that the boundaries are symmetric. That is, PINN(x_lowerbound, t) = PINN(x_upperbound, t) AND \n",
    "        PINN_x(x_lowerbound, t) = PINN_x(x_upperbound, t).\n",
    "        \n",
    "    Outputs: None\n",
    "    '''\n",
    "    def train_step(self, x, t, x_initial, t_initial, u_initial, v_initial, x_lower, x_upper, t_boundary):\n",
    "        with tf.GradientTape(persistent=True) as t3: \n",
    "            with tf.GradientTape(persistent=True) as t2: \n",
    "                with tf.GradientTape(persistent=True) as t1: \n",
    "                    # Forward pass X (PINN data)\n",
    "                    X = tf.concat((x, t), axis=1)\n",
    "                    pred = self.tf_call(X)\n",
    "                    u, v = tf.split(pred, 2, axis=1)\n",
    "                    \n",
    "                    # Forward pass X_initial (initial condition data)\n",
    "                    X_initial = tf.concat((x_initial, t_initial), axis=1)\n",
    "                    u_pred_initial, v_pred_initial = tf.split(self.tf_call(X_initial), 2, axis=1)\n",
    "                    \n",
    "                    # Calculate initial condition loss\n",
    "                    u_initial_loss = tf.math.reduce_mean(tf.math.square(u_pred_initial - u_initial))\n",
    "                    v_initial_loss = tf.math.reduce_mean(tf.math.square(v_pred_initial - v_initial))\n",
    "                    initial_loss = u_initial_loss + v_initial_loss\n",
    "                    \n",
    "                    # Forward pass X_lower and X_upper (boundary data)\n",
    "                    X_lower = tf.concat((x_lower, t_boundary), axis=1)\n",
    "                    X_upper = tf.concat((x_upper, t_boundary), axis=1)\n",
    "                    u_lower, v_lower = tf.split(self.tf_call(X_lower), 2, axis=1) \n",
    "                    u_upper, v_upper = tf.split(self.tf_call(X_upper), 2, axis=1)\n",
    "                    \n",
    "                    # Calculate Boundary loss\n",
    "                    boundary_u_loss = tf.math.reduce_mean(tf.math.square(u_lower - u_upper))\n",
    "                    boundary_v_loss = tf.math.reduce_mean(tf.math.square(v_lower - v_upper))\n",
    "                    boundary_loss = boundary_u_loss + boundary_v_loss\n",
    "                \n",
    "                # Calculate first-order PINN gradients\n",
    "                u_x = t1.gradient(u, x)\n",
    "                v_x = t1.gradient(v, x)\n",
    "                u_t = t1.gradient(u, t)\n",
    "                v_t = t1.gradient(v, t)\n",
    "                \n",
    "                # Calculate first-order boundary gradients\n",
    "                u_x_lower = t1.gradient(u_lower, x_lower)\n",
    "                u_x_upper = t1.gradient(u_upper, x_upper)\n",
    "                v_x_lower = t1.gradient(v_lower, x_lower)\n",
    "                v_x_upper = t1.gradient(v_upper, x_upper)\n",
    "\n",
    "                # Calculate resulting boundary loss\n",
    "                boundary_ux_loss = tf.math.reduce_mean(tf.math.square(u_x_lower - u_x_upper))\n",
    "                boundary_vx_loss = tf.math.reduce_mean(tf.math.square(v_x_lower - v_x_upper))\n",
    "                boundary_x_loss = boundary_ux_loss + boundary_vx_loss\n",
    "            \n",
    "            # Calculate second-order PINN gradients\n",
    "            u_xx = t2.gradient(u_x, x)\n",
    "            v_xx = t2.gradient(v_x, x)\n",
    "            \n",
    "            # Calculate resulting loss = PINN loss + boundary loss + initial loss (see Raissi et al. equations)\n",
    "            pinn_loss = self.pinn_loss(u, u_t, u_xx, v, v_t, v_xx)\n",
    "            total_loss = pinn_loss + (boundary_x_loss + boundary_loss) + initial_loss\n",
    "        \n",
    "        # Backpropagate overall gradients of the model loss to all variables\n",
    "        gradients = t3.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Return losses\n",
    "        return total_loss.numpy(), pinn_loss.numpy(), initial_loss.numpy(), (boundary_loss + boundary_x_loss).numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        predict_X: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        batchsize: batchsize for (x, t) in train step\n",
    "        \n",
    "        initial_batchsize: batchsize for (x_initial, t_initial) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "    \n",
    "    Outputs: Losses for each equation (PDE, Initial Value, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, predict_X, batchsize=64, initial_batchsize=16, boundary_batchsize=16, epochs=20):\n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        total_initial_loss = np.zeros((epochs, ))\n",
    "        total_predictions = np.zeros((51456, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN, initial, and boundary areas and run them through the train step\n",
    "        for epoch in range(epochs):\n",
    "            # Reset loss variables\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            initial_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN variables, boundary variables, and initial condition variables, and run variables through the train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN x and t variables via uniform distribution sampling between lower and upper bounds\n",
    "                # Note: original uses latin hypercube sampling\n",
    "                x = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                t = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "#                 x, t = tf.split(np.array(self.lower_bound + (self.upper_bound - self.lower_bound)*lhs(2, batchsize), dtype='f'), 2, axis=1)\n",
    "#                 x = tf.Variable(x, dtype=tf.float32)\n",
    "#                 t = tf.Variable(t, dtype=tf.float32)\n",
    "                \n",
    "                # Get boundary x_lower, x_upper, and t variables by uniformly sampling data along the lower and upper boundaries\n",
    "                lower_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                lower_bound[:] = self.lower_bound[0]\n",
    "                upper_bound[:] = self.upper_bound[0]\n",
    "                x_lower = tf.Variable(lower_bound, dtype=tf.float32)\n",
    "                x_upper = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                t_boundary = tf.Variable(tf.random.uniform((boundary_batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Get initial x_initial, t_initial, u_initial, and v_initial variables by randomly sampling along t=0\n",
    "                x_idx = np.expand_dims(np.random.choice(self.initial_u.shape[0], initial_batchsize, replace=False), axis=1)\n",
    "                x_initial = self.x[x_idx]\n",
    "                t_initial = np.zeros((initial_batchsize, 1))\n",
    "                u_initial = self.initial_u[x_idx]\n",
    "                v_initial = self.initial_v[x_idx]\n",
    "                \n",
    "                # Pass variables through the model via train_step, and get losses\n",
    "                total_loss = self.train_step(x, t, x_initial, t_initial, u_initial, v_initial, x_lower, x_upper, t_boundary)\n",
    "                pinn_loss[step] = total_loss[1]\n",
    "                initial_loss[step] = total_loss[2]\n",
    "                boundary_loss[step] = total_loss[3]\n",
    "            \n",
    "            # Calculate and print total losses for the epoch\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            total_initial_loss[epoch] = np.sum(initial_loss)\n",
    "            print(f'Training loss for epoch {epoch}: pinn: {total_pinn_loss[epoch]:.4f}, boundary: {total_boundary_loss[epoch]:.4f}, initial: {total_initial_loss[epoch]:.4f}')\n",
    "            \n",
    "            # Get prediction variable loss by the predict function (below)\n",
    "            total_predictions[:, :, epoch] = np.expand_dims(self.predict(predict_X)[1], axis=1)\n",
    "        \n",
    "        # Return epoch losses\n",
    "        return total_pinn_loss, total_boundary_loss, total_initial_loss, total_predictions\n",
    "    \n",
    "    # Predict for some X's the value of the neural network h(x, t)\n",
    "    def predict(self, X, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(X.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((X.shape[0], 2))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = X.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Get prediction data and calculate h\n",
    "            preds[start_idx: end_idx, :] = self(X[start_idx: end_idx, :]).numpy()\n",
    "            h = np.sqrt(preds[:, 0]**2 + preds[:, 1]**2)\n",
    "        \n",
    "        # Return prediction data and h\n",
    "        return preds, h\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by using the PINN function in Raissi et al. and separating real and imag data\n",
    "    @tf.function\n",
    "    def pinn_loss(self, u, u_t, u_xx, v, v_t, v_xx): \n",
    "        l_u = tf.math.reduce_mean(tf.math.square(u_t + 0.5 * v_xx + (u**2 + v**2) * v))\n",
    "        l_v = tf.math.reduce_mean(tf.math.square(v_t - 0.5 * u_xx - (u**2 + v**2) * u))\n",
    "        \n",
    "        return l_u + l_v\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 0: pinn: 0.1313, boundary: 6.8307, initial: 7.1221\n",
      "Training loss for epoch 1: pinn: 0.1334, boundary: 1.5248, initial: 3.5351\n",
      "Training loss for epoch 2: pinn: 0.1184, boundary: 0.7179, initial: 2.9285\n",
      "Training loss for epoch 3: pinn: 0.0902, boundary: 0.2265, initial: 2.6711\n",
      "Training loss for epoch 4: pinn: 0.1032, boundary: 0.1377, initial: 2.2808\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compile the PINN and get loss and prediction outputs\u001b[39;00m\n\u001b[1;32m     21\u001b[0m pinn\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m pinn_loss, boundary_loss, initial_loss, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43minitial_batchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_batchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mPINN.fit\u001b[0;34m(self, predict_X, batchsize, initial_batchsize, boundary_batchsize, epochs)\u001b[0m\n\u001b[1;32m    163\u001b[0m v_initial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_v[x_idx]\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Pass variables through the model via train_step, and get losses\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_lower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_upper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_boundary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m pinn_loss[step] \u001b[38;5;241m=\u001b[39m total_loss[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    168\u001b[0m initial_loss[step] \u001b[38;5;241m=\u001b[39m total_loss[\u001b[38;5;241m2\u001b[39m]\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mPINN.train_step\u001b[0;34m(self, x, t, x_initial, t_initial, u_initial, v_initial, x_lower, x_upper, t_boundary)\u001b[0m\n\u001b[1;32m     77\u001b[0m v_x \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m.\u001b[39mgradient(v, x)\n\u001b[1;32m     78\u001b[0m u_t \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m.\u001b[39mgradient(u, t)\n\u001b[0;32m---> 79\u001b[0m v_t \u001b[38;5;241m=\u001b[39m \u001b[43mt1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Calculate first-order boundary gradients\u001b[39;00m\n\u001b[1;32m     82\u001b[0m u_x_lower \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m.\u001b[39mgradient(u_lower, x_lower)\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1100\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1094\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1095\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1096\u001b[0m           output_gradients))\n\u001b[1;32m   1097\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1098\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1100\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1109\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1205\u001b[0m, in \u001b[0;36m_TapeGradientFunctions._wrap_backward_function.<locals>._backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1203\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m backward_function_inputs:\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremapped_captures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1868\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executing_eagerly:\n\u001b[0;32m-> 1868\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mforward_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_tangents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1871\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_override_gradient_function(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1872\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartitionedCall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_function(),\n\u001b[1;32m   1873\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatefulPartitionedCall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_function()}):\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define neural network as 6 layers (4 hidden), with activation functions of tanh for hidden layers and linear for the output layer\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(inputs)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "outputs = tf.keras.layers.Dense(2, activation='linear')(x_)\n",
    "\n",
    "# Linnea: commented out because it is duplicated in second cell above\n",
    "# # Initialize upper and lower bound data\n",
    "# lb = np.array([-5.0, 0.0])\n",
    "# ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "# Get keras Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "# Define the PINN using the model defined above\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, \n",
    "            x=x[:,0], t=t[:, 0], initial_u=Exact_u[:, 0], initial_v=Exact_v[:, 0])\n",
    "\n",
    "# Compile the PINN and get loss and prediction outputs\n",
    "pinn.compile(optimizer=\"adam\")\n",
    "pinn_loss, boundary_loss, initial_loss, predictions = pinn.fit(predict_X=X_star, batchsize=2048, \n",
    "                                               initial_batchsize=64, boundary_batchsize=32, epochs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Save loss data, prediction data, and h-function approximation\n",
    "with open('./figures/pinn_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/boundary_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/initial_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(initial_loss, file)\n",
    "    \n",
    "with open('./figures/predictions.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "with open('./figures/true.pkl', 'wb') as file:\n",
    "    pkl.dump(h_star, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
