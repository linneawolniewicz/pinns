{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PINN Implementation\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook implements Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs. The implementation is not the same as Raissi as the original PINN was implemented in Tensorflow v1. Here, we use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 11:59:31.089323: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-08-30 11:59:31.089393: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gpu-0008\n",
      "2022-08-30 11:59:31.089402: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gpu-0008\n",
      "2022-08-30 11:59:31.089531: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.43.4\n",
      "2022-08-30 11:59:31.089571: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.43.4\n",
      "2022-08-30 11:59:31.089577: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.43.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The data was gathered from https://github.com/maziarraissi/PINNs/tree/master/main/Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linnea: Commented out  because never used\n",
    "# noise = 0.0        \n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([-5.0, 0.0]) # (x, t)\n",
    "ub = np.array([5.0, np.pi/2]) # (x, t)\n",
    "\n",
    "# Assign number of data points\n",
    "# Linnea: Commented this all out because it is not used. Copied over from Github original\n",
    "# N0 = 50 # number of initial data points\n",
    "# N_b = 50 # number of boundary data points\n",
    "# N_f = 20000 # number of collocation data points\n",
    "# layers = [2, 100, 100, 100, 100, 2]\n",
    "\n",
    "# Load data from NLS.mat\n",
    "# Note: this is the data used to compare the neural network to for losses.\n",
    "data = scipy.io.loadmat('./NLS.mat')\n",
    "\n",
    "t = data['tt'].flatten()[:,None] # time data\n",
    "x = data['x'].flatten()[:,None] # position data\n",
    "Exact = data['uu'] \n",
    "\n",
    "Exact_u = np.real(Exact) # real\n",
    "Exact_v = np.imag(Exact) # imaginary\n",
    "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "# Linnea: Commented this all out because it is not used. Copied over from Github original\n",
    "# u_star = Exact_u.T.flatten()[:,None]\n",
    "# v_star = Exact_v.T.flatten()[:,None]\n",
    "h_star = Exact_h.T.flatten()[:,None]\n",
    "\n",
    "###########################\n",
    "# Linnea: Commented this all out because it is not used. Copied over from Github original\n",
    "# # Get initial condition data\n",
    "# idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "# x0 = x[idx_x,:]\n",
    "# u0 = Exact_u[idx_x,0:1]\n",
    "# v0 = Exact_v[idx_x,0:1]\n",
    "\n",
    "# # Get boundary data\n",
    "# idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "# tb = t[idx_t,:]\n",
    "\n",
    "# # Get PINN data\n",
    "# X_f = lb + (ub-lb)*lhs(2, N_f)\n",
    "\n",
    "# # model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub)\n",
    "\n",
    "# plt.imshow(Exact_h)\n",
    "# plt.plot(x, Exact_h[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions.\n",
    "\n",
    "Michael advises working through the TF2 API introduction to Gradients and Autodifferentiation in tensorflow https://www.tensorflow.org/guide/autodiff and Advanced Autodifferentiation in tensorflow https://www.tensorflow.org/guide/advanced_autodiff. These are the main data structures for the PINN used in the train_step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves Schrodingers equations. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, x, t, initial_u, initial_v, n_samples=20000, n_initial=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.initial_u = initial_u\n",
    "        self.initial_v = initial_v\n",
    "        self.n_samples = n_samples\n",
    "        self.n_initial = n_initial\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 3 types of equations: the main partial differential equations, the \n",
    "    initial value equations, and the boundary value equations. These three equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy ALL 3 equations, the system is solved. Since there are 3 types of \n",
    "    equations (PDE, Initial Value, Boundary Value), we will need 3 types of inputs. Each input is composed of a spatial \n",
    "    variable 'x' and a time variable 't'. The different types of (x, t) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        x, t: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (x, t) through the PINN for the forward propagation. This expression is PINN(x, t) = (u, v). \n",
    "        Next, the partials u_x, v_x, u_t, and v_t are obtained. We also need u_xx and v_xx. We utilize TF2s GradientTape\n",
    "        data structure to obtain all partials. Once we obtain these partials, we can compute the main PDE loss \n",
    "        and optimize weights wrt. to the loss. \n",
    "        \n",
    "        x_initial, t_initial : (initial_batchsize, 1) shaped arrays : These inputs are used to derive the initial value\n",
    "        equations. The initial value loss relies on target data (**not an equation**), so we can just measure the MSE of \n",
    "        PINN(x_initial, t_initial) = (u_pred_initial, v_pred_initial) and (u_initial, v_initial).\n",
    "        \n",
    "        u_initial, v_initial: (initial_batchsize, 1) shaped arrays : These are the target data for the initial value inputs\n",
    "        \n",
    "        x_lower, x_upper, t_boundary: (boundary_batchsize, 1) shaped arrays: These are simply (batchsize, 1) arrays where \n",
    "        x values are spatial boundaries and t values span the entire temporal domain. The boundary value equation specifies \n",
    "        that the boundaries are symmetric. That is, PINN(x_lowerbound, t) = PINN(x_upperbound, t) AND \n",
    "        PINN_x(x_lowerbound, t) = PINN_x(x_upperbound, t).\n",
    "        \n",
    "    Outputs: None\n",
    "    '''\n",
    "    def train_step(self, x, t, x_initial, t_initial, u_initial, v_initial, x_lower, x_upper, t_boundary):\n",
    "        with tf.GradientTape(persistent=True) as t3: \n",
    "            with tf.GradientTape(persistent=True) as t2: \n",
    "                with tf.GradientTape(persistent=True) as t1: \n",
    "                    # Forward pass X (PINN data)\n",
    "                    X = tf.concat((x, t), axis=1)\n",
    "                    pred = self.tf_call(X)\n",
    "                    u, v = tf.split(pred, 2, axis=1)\n",
    "                    \n",
    "                    # Forward pass X_initial (initial condition data)\n",
    "                    X_initial = tf.concat((x_initial, t_initial), axis=1)\n",
    "                    u_pred_initial, v_pred_initial = tf.split(self.tf_call(X_initial), 2, axis=1)\n",
    "                    \n",
    "                    # Calculate initial condition loss\n",
    "                    u_initial_loss = tf.math.reduce_mean(tf.math.square(u_pred_initial - u_initial))\n",
    "                    v_initial_loss = tf.math.reduce_mean(tf.math.square(v_pred_initial - v_initial))\n",
    "                    initial_loss = u_initial_loss + v_initial_loss\n",
    "                    \n",
    "                    # Forward pass X_lower and X_upper (boundary data)\n",
    "                    X_lower = tf.concat((x_lower, t_boundary), axis=1)\n",
    "                    X_upper = tf.concat((x_upper, t_boundary), axis=1)\n",
    "                    u_lower, v_lower = tf.split(self.tf_call(X_lower), 2, axis=1) \n",
    "                    u_upper, v_upper = tf.split(self.tf_call(X_upper), 2, axis=1)\n",
    "                    \n",
    "                    # Calculate Boundary loss\n",
    "                    boundary_u_loss = tf.math.reduce_mean(tf.math.square(u_lower - u_upper))\n",
    "                    boundary_v_loss = tf.math.reduce_mean(tf.math.square(v_lower - v_upper))\n",
    "                    boundary_loss = boundary_u_loss + boundary_v_loss\n",
    "                \n",
    "                # Calculate first-order PINN gradients\n",
    "                u_x = t1.gradient(u, x)\n",
    "                v_x = t1.gradient(v, x)\n",
    "                u_t = t1.gradient(u, t)\n",
    "                v_t = t1.gradient(v, t)\n",
    "                \n",
    "                # Calculate first-order boundary gradients\n",
    "                u_x_lower = t1.gradient(u_lower, x_lower)\n",
    "                u_x_upper = t1.gradient(u_upper, x_upper)\n",
    "                v_x_lower = t1.gradient(v_lower, x_lower)\n",
    "                v_x_upper = t1.gradient(v_upper, x_upper)\n",
    "\n",
    "                # Calculate resulting boundary loss\n",
    "                boundary_ux_loss = tf.math.reduce_mean(tf.math.square(u_x_lower - u_x_upper))\n",
    "                boundary_vx_loss = tf.math.reduce_mean(tf.math.square(v_x_lower - v_x_upper))\n",
    "                boundary_x_loss = boundary_ux_loss + boundary_vx_loss\n",
    "            \n",
    "            # Calculate second-order PINN gradients\n",
    "            u_xx = t2.gradient(u_x, x)\n",
    "            v_xx = t2.gradient(v_x, x)\n",
    "            \n",
    "            # Calculate resulting loss = PINN loss + boundary loss + initial loss (see Raissi et al. equations)\n",
    "            pinn_loss = self.pinn_loss(u, u_t, u_xx, v, v_t, v_xx)\n",
    "            total_loss = pinn_loss + (boundary_x_loss + boundary_loss) + initial_loss\n",
    "        \n",
    "        # Backpropagate overall gradients of the model loss to all variables\n",
    "        gradients = t3.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Return losses\n",
    "        return total_loss.numpy(), pinn_loss.numpy(), initial_loss.numpy(), (boundary_loss + boundary_x_loss).numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        predict_X: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        batchsize: batchsize for (x, t) in train step\n",
    "        \n",
    "        initial_batchsize: batchsize for (x_initial, t_initial) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "    \n",
    "    Outputs: Losses for each equation (PDE, Initial Value, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, predict_X, batchsize=64, initial_batchsize=16, boundary_batchsize=16, epochs=20):\n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        total_initial_loss = np.zeros((epochs, ))\n",
    "        total_predictions = np.zeros((51456, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN, initial, and boundary areas and run them through the train step\n",
    "        for epoch in range(epochs):\n",
    "            # Reset loss variables\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            initial_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN variables, boundary variables, and initial condition variables, and run variables through the train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN x and t variables via uniform distribution sampling between lower and upper bounds\n",
    "                # Note: original uses latin hypercube sampling\n",
    "                x = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                t = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "#                 x, t = tf.split(np.array(self.lower_bound + (self.upper_bound - self.lower_bound)*lhs(2, batchsize), dtype='f'), 2, axis=1)\n",
    "#                 x = tf.Variable(x, dtype=tf.float32)\n",
    "#                 t = tf.Variable(t, dtype=tf.float32)\n",
    "                \n",
    "                # Get boundary x_lower, x_upper, and t variables by uniformly sampling data along the lower and upper boundaries\n",
    "                lower_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                lower_bound[:] = self.lower_bound[0]\n",
    "                upper_bound[:] = self.upper_bound[0]\n",
    "                x_lower = tf.Variable(lower_bound, dtype=tf.float32)\n",
    "                x_upper = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                t_boundary = tf.Variable(tf.random.uniform((boundary_batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Get initial x_initial, t_initial, u_initial, and v_initial variables by randomly sampling along t=0\n",
    "                x_idx = np.expand_dims(np.random.choice(self.initial_u.shape[0], initial_batchsize, replace=False), axis=1)\n",
    "                x_initial = self.x[x_idx]\n",
    "                t_initial = np.zeros((initial_batchsize, 1))\n",
    "                u_initial = self.initial_u[x_idx]\n",
    "                v_initial = self.initial_v[x_idx]\n",
    "                \n",
    "                # Pass variables through the model via train_step, and get losses\n",
    "                total_loss = self.train_step(x, t, x_initial, t_initial, u_initial, v_initial, x_lower, x_upper, t_boundary)\n",
    "                pinn_loss[step] = total_loss[1]\n",
    "                initial_loss[step] = total_loss[2]\n",
    "                boundary_loss[step] = total_loss[3]\n",
    "            \n",
    "            # Calculate and print total losses for the epoch\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            total_initial_loss[epoch] = np.sum(initial_loss)\n",
    "            print(f'Training loss for epoch {epoch}: pinn: {total_pinn_loss[epoch]:.4f}, boundary: {total_boundary_loss[epoch]:.4f}, initial: {total_initial_loss[epoch]:.4f}')\n",
    "            \n",
    "            # Get prediction variable loss by the predict function (below)\n",
    "            total_predictions[:, :, epoch] = np.expand_dims(self.predict(predict_X)[1], axis=1)\n",
    "        \n",
    "        # Return epoch losses\n",
    "        return total_pinn_loss, total_boundary_loss, total_initial_loss, total_predictions\n",
    "    \n",
    "    # Predict for some X's the value of the neural network h(x, t), where h = [u, v]\n",
    "    def predict(self, X, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(X.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((X.shape[0], 2))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = X.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Get prediction data and calculate h\n",
    "            preds[start_idx: end_idx, :] = self(X[start_idx: end_idx, :]).numpy()\n",
    "        \n",
    "        h = np.sqrt(preds[:, 0]**2 + preds[:, 1]**2)\n",
    "        \n",
    "        # Return prediction data and h\n",
    "        return preds, h\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by using the PINN function in Raissi et al. and separating real and imag data\n",
    "    @tf.function\n",
    "    def pinn_loss(self, u, u_t, u_xx, v, v_t, v_xx): \n",
    "        l_u = tf.math.reduce_mean(tf.math.square(u_t + 0.5 * v_xx + (u**2 + v**2) * v))\n",
    "        l_v = tf.math.reduce_mean(tf.math.square(v_t - 0.5 * u_xx - (u**2 + v**2) * u))\n",
    "        \n",
    "        return l_u + l_v\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 0: pinn: 0.0499, boundary: 5.9369, initial: 7.4406\n",
      "Training loss for epoch 1: pinn: 0.0959, boundary: 1.2661, initial: 3.8575\n",
      "Training loss for epoch 2: pinn: 0.1252, boundary: 0.4844, initial: 2.9477\n"
     ]
    }
   ],
   "source": [
    "# Define neural network as 6 layers (4 hidden), with activation functions of tanh for hidden layers and linear for the output layer\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(inputs)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "outputs = tf.keras.layers.Dense(2, activation='linear')(x_) # Note: 2 outputs u & v where h=(u, v)\n",
    "\n",
    "# Linnea: commented out because it is duplicated above\n",
    "# # Initialize upper and lower bound data\n",
    "# lb = np.array([-5.0, 0.0])\n",
    "# ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "# Get keras Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "# Define the PINN using the model defined above\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, \n",
    "            x=x[:,0], t=t[:, 0], initial_u=Exact_u[:, 0], initial_v=Exact_v[:, 0])\n",
    "\n",
    "# Compile the PINN and get loss and prediction outputs\n",
    "pinn.compile(optimizer=\"adam\")\n",
    "pinn_loss, boundary_loss, initial_loss, predictions = pinn.fit(predict_X=X_star, batchsize=2048, \n",
    "                                               initial_batchsize=64, boundary_batchsize=32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Save loss data, prediction data, and h-function approximation\n",
    "with open('./figures/pinn_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/boundary_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/initial_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(initial_loss, file)\n",
    "    \n",
    "with open('./figures/predictions.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "with open('./figures/true.pkl', 'wb') as file:\n",
    "    pkl.dump(h_star, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
