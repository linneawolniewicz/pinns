{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Physics Informed Neural Networks\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook adapts the code implementation of Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs, and the code by Michael Ito, to solve the force-field equation for solar modulation of cosmic rays. Michael Ito's code adaption use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure. \n",
    "\n",
    "In this application, our PINN is $h(r, p) = \\frac{\\partial f}{\\partial r} + \\frac{RV}{3k} \\frac{\\partial f}{\\partial p}$ where $k=\\beta(p)k_1(r)k_2(r)$ and $\\beta = \\frac{p}{\\sqrt{p^2 + M^2}}$. We will approximate $f(r, p)$ using a neural network.\n",
    "\n",
    "We have no initial data, but our boundary data will be given by $f(r_{HP}, p) = \\frac{J(r_{HP}, T)}{p^2} = \\frac{(T+M)^\\gamma}{p^2}$, where $r_{HP} = 120$ AU (i.e. the radius of Heliopause), $M=0.938$ GeV, $\\gamma$ is between $-2$ and $-3$, and $T = \\sqrt{p^2 + M^2} - M$. Or, vice versa, $p = \\sqrt{T^2 + 2TM}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants  \n",
    "m = 0.938 # GeV/c^2\n",
    "gamma = -3 # Between -2 and -3\n",
    "size = 512 # size of r, T, p, and f_boundary\n",
    "au = 150e6 # 150e6 m/AU\n",
    "r_limits = [119, 120]\n",
    "T_limits = [0.001, 1000]\n",
    "\n",
    "# Create data\n",
    "T = np.logspace(np.log10(T_limits[0]), np.log10(T_limits[1]), size).flatten()[:, None] # GeV\n",
    "p = (np.sqrt((T+m)**2-m**2)).flatten()[:,None] # GeV/c\n",
    "r = np.logspace(np.log10(r_limits[0]*au), np.log10(r_limits[1]*au), size).flatten()[:, None] # km\n",
    "f_boundary = ((T + m)**gamma)/(p**2) # particles/(m^3 (GeV/c)^3)\n",
    "\n",
    "# Take the log of all input data\n",
    "r = np.log(r)\n",
    "T = np.log(T)\n",
    "p = np.log(p)\n",
    "f_boundary = np.log(f_boundary)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([p[0], r[0]]) # (p, r) in (GeV, AU)\n",
    "ub = np.array([p[-1], r[-1]]) # (p, r) in (GeV, AU)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "P, R = np.meshgrid(p, r)\n",
    "P_star = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))\n",
    "\n",
    "# Check inputs\n",
    "# print(f'r: {r.shape}, p: {p.shape}, T: {T.shape}, f_boundary: {f_boundary.shape}, P_star: {P_star.shape}, lb: {lb}, ub:{ub}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, r, f_boundary, size, n_samples=20000, n_boundary=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.r = r\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.n_boundary = n_boundary\n",
    "        self.size = size\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and boundary_f.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "        alpha = weight on pinn_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                # Forward pass P (PINN data)\n",
    "                P = tf.concat((p, r), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Forward pass P_boundary (boundary condition data)\n",
    "                P_boundary = tf.concat((p_boundary, r_boundary), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "\n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tf.math.reduce_mean(tf.math.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            f_p = t1.gradient(f, p)\n",
    "            f_r = t1.gradient(f, r)\n",
    "            \n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r)\n",
    "            total_loss = (1-alpha)*pinn_loss + alpha*boundary_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        alpha: weight on pinn_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_decay: If -1, learning rate will not be decayed. Otherwise, lr = lr_decay*lr if loss hasn't \n",
    "        decreased\n",
    "        \n",
    "        alpha_decay: If -1, alpha will not be changed. Otherwise, alpha = alpha_decay*alpha if loss \n",
    "        hasn't decreased\n",
    "        \n",
    "        patience: Number of epochs to check whether loss has decreased before updating lr or alpha\n",
    "        \n",
    "        filename: Name for the checkpoint file\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, alpha=1, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, size=256, \n",
    "            save=False, load_epoch=-1, lr_decay=-1, alpha_decay=-1, patience=3, filename=''):\n",
    "        \n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            name = './ckpts/pinn_' + filename + '_epoch_' + str(load_epoch)\n",
    "            self.load_weights(name)\n",
    "        \n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            # Reset loss variables\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN and boundary variables and pass them to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN p and r variables via uniform distribution sampling between lower and upper bounds\n",
    "                p = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                r = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = self.p[p_idx]\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound[:] = self.upper_bound[1]\n",
    "                r_boundary = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                \n",
    "                # Train and get loss\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Calculate total epoch loss\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Epoch {epoch}. Current alpha: {alpha:.4f}, lr: {lr:.4f}. Training losses: pinn: {total_pinn_loss[epoch]:.4f}, ' +\n",
    "                  f'boundary: {total_boundary_loss[epoch]:.4f}, weighted total: {((alpha*total_boundary_loss[epoch])+((1-alpha)*total_pinn_loss[epoch])):.4f}')\n",
    "            \n",
    "            predictions[:, :, epoch] = self.predict(P_predict, size)\n",
    "            \n",
    "            # Decay lr if loss has decreased since the last patience epoch\n",
    "            if (epoch > patience):\n",
    "                hasntDecreased = False\n",
    "                if (total_pinn_loss[epoch] + total_boundary_loss[epoch]) > (total_pinn_loss[epoch-patience] + total_boundary_loss[epoch-patience]):\n",
    "                    hasntDecreased = True\n",
    "                        \n",
    "                if (lr_decay != -1) & hasntDecreased:\n",
    "                    lr = lr_decay*lr\n",
    "\n",
    "            # Decrease alpha each epoch\n",
    "            if alpha_decay != -1:\n",
    "                alpha = alpha_decay*alpha\n",
    "\n",
    "            # If the epoch is a multiple of 10, save to a checkpoint\n",
    "            if (epoch%10 == 0) & (save == True):\n",
    "                name = './ckpts/pinn_' + filename + '_epoch_' + str(epoch)\n",
    "                self.save_weights(name, overwrite=True, save_format=None, options=None)\n",
    "        \n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, size, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(P.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((size**2, 1))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Pass prediction data through the model\n",
    "            preds[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r): # To-do: add loss pass-through!\n",
    "        # Note: p and r are taken out of logspace for the PINN calculation\n",
    "        p = tf.math.exp(p) # GeV/c\n",
    "        r = tf.math.exp(r) # km\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tf.math.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tf.math.divide(p, tf.math.sqrt(tf.math.square(p) + tf.math.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tf.math.reduce_mean(tf.math.abs(f_r + (tf.math.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 0: pinn: 10.6393, boundary: 355.4385,total: 366.0778. Current alpha, lr: 0.9000, 0.0300\n",
      "Training loss for epoch 1: pinn: 11.9094, boundary: 429.4271,total: 441.3365. Current alpha, lr: 0.8982, 0.0300\n",
      "Training loss for epoch 2: pinn: 4.2046, boundary: 149.0108,total: 153.2154. Current alpha, lr: 0.8964, 0.0300\n",
      "Training loss for epoch 3: pinn: 4.5946, boundary: 171.5979,total: 176.1925. Current alpha, lr: 0.8946, 0.0300\n",
      "Training loss for epoch 4: pinn: 7.1890, boundary: 170.2434,total: 177.4324. Current alpha, lr: 0.8928, 0.0300\n",
      "Training loss for epoch 5: pinn: 5.0842, boundary: 118.1328,total: 123.2171. Current alpha, lr: 0.8910, 0.0300\n",
      "Training loss for epoch 6: pinn: 7.7983, boundary: 95.4934,total: 103.2917. Current alpha, lr: 0.8893, 0.0300\n",
      "Training loss for epoch 7: pinn: 7.2198, boundary: 76.0743,total: 83.2941. Current alpha, lr: 0.8875, 0.0300\n",
      "Training loss for epoch 8: pinn: 5.7387, boundary: 120.7658,total: 126.5045. Current alpha, lr: 0.8857, 0.0300\n",
      "Training loss for epoch 9: pinn: 6.6609, boundary: 70.7255,total: 77.3863. Current alpha, lr: 0.8839, 0.0300\n",
      "Training loss for epoch 10: pinn: 4.7164, boundary: 108.9724,total: 113.6887. Current alpha, lr: 0.8822, 0.0300\n",
      "Training loss for epoch 11: pinn: 4.8793, boundary: 86.2432,total: 91.1225. Current alpha, lr: 0.8804, 0.0300\n",
      "Training loss for epoch 12: pinn: 6.4671, boundary: 57.4246,total: 63.8917. Current alpha, lr: 0.8786, 0.0300\n",
      "Training loss for epoch 13: pinn: 5.3527, boundary: 97.1872,total: 102.5398. Current alpha, lr: 0.8769, 0.0300\n",
      "Training loss for epoch 14: pinn: 4.5566, boundary: 69.0838,total: 73.6404. Current alpha, lr: 0.8751, 0.0300\n",
      "Training loss for epoch 15: pinn: 7.2451, boundary: 51.1368,total: 58.3819. Current alpha, lr: 0.8734, 0.0300\n",
      "Training loss for epoch 16: pinn: 5.8853, boundary: 84.7403,total: 90.6256. Current alpha, lr: 0.8716, 0.0300\n",
      "Training loss for epoch 17: pinn: 5.7893, boundary: 81.5119,total: 87.3012. Current alpha, lr: 0.8699, 0.0300\n",
      "Training loss for epoch 18: pinn: 6.7570, boundary: 74.1324,total: 80.8894. Current alpha, lr: 0.8681, 0.0270\n",
      "Training loss for epoch 19: pinn: 7.2471, boundary: 88.6914,total: 95.9385. Current alpha, lr: 0.8664, 0.0270\n",
      "Training loss for epoch 20: pinn: 6.9039, boundary: 101.1080,total: 108.0119. Current alpha, lr: 0.8647, 0.0243\n",
      "Training loss for epoch 21: pinn: 8.2480, boundary: 58.5182,total: 66.7662. Current alpha, lr: 0.8629, 0.0219\n",
      "Training loss for epoch 22: pinn: 7.8666, boundary: 68.5365,total: 76.4031. Current alpha, lr: 0.8612, 0.0219\n",
      "Training loss for epoch 23: pinn: 8.5620, boundary: 37.8888,total: 46.4508. Current alpha, lr: 0.8595, 0.0197\n",
      "Training loss for epoch 24: pinn: 7.6311, boundary: 39.4175,total: 47.0486. Current alpha, lr: 0.8578, 0.0197\n",
      "Training loss for epoch 25: pinn: 7.4273, boundary: 53.5073,total: 60.9346. Current alpha, lr: 0.8561, 0.0197\n",
      "Training loss for epoch 26: pinn: 7.4665, boundary: 43.6963,total: 51.1628. Current alpha, lr: 0.8544, 0.0197\n",
      "Training loss for epoch 27: pinn: 7.3882, boundary: 62.7356,total: 70.1238. Current alpha, lr: 0.8526, 0.0197\n",
      "Training loss for epoch 28: pinn: 7.0757, boundary: 41.4213,total: 48.4970. Current alpha, lr: 0.8509, 0.0197\n",
      "Training loss for epoch 29: pinn: 7.6008, boundary: 46.6685,total: 54.2693. Current alpha, lr: 0.8492, 0.0197\n",
      "Training loss for epoch 30: pinn: 7.3474, boundary: 46.5856,total: 53.9330. Current alpha, lr: 0.8475, 0.0197\n",
      "Training loss for epoch 31: pinn: 6.3121, boundary: 51.3269,total: 57.6390. Current alpha, lr: 0.8458, 0.0197\n",
      "Training loss for epoch 32: pinn: 6.8957, boundary: 46.9388,total: 53.8346. Current alpha, lr: 0.8442, 0.0177\n",
      "Training loss for epoch 33: pinn: 6.7797, boundary: 37.2648,total: 44.0444. Current alpha, lr: 0.8425, 0.0159\n",
      "Training loss for epoch 34: pinn: 6.7923, boundary: 40.4178,total: 47.2101. Current alpha, lr: 0.8408, 0.0159\n",
      "Training loss for epoch 35: pinn: 6.9986, boundary: 47.5832,total: 54.5817. Current alpha, lr: 0.8391, 0.0159\n",
      "Training loss for epoch 36: pinn: 6.8900, boundary: 37.5017,total: 44.3917. Current alpha, lr: 0.8374, 0.0159\n",
      "Training loss for epoch 37: pinn: 6.7830, boundary: 36.7819,total: 43.5650. Current alpha, lr: 0.8357, 0.0159\n",
      "Training loss for epoch 38: pinn: 7.2255, boundary: 25.0951,total: 32.3206. Current alpha, lr: 0.8341, 0.0159\n",
      "Training loss for epoch 39: pinn: 7.5297, boundary: 30.1200,total: 37.6497. Current alpha, lr: 0.8324, 0.0159\n",
      "Training loss for epoch 40: pinn: 7.0399, boundary: 35.6989,total: 42.7388. Current alpha, lr: 0.8307, 0.0159\n",
      "Training loss for epoch 41: pinn: 6.5333, boundary: 29.7768,total: 36.3101. Current alpha, lr: 0.8291, 0.0159\n",
      "Training loss for epoch 42: pinn: 7.2890, boundary: 42.6830,total: 49.9720. Current alpha, lr: 0.8274, 0.0159\n",
      "Training loss for epoch 43: pinn: 7.0118, boundary: 30.4652,total: 37.4770. Current alpha, lr: 0.8258, 0.0143\n",
      "Training loss for epoch 44: pinn: 6.5699, boundary: 30.6595,total: 37.2293. Current alpha, lr: 0.8241, 0.0143\n",
      "Training loss for epoch 45: pinn: 6.7917, boundary: 32.8859,total: 39.6776. Current alpha, lr: 0.8225, 0.0143\n",
      "Training loss for epoch 46: pinn: 6.6765, boundary: 43.5814,total: 50.2580. Current alpha, lr: 0.8208, 0.0143\n",
      "Training loss for epoch 47: pinn: 6.2929, boundary: 29.6082,total: 35.9012. Current alpha, lr: 0.8192, 0.0129\n",
      "Training loss for epoch 48: pinn: 6.4527, boundary: 26.6624,total: 33.1152. Current alpha, lr: 0.8175, 0.0129\n",
      "Training loss for epoch 49: pinn: 6.5448, boundary: 39.8821,total: 46.4269. Current alpha, lr: 0.8159, 0.0129\n",
      "Training loss for epoch 50: pinn: 6.1225, boundary: 26.5916,total: 32.7141. Current alpha, lr: 0.8143, 0.0116\n",
      "Training loss for epoch 51: pinn: 6.3094, boundary: 22.1036,total: 28.4130. Current alpha, lr: 0.8126, 0.0116\n",
      "Training loss for epoch 52: pinn: 6.1828, boundary: 29.7490,total: 35.9318. Current alpha, lr: 0.8110, 0.0116\n",
      "Training loss for epoch 53: pinn: 5.9773, boundary: 30.6510,total: 36.6284. Current alpha, lr: 0.8094, 0.0116\n",
      "Training loss for epoch 54: pinn: 6.1405, boundary: 34.2925,total: 40.4330. Current alpha, lr: 0.8078, 0.0116\n",
      "Training loss for epoch 55: pinn: 5.6688, boundary: 26.1285,total: 31.7973. Current alpha, lr: 0.8062, 0.0116\n",
      "Training loss for epoch 56: pinn: 5.8313, boundary: 36.4544,total: 42.2857. Current alpha, lr: 0.8045, 0.0116\n",
      "Training loss for epoch 57: pinn: 5.9865, boundary: 25.8626,total: 31.8491. Current alpha, lr: 0.8029, 0.0105\n",
      "Training loss for epoch 58: pinn: 5.7870, boundary: 19.0313,total: 24.8183. Current alpha, lr: 0.8013, 0.0105\n",
      "Training loss for epoch 59: pinn: 5.8455, boundary: 27.0759,total: 32.9214. Current alpha, lr: 0.7997, 0.0105\n",
      "Training loss for epoch 60: pinn: 5.7132, boundary: 37.0883,total: 42.8016. Current alpha, lr: 0.7981, 0.0094\n",
      "Training loss for epoch 61: pinn: 5.6990, boundary: 22.9177,total: 28.6167. Current alpha, lr: 0.7965, 0.0085\n",
      "Training loss for epoch 62: pinn: 5.6318, boundary: 26.8834,total: 32.5152. Current alpha, lr: 0.7949, 0.0085\n",
      "Training loss for epoch 63: pinn: 5.6084, boundary: 25.6080,total: 31.2164. Current alpha, lr: 0.7934, 0.0085\n",
      "Training loss for epoch 64: pinn: 5.4742, boundary: 24.4959,total: 29.9702. Current alpha, lr: 0.7918, 0.0085\n",
      "Training loss for epoch 65: pinn: 5.3182, boundary: 26.8794,total: 32.1977. Current alpha, lr: 0.7902, 0.0085\n",
      "Training loss for epoch 66: pinn: 5.1317, boundary: 22.4185,total: 27.5503. Current alpha, lr: 0.7886, 0.0076\n",
      "Training loss for epoch 67: pinn: 5.1335, boundary: 18.0994,total: 23.2329. Current alpha, lr: 0.7870, 0.0069\n",
      "Training loss for epoch 68: pinn: 5.1267, boundary: 18.4011,total: 23.5278. Current alpha, lr: 0.7855, 0.0069\n",
      "Training loss for epoch 69: pinn: 5.1473, boundary: 19.2643,total: 24.4116. Current alpha, lr: 0.7839, 0.0069\n",
      "Training loss for epoch 70: pinn: 5.1191, boundary: 21.7932,total: 26.9122. Current alpha, lr: 0.7823, 0.0069\n",
      "Training loss for epoch 71: pinn: 4.9722, boundary: 17.8347,total: 22.8069. Current alpha, lr: 0.7807, 0.0069\n",
      "Training loss for epoch 72: pinn: 4.9776, boundary: 17.6984,total: 22.6760. Current alpha, lr: 0.7792, 0.0069\n",
      "Training loss for epoch 73: pinn: 4.8853, boundary: 17.2854,total: 22.1708. Current alpha, lr: 0.7776, 0.0069\n",
      "Training loss for epoch 74: pinn: 4.8617, boundary: 15.4780,total: 20.3398. Current alpha, lr: 0.7761, 0.0069\n",
      "Training loss for epoch 75: pinn: 5.2692, boundary: 12.4861,total: 17.7553. Current alpha, lr: 0.7745, 0.0069\n",
      "Training loss for epoch 76: pinn: 5.1758, boundary: 17.5351,total: 22.7109. Current alpha, lr: 0.7730, 0.0069\n",
      "Training loss for epoch 77: pinn: 5.3202, boundary: 20.8602,total: 26.1804. Current alpha, lr: 0.7714, 0.0069\n",
      "Training loss for epoch 78: pinn: 5.3029, boundary: 12.4230,total: 17.7259. Current alpha, lr: 0.7699, 0.0062\n",
      "Training loss for epoch 79: pinn: 5.1911, boundary: 18.2807,total: 23.4717. Current alpha, lr: 0.7683, 0.0062\n",
      "Training loss for epoch 80: pinn: 5.2231, boundary: 14.1843,total: 19.4074. Current alpha, lr: 0.7668, 0.0056\n",
      "Training loss for epoch 81: pinn: 5.1092, boundary: 16.0015,total: 21.1107. Current alpha, lr: 0.7653, 0.0056\n",
      "Training loss for epoch 82: pinn: 5.1553, boundary: 13.8642,total: 19.0195. Current alpha, lr: 0.7637, 0.0056\n",
      "Training loss for epoch 83: pinn: 4.9922, boundary: 12.4972,total: 17.4894. Current alpha, lr: 0.7622, 0.0056\n",
      "Training loss for epoch 84: pinn: 4.9782, boundary: 15.0183,total: 19.9965. Current alpha, lr: 0.7607, 0.0056\n",
      "Training loss for epoch 85: pinn: 4.9253, boundary: 14.1418,total: 19.0671. Current alpha, lr: 0.7592, 0.0056\n",
      "Training loss for epoch 86: pinn: 4.9225, boundary: 12.0471,total: 16.9696. Current alpha, lr: 0.7577, 0.0056\n",
      "Training loss for epoch 87: pinn: 4.8220, boundary: 15.1448,total: 19.9668. Current alpha, lr: 0.7561, 0.0056\n",
      "Training loss for epoch 88: pinn: 4.7816, boundary: 15.7017,total: 20.4833. Current alpha, lr: 0.7546, 0.0056\n",
      "Training loss for epoch 89: pinn: 4.7443, boundary: 13.5048,total: 18.2491. Current alpha, lr: 0.7531, 0.0050\n",
      "Training loss for epoch 90: pinn: 4.7480, boundary: 15.3523,total: 20.1003. Current alpha, lr: 0.7516, 0.0050\n",
      "Training loss for epoch 91: pinn: 4.6630, boundary: 12.1738,total: 16.8368. Current alpha, lr: 0.7501, 0.0045\n",
      "Training loss for epoch 92: pinn: 4.7697, boundary: 11.3868,total: 16.1565. Current alpha, lr: 0.7486, 0.0045\n",
      "Training loss for epoch 93: pinn: 4.7193, boundary: 13.7214,total: 18.4408. Current alpha, lr: 0.7471, 0.0045\n",
      "Training loss for epoch 94: pinn: 4.6319, boundary: 14.0148,total: 18.6467. Current alpha, lr: 0.7456, 0.0045\n",
      "Training loss for epoch 95: pinn: 4.6004, boundary: 10.7747,total: 15.3751. Current alpha, lr: 0.7441, 0.0041\n",
      "Training loss for epoch 96: pinn: 4.5331, boundary: 10.2626,total: 14.7956. Current alpha, lr: 0.7426, 0.0041\n",
      "Training loss for epoch 97: pinn: 4.4227, boundary: 10.0721,total: 14.4947. Current alpha, lr: 0.7411, 0.0041\n",
      "Training loss for epoch 98: pinn: 4.4492, boundary: 9.4428,total: 13.8920. Current alpha, lr: 0.7397, 0.0041\n",
      "Training loss for epoch 99: pinn: 4.4163, boundary: 10.9419,total: 15.3582. Current alpha, lr: 0.7382, 0.0041\n",
      "Training loss for epoch 100: pinn: 4.4491, boundary: 10.5007,total: 14.9498. Current alpha, lr: 0.7367, 0.0041\n",
      "Training loss for epoch 101: pinn: 4.4033, boundary: 8.8290,total: 13.2323. Current alpha, lr: 0.7352, 0.0041\n",
      "Training loss for epoch 102: pinn: 4.5586, boundary: 7.8919,total: 12.4505. Current alpha, lr: 0.7338, 0.0041\n",
      "Training loss for epoch 103: pinn: 4.4111, boundary: 10.2501,total: 14.6612. Current alpha, lr: 0.7323, 0.0041\n",
      "Training loss for epoch 104: pinn: 4.3551, boundary: 10.2595,total: 14.6146. Current alpha, lr: 0.7308, 0.0041\n",
      "Training loss for epoch 105: pinn: 4.3185, boundary: 10.8500,total: 15.1685. Current alpha, lr: 0.7294, 0.0041\n",
      "Training loss for epoch 106: pinn: 4.2880, boundary: 11.0729,total: 15.3608. Current alpha, lr: 0.7279, 0.0036\n",
      "Training loss for epoch 107: pinn: 4.2523, boundary: 6.1969,total: 10.4492. Current alpha, lr: 0.7265, 0.0033\n",
      "Training loss for epoch 108: pinn: 4.2425, boundary: 9.1620,total: 13.4046. Current alpha, lr: 0.7250, 0.0033\n",
      "Training loss for epoch 109: pinn: 4.2074, boundary: 11.3864,total: 15.5938. Current alpha, lr: 0.7236, 0.0033\n",
      "Training loss for epoch 110: pinn: 4.1648, boundary: 7.2885,total: 11.4534. Current alpha, lr: 0.7221, 0.0030\n",
      "Training loss for epoch 111: pinn: 4.0677, boundary: 7.5811,total: 11.6488. Current alpha, lr: 0.7207, 0.0030\n",
      "Training loss for epoch 112: pinn: 4.1166, boundary: 7.6755,total: 11.7921. Current alpha, lr: 0.7192, 0.0030\n",
      "Training loss for epoch 113: pinn: 4.1057, boundary: 7.7156,total: 11.8213. Current alpha, lr: 0.7178, 0.0030\n",
      "Training loss for epoch 114: pinn: 4.0319, boundary: 8.2959,total: 12.3278. Current alpha, lr: 0.7163, 0.0030\n",
      "Training loss for epoch 115: pinn: 4.1145, boundary: 6.0645,total: 10.1790. Current alpha, lr: 0.7149, 0.0030\n",
      "Training loss for epoch 116: pinn: 4.0950, boundary: 10.5664,total: 14.6614. Current alpha, lr: 0.7135, 0.0030\n",
      "Training loss for epoch 117: pinn: 4.0110, boundary: 7.1606,total: 11.1716. Current alpha, lr: 0.7121, 0.0027\n",
      "Training loss for epoch 118: pinn: 3.9964, boundary: 7.9405,total: 11.9368. Current alpha, lr: 0.7106, 0.0027\n",
      "Training loss for epoch 119: pinn: 4.0415, boundary: 6.4173,total: 10.4588. Current alpha, lr: 0.7092, 0.0024\n",
      "Training loss for epoch 120: pinn: 4.0404, boundary: 6.4056,total: 10.4460. Current alpha, lr: 0.7078, 0.0024\n",
      "Training loss for epoch 121: pinn: 4.0610, boundary: 6.5290,total: 10.5900. Current alpha, lr: 0.7064, 0.0024\n",
      "Training loss for epoch 122: pinn: 4.0567, boundary: 5.9274,total: 9.9841. Current alpha, lr: 0.7050, 0.0024\n",
      "Training loss for epoch 123: pinn: 4.0000, boundary: 5.4762,total: 9.4762. Current alpha, lr: 0.7036, 0.0024\n",
      "Training loss for epoch 124: pinn: 3.9759, boundary: 8.0476,total: 12.0235. Current alpha, lr: 0.7021, 0.0024\n",
      "Training loss for epoch 125: pinn: 3.9515, boundary: 6.5199,total: 10.4714. Current alpha, lr: 0.7007, 0.0024\n",
      "Training loss for epoch 126: pinn: 3.9876, boundary: 4.6917,total: 8.6793. Current alpha, lr: 0.6993, 0.0024\n",
      "Training loss for epoch 127: pinn: 4.0137, boundary: 4.9884,total: 9.0021. Current alpha, lr: 0.6979, 0.0024\n",
      "Training loss for epoch 128: pinn: 3.9259, boundary: 5.0776,total: 9.0035. Current alpha, lr: 0.6965, 0.0024\n",
      "Training loss for epoch 129: pinn: 3.9331, boundary: 6.9577,total: 10.8909. Current alpha, lr: 0.6952, 0.0024\n",
      "Training loss for epoch 130: pinn: 3.8939, boundary: 5.8397,total: 9.7336. Current alpha, lr: 0.6938, 0.0022\n",
      "Training loss for epoch 131: pinn: 3.9288, boundary: 4.9708,total: 8.8996. Current alpha, lr: 0.6924, 0.0022\n",
      "Training loss for epoch 132: pinn: 3.9039, boundary: 5.5651,total: 9.4690. Current alpha, lr: 0.6910, 0.0022\n",
      "Training loss for epoch 133: pinn: 3.9514, boundary: 4.1020,total: 8.0535. Current alpha, lr: 0.6896, 0.0022\n",
      "Training loss for epoch 134: pinn: 3.9088, boundary: 6.2063,total: 10.1151. Current alpha, lr: 0.6882, 0.0022\n",
      "Training loss for epoch 135: pinn: 3.8683, boundary: 5.5478,total: 9.4162. Current alpha, lr: 0.6869, 0.0019\n",
      "Training loss for epoch 136: pinn: 3.8478, boundary: 4.6189,total: 8.4667. Current alpha, lr: 0.6855, 0.0017\n",
      "Training loss for epoch 137: pinn: 3.8323, boundary: 5.2123,total: 9.0445. Current alpha, lr: 0.6841, 0.0017\n",
      "Training loss for epoch 138: pinn: 3.8336, boundary: 5.3330,total: 9.1666. Current alpha, lr: 0.6827, 0.0017\n",
      "Training loss for epoch 139: pinn: 3.8344, boundary: 4.1547,total: 7.9890. Current alpha, lr: 0.6814, 0.0017\n",
      "Training loss for epoch 140: pinn: 3.8287, boundary: 4.0159,total: 7.8445. Current alpha, lr: 0.6800, 0.0017\n",
      "Training loss for epoch 141: pinn: 3.8788, boundary: 3.9078,total: 7.7865. Current alpha, lr: 0.6787, 0.0017\n",
      "Training loss for epoch 142: pinn: 3.7855, boundary: 5.0191,total: 8.8046. Current alpha, lr: 0.6773, 0.0017\n",
      "Training loss for epoch 143: pinn: 3.8015, boundary: 5.6033,total: 9.4048. Current alpha, lr: 0.6759, 0.0017\n",
      "Training loss for epoch 144: pinn: 3.8514, boundary: 4.8065,total: 8.6579. Current alpha, lr: 0.6746, 0.0017\n",
      "Training loss for epoch 145: pinn: 3.7806, boundary: 3.9018,total: 7.6824. Current alpha, lr: 0.6732, 0.0016\n",
      "Training loss for epoch 146: pinn: 3.7859, boundary: 4.8132,total: 8.5991. Current alpha, lr: 0.6719, 0.0016\n",
      "Training loss for epoch 147: pinn: 3.7584, boundary: 4.6332,total: 8.3916. Current alpha, lr: 0.6706, 0.0016\n",
      "Training loss for epoch 148: pinn: 3.7496, boundary: 3.7587,total: 7.5083. Current alpha, lr: 0.6692, 0.0014\n",
      "Training loss for epoch 149: pinn: 3.7694, boundary: 4.1979,total: 7.9673. Current alpha, lr: 0.6679, 0.0014\n",
      "Training loss for epoch 150: pinn: 3.7734, boundary: 3.4612,total: 7.2347. Current alpha, lr: 0.6665, 0.0013\n",
      "Training loss for epoch 151: pinn: 3.7573, boundary: 3.9304,total: 7.6877. Current alpha, lr: 0.6652, 0.0013\n",
      "Training loss for epoch 152: pinn: 3.7245, boundary: 3.3362,total: 7.0606. Current alpha, lr: 0.6639, 0.0013\n",
      "Training loss for epoch 153: pinn: 3.6966, boundary: 3.4343,total: 7.1309. Current alpha, lr: 0.6625, 0.0013\n",
      "Training loss for epoch 154: pinn: 3.6812, boundary: 3.2545,total: 6.9358. Current alpha, lr: 0.6612, 0.0013\n",
      "Training loss for epoch 155: pinn: 3.7093, boundary: 3.1435,total: 6.8528. Current alpha, lr: 0.6599, 0.0013\n",
      "Training loss for epoch 156: pinn: 3.6247, boundary: 3.0333,total: 6.6579. Current alpha, lr: 0.6586, 0.0013\n",
      "Training loss for epoch 157: pinn: 3.5866, boundary: 3.7370,total: 7.3236. Current alpha, lr: 0.6573, 0.0013\n",
      "Training loss for epoch 158: pinn: 3.5971, boundary: 3.4111,total: 7.0082. Current alpha, lr: 0.6559, 0.0013\n",
      "Training loss for epoch 159: pinn: 3.5925, boundary: 3.8457,total: 7.4382. Current alpha, lr: 0.6546, 0.0013\n",
      "Training loss for epoch 160: pinn: 3.5234, boundary: 3.7966,total: 7.3200. Current alpha, lr: 0.6533, 0.0013\n",
      "Training loss for epoch 161: pinn: 3.5361, boundary: 3.6296,total: 7.1658. Current alpha, lr: 0.6520, 0.0011\n",
      "Training loss for epoch 162: pinn: 3.5021, boundary: 2.2665,total: 5.7686. Current alpha, lr: 0.6507, 0.0010\n",
      "Training loss for epoch 163: pinn: 3.5186, boundary: 2.5719,total: 6.0905. Current alpha, lr: 0.6494, 0.0010\n",
      "Training loss for epoch 164: pinn: 3.4748, boundary: 2.7340,total: 6.2088. Current alpha, lr: 0.6481, 0.0010\n",
      "Training loss for epoch 165: pinn: 3.4910, boundary: 2.6921,total: 6.1830. Current alpha, lr: 0.6468, 0.0010\n",
      "Training loss for epoch 166: pinn: 3.4887, boundary: 2.8476,total: 6.3363. Current alpha, lr: 0.6455, 0.0010\n",
      "Training loss for epoch 167: pinn: 3.4455, boundary: 3.1523,total: 6.5979. Current alpha, lr: 0.6442, 0.0010\n",
      "Training loss for epoch 168: pinn: 3.4262, boundary: 2.7852,total: 6.2115. Current alpha, lr: 0.6429, 0.0010\n",
      "Training loss for epoch 169: pinn: 3.4415, boundary: 2.6102,total: 6.0517. Current alpha, lr: 0.6417, 0.0010\n",
      "Training loss for epoch 170: pinn: 3.4273, boundary: 2.6814,total: 6.1087. Current alpha, lr: 0.6404, 0.0010\n",
      "Training loss for epoch 171: pinn: 3.4311, boundary: 2.8145,total: 6.2456. Current alpha, lr: 0.6391, 0.0009\n",
      "Training loss for epoch 172: pinn: 3.3917, boundary: 2.4878,total: 5.8795. Current alpha, lr: 0.6378, 0.0008\n",
      "Training loss for epoch 173: pinn: 3.4007, boundary: 2.4891,total: 5.8897. Current alpha, lr: 0.6365, 0.0008\n",
      "Training loss for epoch 174: pinn: 3.3905, boundary: 2.8326,total: 6.2230. Current alpha, lr: 0.6353, 0.0008\n",
      "Training loss for epoch 175: pinn: 3.3382, boundary: 2.7662,total: 6.1044. Current alpha, lr: 0.6340, 0.0008\n",
      "Training loss for epoch 176: pinn: 3.3642, boundary: 2.8963,total: 6.2605. Current alpha, lr: 0.6327, 0.0008\n",
      "Training loss for epoch 177: pinn: 3.3789, boundary: 2.4059,total: 5.7848. Current alpha, lr: 0.6315, 0.0008\n",
      "Training loss for epoch 178: pinn: 3.3510, boundary: 2.2146,total: 5.5657. Current alpha, lr: 0.6302, 0.0008\n",
      "Training loss for epoch 179: pinn: 3.3690, boundary: 2.3416,total: 5.7106. Current alpha, lr: 0.6289, 0.0008\n",
      "Training loss for epoch 180: pinn: 3.3530, boundary: 2.5367,total: 5.8897. Current alpha, lr: 0.6277, 0.0008\n",
      "Training loss for epoch 181: pinn: 3.3322, boundary: 1.6458,total: 4.9780. Current alpha, lr: 0.6264, 0.0007\n",
      "Training loss for epoch 182: pinn: 3.2799, boundary: 1.7199,total: 4.9998. Current alpha, lr: 0.6252, 0.0007\n",
      "Training loss for epoch 183: pinn: 3.2993, boundary: 1.8496,total: 5.1489. Current alpha, lr: 0.6239, 0.0007\n",
      "Training loss for epoch 184: pinn: 3.3013, boundary: 2.0936,total: 5.3949. Current alpha, lr: 0.6227, 0.0007\n",
      "Training loss for epoch 185: pinn: 3.2942, boundary: 1.8163,total: 5.1105. Current alpha, lr: 0.6214, 0.0007\n",
      "Training loss for epoch 186: pinn: 3.2795, boundary: 2.2458,total: 5.5253. Current alpha, lr: 0.6202, 0.0007\n",
      "Training loss for epoch 187: pinn: 3.2738, boundary: 1.9157,total: 5.1896. Current alpha, lr: 0.6189, 0.0007\n",
      "Training loss for epoch 188: pinn: 3.2389, boundary: 2.3704,total: 5.6093. Current alpha, lr: 0.6177, 0.0007\n",
      "Training loss for epoch 189: pinn: 3.2465, boundary: 2.3920,total: 5.6386. Current alpha, lr: 0.6165, 0.0007\n",
      "Training loss for epoch 190: pinn: 3.2286, boundary: 1.8826,total: 5.1112. Current alpha, lr: 0.6152, 0.0006\n",
      "Training loss for epoch 191: pinn: 3.2331, boundary: 1.6872,total: 4.9203. Current alpha, lr: 0.6140, 0.0005\n",
      "Training loss for epoch 192: pinn: 3.2189, boundary: 1.8873,total: 5.1062. Current alpha, lr: 0.6128, 0.0005\n",
      "Training loss for epoch 193: pinn: 3.2048, boundary: 1.7958,total: 5.0006. Current alpha, lr: 0.6116, 0.0005\n",
      "Training loss for epoch 194: pinn: 3.2194, boundary: 1.4698,total: 4.6892. Current alpha, lr: 0.6103, 0.0005\n",
      "Training loss for epoch 195: pinn: 3.2134, boundary: 1.9619,total: 5.1753. Current alpha, lr: 0.6091, 0.0005\n",
      "Training loss for epoch 196: pinn: 3.2028, boundary: 1.5287,total: 4.7314. Current alpha, lr: 0.6079, 0.0005\n",
      "Training loss for epoch 197: pinn: 3.1760, boundary: 2.0117,total: 5.1877. Current alpha, lr: 0.6067, 0.0005\n",
      "Training loss for epoch 198: pinn: 3.1672, boundary: 2.0066,total: 5.1738. Current alpha, lr: 0.6055, 0.0005\n",
      "Training loss for epoch 199: pinn: 3.1620, boundary: 1.6054,total: 4.7675. Current alpha, lr: 0.6043, 0.0005\n",
      "Training loss for epoch 200: pinn: 3.1626, boundary: 1.6338,total: 4.7964. Current alpha, lr: 0.6030, 0.0005\n",
      "Training loss for epoch 201: pinn: 3.1749, boundary: 1.6652,total: 4.8402. Current alpha, lr: 0.6018, 0.0005\n",
      "Training loss for epoch 202: pinn: 3.1604, boundary: 1.6599,total: 4.8203. Current alpha, lr: 0.6006, 0.0005\n",
      "Training loss for epoch 203: pinn: 3.1376, boundary: 1.6604,total: 4.7980. Current alpha, lr: 0.5994, 0.0004\n",
      "Training loss for epoch 204: pinn: 3.1161, boundary: 1.5536,total: 4.6697. Current alpha, lr: 0.5982, 0.0004\n",
      "Training loss for epoch 205: pinn: 3.0998, boundary: 1.5257,total: 4.6255. Current alpha, lr: 0.5970, 0.0004\n",
      "Training loss for epoch 206: pinn: 3.0983, boundary: 1.6112,total: 4.7095. Current alpha, lr: 0.5958, 0.0004\n",
      "Training loss for epoch 207: pinn: 3.0903, boundary: 1.3587,total: 4.4490. Current alpha, lr: 0.5947, 0.0004\n",
      "Training loss for epoch 208: pinn: 3.0796, boundary: 1.6908,total: 4.7704. Current alpha, lr: 0.5935, 0.0004\n",
      "Training loss for epoch 209: pinn: 3.0527, boundary: 1.4769,total: 4.5295. Current alpha, lr: 0.5923, 0.0004\n",
      "Training loss for epoch 210: pinn: 3.0808, boundary: 1.3997,total: 4.4806. Current alpha, lr: 0.5911, 0.0004\n",
      "Training loss for epoch 211: pinn: 3.0869, boundary: 1.5783,total: 4.6653. Current alpha, lr: 0.5899, 0.0004\n",
      "Training loss for epoch 212: pinn: 3.0492, boundary: 1.5338,total: 4.5830. Current alpha, lr: 0.5887, 0.0004\n",
      "Training loss for epoch 213: pinn: 3.0330, boundary: 1.7262,total: 4.7591. Current alpha, lr: 0.5876, 0.0004\n",
      "Training loss for epoch 214: pinn: 3.0402, boundary: 1.4691,total: 4.5092. Current alpha, lr: 0.5864, 0.0004\n",
      "Training loss for epoch 215: pinn: 3.0907, boundary: 1.5147,total: 4.6054. Current alpha, lr: 0.5852, 0.0004\n",
      "Training loss for epoch 216: pinn: 3.0244, boundary: 1.3842,total: 4.4086. Current alpha, lr: 0.5840, 0.0004\n",
      "Training loss for epoch 217: pinn: 3.0609, boundary: 1.3234,total: 4.3843. Current alpha, lr: 0.5829, 0.0004\n",
      "Training loss for epoch 218: pinn: 3.0249, boundary: 1.3704,total: 4.3953. Current alpha, lr: 0.5817, 0.0004\n",
      "Training loss for epoch 219: pinn: 2.9936, boundary: 1.5456,total: 4.5392. Current alpha, lr: 0.5805, 0.0004\n",
      "Training loss for epoch 220: pinn: 3.0311, boundary: 1.4535,total: 4.4846. Current alpha, lr: 0.5794, 0.0004\n",
      "Training loss for epoch 221: pinn: 2.9994, boundary: 1.4436,total: 4.4430. Current alpha, lr: 0.5782, 0.0004\n",
      "Training loss for epoch 222: pinn: 3.0342, boundary: 1.3696,total: 4.4038. Current alpha, lr: 0.5771, 0.0004\n",
      "Training loss for epoch 223: pinn: 2.9776, boundary: 1.3830,total: 4.3607. Current alpha, lr: 0.5759, 0.0004\n",
      "Training loss for epoch 224: pinn: 3.0026, boundary: 1.3806,total: 4.3831. Current alpha, lr: 0.5748, 0.0004\n",
      "Training loss for epoch 225: pinn: 2.9530, boundary: 1.1872,total: 4.1401. Current alpha, lr: 0.5736, 0.0004\n",
      "Training loss for epoch 226: pinn: 2.9511, boundary: 1.4752,total: 4.4263. Current alpha, lr: 0.5725, 0.0004\n",
      "Training loss for epoch 227: pinn: 2.9521, boundary: 1.4168,total: 4.3690. Current alpha, lr: 0.5713, 0.0003\n",
      "Training loss for epoch 228: pinn: 2.9886, boundary: 1.4045,total: 4.3931. Current alpha, lr: 0.5702, 0.0003\n",
      "Training loss for epoch 229: pinn: 2.9546, boundary: 1.3768,total: 4.3313. Current alpha, lr: 0.5690, 0.0003\n",
      "Training loss for epoch 230: pinn: 2.9610, boundary: 1.3008,total: 4.2618. Current alpha, lr: 0.5679, 0.0003\n",
      "Training loss for epoch 231: pinn: 2.9597, boundary: 1.3041,total: 4.2638. Current alpha, lr: 0.5668, 0.0003\n",
      "Training loss for epoch 232: pinn: 2.9501, boundary: 1.2591,total: 4.2092. Current alpha, lr: 0.5656, 0.0003\n",
      "Training loss for epoch 233: pinn: 2.9081, boundary: 1.2789,total: 4.1870. Current alpha, lr: 0.5645, 0.0003\n",
      "Training loss for epoch 234: pinn: 2.9335, boundary: 1.2306,total: 4.1641. Current alpha, lr: 0.5634, 0.0003\n",
      "Training loss for epoch 235: pinn: 2.9214, boundary: 1.2689,total: 4.1903. Current alpha, lr: 0.5622, 0.0003\n",
      "Training loss for epoch 236: pinn: 2.9224, boundary: 1.2902,total: 4.2126. Current alpha, lr: 0.5611, 0.0003\n",
      "Training loss for epoch 237: pinn: 2.8892, boundary: 1.3051,total: 4.1944. Current alpha, lr: 0.5600, 0.0003\n",
      "Training loss for epoch 238: pinn: 2.9234, boundary: 1.2511,total: 4.1745. Current alpha, lr: 0.5589, 0.0003\n",
      "Training loss for epoch 239: pinn: 2.8601, boundary: 1.2486,total: 4.1087. Current alpha, lr: 0.5578, 0.0003\n",
      "Training loss for epoch 240: pinn: 2.9095, boundary: 1.1298,total: 4.0394. Current alpha, lr: 0.5566, 0.0003\n",
      "Training loss for epoch 241: pinn: 2.8828, boundary: 1.1417,total: 4.0244. Current alpha, lr: 0.5555, 0.0003\n",
      "Training loss for epoch 242: pinn: 2.8847, boundary: 1.1232,total: 4.0079. Current alpha, lr: 0.5544, 0.0003\n",
      "Training loss for epoch 243: pinn: 2.8435, boundary: 1.2167,total: 4.0603. Current alpha, lr: 0.5533, 0.0003\n",
      "Training loss for epoch 244: pinn: 2.8631, boundary: 1.2462,total: 4.1094. Current alpha, lr: 0.5522, 0.0003\n",
      "Training loss for epoch 245: pinn: 2.8509, boundary: 1.2774,total: 4.1282. Current alpha, lr: 0.5511, 0.0003\n",
      "Training loss for epoch 246: pinn: 2.8198, boundary: 1.2396,total: 4.0594. Current alpha, lr: 0.5500, 0.0003\n",
      "Training loss for epoch 247: pinn: 2.8464, boundary: 1.2742,total: 4.1206. Current alpha, lr: 0.5489, 0.0003\n",
      "Training loss for epoch 248: pinn: 2.7849, boundary: 1.1644,total: 3.9493. Current alpha, lr: 0.5478, 0.0003\n",
      "Training loss for epoch 249: pinn: 2.8002, boundary: 1.2461,total: 4.0463. Current alpha, lr: 0.5467, 0.0003\n",
      "Training loss for epoch 250: pinn: 2.8161, boundary: 1.2133,total: 4.0295. Current alpha, lr: 0.5456, 0.0002\n",
      "Training loss for epoch 251: pinn: 2.8071, boundary: 1.0691,total: 3.8762. Current alpha, lr: 0.5445, 0.0002\n",
      "Training loss for epoch 252: pinn: 2.8153, boundary: 1.0179,total: 3.8332. Current alpha, lr: 0.5434, 0.0002\n",
      "Training loss for epoch 253: pinn: 2.7768, boundary: 1.0139,total: 3.7908. Current alpha, lr: 0.5423, 0.0002\n",
      "Training loss for epoch 254: pinn: 2.7617, boundary: 1.0010,total: 3.7627. Current alpha, lr: 0.5413, 0.0002\n",
      "Training loss for epoch 255: pinn: 2.7761, boundary: 1.0611,total: 3.8372. Current alpha, lr: 0.5402, 0.0002\n",
      "Training loss for epoch 256: pinn: 2.7914, boundary: 1.0408,total: 3.8322. Current alpha, lr: 0.5391, 0.0002\n",
      "Training loss for epoch 257: pinn: 2.7316, boundary: 1.0246,total: 3.7562. Current alpha, lr: 0.5380, 0.0002\n",
      "Training loss for epoch 258: pinn: 2.7639, boundary: 1.0487,total: 3.8127. Current alpha, lr: 0.5369, 0.0002\n",
      "Training loss for epoch 259: pinn: 2.7557, boundary: 1.0516,total: 3.8073. Current alpha, lr: 0.5359, 0.0002\n",
      "Training loss for epoch 260: pinn: 2.7391, boundary: 1.1335,total: 3.8726. Current alpha, lr: 0.5348, 0.0002\n",
      "Training loss for epoch 261: pinn: 2.7481, boundary: 1.0143,total: 3.7624. Current alpha, lr: 0.5337, 0.0002\n",
      "Training loss for epoch 262: pinn: 2.7155, boundary: 1.0060,total: 3.7215. Current alpha, lr: 0.5327, 0.0002\n",
      "Training loss for epoch 263: pinn: 2.7175, boundary: 1.0336,total: 3.7511. Current alpha, lr: 0.5316, 0.0002\n",
      "Training loss for epoch 264: pinn: 2.7216, boundary: 1.0235,total: 3.7451. Current alpha, lr: 0.5305, 0.0002\n",
      "Training loss for epoch 265: pinn: 2.7131, boundary: 1.0169,total: 3.7299. Current alpha, lr: 0.5295, 0.0002\n",
      "Training loss for epoch 266: pinn: 2.6609, boundary: 1.0919,total: 3.7527. Current alpha, lr: 0.5284, 0.0002\n",
      "Training loss for epoch 267: pinn: 2.6617, boundary: 0.9994,total: 3.6610. Current alpha, lr: 0.5273, 0.0002\n",
      "Training loss for epoch 268: pinn: 2.6922, boundary: 0.9850,total: 3.6772. Current alpha, lr: 0.5263, 0.0002\n",
      "Training loss for epoch 269: pinn: 2.6860, boundary: 0.9864,total: 3.6724. Current alpha, lr: 0.5252, 0.0002\n",
      "Training loss for epoch 270: pinn: 2.6541, boundary: 1.1668,total: 3.8209. Current alpha, lr: 0.5242, 0.0002\n",
      "Training loss for epoch 271: pinn: 2.6482, boundary: 0.9732,total: 3.6215. Current alpha, lr: 0.5231, 0.0002\n",
      "Training loss for epoch 272: pinn: 2.6311, boundary: 0.9468,total: 3.5779. Current alpha, lr: 0.5221, 0.0002\n",
      "Training loss for epoch 273: pinn: 2.6580, boundary: 0.9929,total: 3.6510. Current alpha, lr: 0.5211, 0.0002\n",
      "Training loss for epoch 274: pinn: 2.6323, boundary: 0.9479,total: 3.5802. Current alpha, lr: 0.5200, 0.0002\n",
      "Training loss for epoch 275: pinn: 2.6246, boundary: 0.9817,total: 3.6063. Current alpha, lr: 0.5190, 0.0002\n",
      "Training loss for epoch 276: pinn: 2.6237, boundary: 0.9910,total: 3.6147. Current alpha, lr: 0.5179, 0.0002\n",
      "Training loss for epoch 277: pinn: 2.6083, boundary: 1.0082,total: 3.6164. Current alpha, lr: 0.5169, 0.0002\n",
      "Training loss for epoch 278: pinn: 2.5951, boundary: 1.0401,total: 3.6352. Current alpha, lr: 0.5159, 0.0002\n",
      "Training loss for epoch 279: pinn: 2.5797, boundary: 0.9771,total: 3.5568. Current alpha, lr: 0.5148, 0.0002\n",
      "Training loss for epoch 280: pinn: 2.5548, boundary: 1.0113,total: 3.5661. Current alpha, lr: 0.5138, 0.0002\n",
      "Training loss for epoch 281: pinn: 2.5829, boundary: 0.9545,total: 3.5374. Current alpha, lr: 0.5128, 0.0002\n",
      "Training loss for epoch 282: pinn: 2.5840, boundary: 0.9799,total: 3.5639. Current alpha, lr: 0.5117, 0.0002\n",
      "Training loss for epoch 283: pinn: 2.5654, boundary: 1.0141,total: 3.5795. Current alpha, lr: 0.5107, 0.0002\n",
      "Training loss for epoch 284: pinn: 2.5962, boundary: 1.0338,total: 3.6300. Current alpha, lr: 0.5097, 0.0002\n",
      "Training loss for epoch 285: pinn: 2.5458, boundary: 1.0000,total: 3.5457. Current alpha, lr: 0.5087, 0.0002\n",
      "Training loss for epoch 286: pinn: 2.5347, boundary: 0.9971,total: 3.5318. Current alpha, lr: 0.5077, 0.0002\n",
      "Training loss for epoch 287: pinn: 2.5468, boundary: 0.9264,total: 3.4732. Current alpha, lr: 0.5066, 0.0002\n",
      "Training loss for epoch 288: pinn: 2.5266, boundary: 0.8880,total: 3.4146. Current alpha, lr: 0.5056, 0.0002\n",
      "Training loss for epoch 289: pinn: 2.5393, boundary: 0.9409,total: 3.4802. Current alpha, lr: 0.5046, 0.0002\n",
      "Training loss for epoch 290: pinn: 2.5202, boundary: 0.9775,total: 3.4977. Current alpha, lr: 0.5036, 0.0002\n",
      "Training loss for epoch 291: pinn: 2.5546, boundary: 0.9617,total: 3.5163. Current alpha, lr: 0.5026, 0.0002\n",
      "Training loss for epoch 292: pinn: 2.4962, boundary: 0.9319,total: 3.4282. Current alpha, lr: 0.5016, 0.0002\n",
      "Training loss for epoch 293: pinn: 2.4983, boundary: 0.9201,total: 3.4184. Current alpha, lr: 0.5006, 0.0002\n",
      "Training loss for epoch 294: pinn: 2.5253, boundary: 0.9487,total: 3.4741. Current alpha, lr: 0.4996, 0.0002\n",
      "Training loss for epoch 295: pinn: 2.5289, boundary: 0.9539,total: 3.4827. Current alpha, lr: 0.4986, 0.0002\n",
      "Training loss for epoch 296: pinn: 2.4942, boundary: 0.9234,total: 3.4176. Current alpha, lr: 0.4976, 0.0001\n",
      "Training loss for epoch 297: pinn: 2.4705, boundary: 0.8707,total: 3.3412. Current alpha, lr: 0.4966, 0.0001\n",
      "Training loss for epoch 298: pinn: 2.4821, boundary: 0.8924,total: 3.3746. Current alpha, lr: 0.4956, 0.0001\n",
      "Training loss for epoch 299: pinn: 2.4512, boundary: 0.8891,total: 3.3403. Current alpha, lr: 0.4946, 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Neural network. Note: 2 inputs- (p, r), 1 output- f(r, p)\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(140, activation='selu')(inputs)\n",
    "x_ = tf.keras.layers.Dense(140, activation='selu')(x_)\n",
    "x_ = tf.keras.layers.Dense(140, activation='selu')(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.9\n",
    "alpha_decay = 0.998\n",
    "lr = 3e-2\n",
    "lr_decay = 0.95\n",
    "patience = 10\n",
    "batchsize = 1032\n",
    "boundary_batchsize = 256\n",
    "epochs = 300\n",
    "save = False\n",
    "load_epoch = -1\n",
    "filename = ''\n",
    "\n",
    "# Initialize and fit the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "            f_boundary=f_boundary[:, 0], size=size)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, batchsize=batchsize, boundary_batchsize=boundary_batchsize,\n",
    "                                                         epochs=epochs, lr=lr, size=size, save=save, load_epoch=load_epoch, lr_decay=lr_decay,\n",
    "                                                         alpha_decay=alpha_decay, patience=patience, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the outputs to a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open('./figures/pickles/pinn_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/pickles/boundary_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/pickles/predictions_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "# with open('./figures/pickles/f_boundary.pkl', 'wb') as file:\n",
    "#     pkl.dump(f_boundary, file)\n",
    "    \n",
    "# with open('./figures/pickles/p.pkl', 'wb') as file:\n",
    "#     pkl.dump(p, file)\n",
    "    \n",
    "# with open('./figures/pickles/T.pkl', 'wb') as file:\n",
    "#     pkl.dump(T, file)\n",
    "    \n",
    "# with open('./figures/pickles/r.pkl', 'wb') as file:\n",
    "#     pkl.dump(r, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
