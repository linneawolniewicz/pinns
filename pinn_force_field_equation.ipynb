{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Physics Informed Neural Networks\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook adapts the code implementation of Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs, and the code by Michael Ito, to solve the force-field equation for solar modulation of cosmic rays. Michael Ito's code adaption use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure. \n",
    "\n",
    "In this application, our PINN is $h(r, p) = \\frac{\\partial f}{\\partial r} + \\frac{RV}{3k} \\frac{\\partial f}{\\partial p}$ where $k=\\beta(p)k_1(r)k_2(r)$ and $\\beta = \\frac{p}{\\sqrt{p^2 + M^2}}$. We will approximate $f(r, p)$ using a neural network.\n",
    "\n",
    "We have no initial data, but our boundary data will be given by $f(r_{HP}, p) = \\frac{J(r_{HP}, T)}{p^2} = \\frac{(T+M)^\\gamma}{p^2}$, where $r_{HP} = 120$ AU (i.e. the radius of Heliopause), $M=0.938$ GeV, $\\gamma$ is between $-2$ and $-3$, and $T = \\sqrt{p^2 + M^2} - M$. Or, vice versa, $p = \\sqrt{T^2 + 2TM}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 14:45:45.454222: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-08-31 14:45:45.454295: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gpu-0008\n",
      "2022-08-31 14:45:45.454306: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gpu-0008\n",
      "2022-08-31 14:45:45.454423: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.43.4\n",
      "2022-08-31 14:45:45.454466: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.43.4\n",
      "2022-08-31 14:45:45.454473: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.43.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: (256, 1), p: (256, 1), T: (256, 1), f_boundary: (256, 1), P_star: (65536, 2), lb: [[-3.13904026]\n",
      " [ 0.        ]], ub:[[6.9086924 ]\n",
      " [4.78749174]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'f(r_HP, p)')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constants  \n",
    "M = 0.938 # GeV\n",
    "gamma = -2.5 # Between -2 and -3\n",
    "size = 256 # size of r, T, p, and f_boundary\n",
    "\n",
    "# Create intial r, p, and T predict data\n",
    "T = np.linspace(0.001, 1000, size).flatten()[:,None]\n",
    "p = np.sqrt(T**2 + 2*T*M).flatten()[:,None] # p values\n",
    "r = np.linspace(1, 120, size).flatten()[:,None] # r values\n",
    "\n",
    "# Create boundary f data (f at r_HP) for boundary loss\n",
    "f_boundary = ((T + M)**gamma)/(p**2)\n",
    "\n",
    "# Take the log of all input data\n",
    "r = np.log(r)\n",
    "T = np.log(T)\n",
    "p = np.log(p)\n",
    "f_boundary = np.log(f_boundary)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([p[0], r[0]]) # (p, r) in (GeV, AU)\n",
    "ub = np.array([p[-1], r[-1]]) # (p, r) in (GeV, AU)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "P, R = np.meshgrid(p, r)\n",
    "P_star = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))\n",
    "\n",
    "# Check inputs\n",
    "print(f'r: {r.shape}, p: {p.shape}, T: {T.shape}, f_boundary: {f_boundary.shape}, P_star: {P_star.shape}, lb: {lb}, ub:{ub}')\n",
    "\n",
    "plt.plot(p, f_boundary)\n",
    "plt.xlabel(\"p (ln(GeV))\")\n",
    "plt.ylabel(\"f(r_HP, p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, r, f_boundary, size, n_samples=20000, n_boundary=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.r = r\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.n_boundary = n_boundary\n",
    "        self.size = size\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and boundary_f.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "    Outputs: total_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha=1, beta=1):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                # Forward pass P (PINN data)\n",
    "                P = tf.concat((p, r), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Forward pass P_boundary (boundary condition data)\n",
    "                P_boundary = tf.concat((p_boundary, r_boundary), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "\n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tf.math.reduce_mean(tf.math.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            f_p = t1.gradient(f, p)\n",
    "            f_r = t1.gradient(f, r)\n",
    "\n",
    "            # Calculate resulting loss = PINN loss + boundary loss\n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r)\n",
    "            sum_loss = alpha*pinn_loss + beta*boundary_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(sum_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Return losses\n",
    "        return sum_loss.numpy(), pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        initial_batchsize: batchsize for (x_initial, t_initial) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "    \n",
    "    Outputs: Losses for each equation (PDE, Initial Value, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, size, alpha=1, beta=1, batchsize=64, boundary_batchsize=16, epochs=20):\n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        total_loss = np.zeros((epochs, ))\n",
    "        total_predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Reset loss variables\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN and boundary variables and pass them to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN p and r variables via uniform distribution sampling between lower and upper bounds\n",
    "                p = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                r = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = self.p[p_idx]\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound[:] = self.upper_bound[1]\n",
    "                r_boundary = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                \n",
    "                # Pass variables through the model via train_step and get losses\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\n",
    "                sum_loss[step] = losses[0]\n",
    "                pinn_loss[step] = losses[1]\n",
    "                boundary_loss[step] = losses[2]\n",
    "            \n",
    "            # Calculate and print total losses for the epoch\n",
    "            total_loss[epoch] = np.sum(sum_loss)\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Training loss for epoch {epoch}: pinn: {total_pinn_loss[epoch]:.4f}, boundary: {total_boundary_loss[epoch]:.4f}, total: {total_loss[epoch]:.4f}')\n",
    "            \n",
    "            # Get prediction variable loss by the predict function (below)\n",
    "            total_predictions[:, :, epoch] = self.predict_epoch(P_predict, size)\n",
    "        \n",
    "        # Return epoch losses\n",
    "        return total_loss, total_pinn_loss, total_boundary_loss, total_predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict_epoch(self, P, size, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(P.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((size**2, 1))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Pass prediction data through the model\n",
    "            preds[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        # Return f\n",
    "        return preds\n",
    "    \n",
    "    def predict(self, p, r):\n",
    "        P = tf.concat((p, r), axis=1)\n",
    "        f = self.tf_call(P)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r): \n",
    "        # Note: p and r are taken out of logspace for the PINN calculation\n",
    "        p = tf.math.exp(p)\n",
    "        r = tf.math.exp(r)\n",
    "        R = p # GeV * km/s / coulomb Note: R = p * c/q but skipping c/q here \n",
    "        V = 400 # 400 km/s\n",
    "        M = 0.938 # GeV\n",
    "        k_0 = 1 # km^2/s\n",
    "        \n",
    "        # Calculate k and l_f\n",
    "        k = tf.math.divide(p, tf.math.sqrt(tf.math.square(p) + tf.math.square(M))) * k_0 * r * p\n",
    "        l_f = tf.math.reduce_mean(tf.math.abs(f_r + (tf.math.divide(R * V, 3 * k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 0: pinn: 1450.4644, boundary: 74.8113, total: 2198.5778\n",
      "Training loss for epoch 1: pinn: 310.2296, boundary: 34.1790, total: 652.0198\n",
      "Training loss for epoch 2: pinn: 165.1668, boundary: 31.6900, total: 482.0669\n",
      "Training loss for epoch 3: pinn: 93.0511, boundary: 31.3385, total: 406.4363\n",
      "Training loss for epoch 4: pinn: 38.6751, boundary: 31.5008, total: 353.6829\n",
      "Training loss for epoch 5: pinn: 16.1343, boundary: 31.6384, total: 332.5181\n",
      "Training loss for epoch 6: pinn: 12.5911, boundary: 31.5628, total: 328.2194\n",
      "Training loss for epoch 7: pinn: 22.8968, boundary: 31.4215, total: 337.1119\n",
      "Training loss for epoch 8: pinn: 33.2772, boundary: 31.2833, total: 346.1104\n",
      "Training loss for epoch 9: pinn: 20.1344, boundary: 31.1968, total: 332.1022\n",
      "Training loss for epoch 10: pinn: 6.6858, boundary: 31.0805, total: 317.4904\n",
      "Training loss for epoch 11: pinn: 6.8770, boundary: 30.8791, total: 315.6684\n",
      "Training loss for epoch 12: pinn: 8.2141, boundary: 30.6707, total: 314.9208\n",
      "Training loss for epoch 13: pinn: 7.8084, boundary: 30.3443, total: 311.2518\n",
      "Training loss for epoch 14: pinn: 9.1071, boundary: 30.0016, total: 309.1234\n",
      "Training loss for epoch 15: pinn: 9.3939, boundary: 29.5479, total: 304.8728\n",
      "Training loss for epoch 16: pinn: 10.3513, boundary: 28.9189, total: 299.5404\n",
      "Training loss for epoch 17: pinn: 10.8147, boundary: 28.1669, total: 292.4832\n",
      "Training loss for epoch 18: pinn: 10.9862, boundary: 27.1076, total: 282.0621\n",
      "Training loss for epoch 19: pinn: 13.4013, boundary: 25.8499, total: 271.9002\n",
      "Training loss for epoch 20: pinn: 19.1734, boundary: 24.2446, total: 261.6194\n",
      "Training loss for epoch 21: pinn: 21.8000, boundary: 22.2079, total: 243.8787\n",
      "Training loss for epoch 22: pinn: 26.6600, boundary: 21.3942, total: 240.6020\n",
      "Training loss for epoch 23: pinn: 34.8160, boundary: 20.3692, total: 238.5083\n",
      "Training loss for epoch 24: pinn: 32.9884, boundary: 17.5574, total: 208.5625\n",
      "Training loss for epoch 25: pinn: 35.4785, boundary: 17.6442, total: 211.9206\n",
      "Training loss for epoch 26: pinn: 53.3199, boundary: 19.0414, total: 243.7342\n",
      "Training loss for epoch 27: pinn: 55.9701, boundary: 21.5717, total: 271.6876\n",
      "Training loss for epoch 28: pinn: 52.4576, boundary: 16.1081, total: 213.5386\n",
      "Training loss for epoch 29: pinn: 39.4534, boundary: 17.2728, total: 212.1816\n",
      "Training loss for epoch 30: pinn: 42.7902, boundary: 22.6310, total: 269.0999\n",
      "Training loss for epoch 31: pinn: 41.9402, boundary: 20.6665, total: 248.6055\n",
      "Training loss for epoch 32: pinn: 35.0766, boundary: 12.6228, total: 161.3041\n",
      "Training loss for epoch 33: pinn: 30.0549, boundary: 14.8415, total: 178.4697\n",
      "Training loss for epoch 34: pinn: 32.7679, boundary: 14.3644, total: 176.4123\n",
      "Training loss for epoch 35: pinn: 29.7789, boundary: 14.4710, total: 174.4889\n",
      "Training loss for epoch 36: pinn: 32.2010, boundary: 13.6475, total: 168.6759\n",
      "Training loss for epoch 37: pinn: 26.0213, boundary: 9.9674, total: 125.6952\n",
      "Training loss for epoch 38: pinn: 27.1944, boundary: 11.8823, total: 146.0177\n",
      "Training loss for epoch 39: pinn: 32.7340, boundary: 11.4775, total: 147.5092\n",
      "Training loss for epoch 40: pinn: 29.5781, boundary: 11.5604, total: 145.1822\n",
      "Training loss for epoch 41: pinn: 42.1353, boundary: 15.0524, total: 192.6598\n",
      "Training loss for epoch 42: pinn: 38.9584, boundary: 15.4091, total: 193.0494\n",
      "Training loss for epoch 43: pinn: 37.1187, boundary: 16.0770, total: 197.8885\n",
      "Training loss for epoch 44: pinn: 37.6119, boundary: 14.3198, total: 180.8096\n",
      "Training loss for epoch 45: pinn: 31.6740, boundary: 8.1465, total: 113.1386\n",
      "Training loss for epoch 46: pinn: 26.1767, boundary: 8.9254, total: 115.4302\n",
      "Training loss for epoch 47: pinn: 30.2916, boundary: 10.7079, total: 137.3704\n",
      "Training loss for epoch 48: pinn: 30.6735, boundary: 10.8207, total: 138.8807\n",
      "Training loss for epoch 49: pinn: 28.5718, boundary: 8.8900, total: 117.4718\n",
      "Training loss for epoch 50: pinn: 26.3831, boundary: 8.3163, total: 109.5458\n",
      "Training loss for epoch 51: pinn: 25.4047, boundary: 8.9357, total: 114.7614\n",
      "Training loss for epoch 52: pinn: 27.0085, boundary: 11.8520, total: 145.5282\n",
      "Training loss for epoch 53: pinn: 33.1099, boundary: 10.9737, total: 142.8466\n",
      "Training loss for epoch 54: pinn: 33.5010, boundary: 11.1508, total: 145.0093\n",
      "Training loss for epoch 55: pinn: 33.2649, boundary: 12.8183, total: 161.4476\n",
      "Training loss for epoch 56: pinn: 30.6445, boundary: 8.0882, total: 111.5261\n",
      "Training loss for epoch 57: pinn: 26.3961, boundary: 8.6920, total: 113.3162\n",
      "Training loss for epoch 58: pinn: 28.2794, boundary: 7.9680, total: 107.9599\n",
      "Training loss for epoch 59: pinn: 27.8094, boundary: 10.8021, total: 135.8302\n",
      "Training loss for epoch 60: pinn: 32.6415, boundary: 7.8209, total: 110.8509\n",
      "Training loss for epoch 61: pinn: 23.6774, boundary: 6.7635, total: 91.3122\n",
      "Training loss for epoch 62: pinn: 23.1357, boundary: 6.3877, total: 87.0122\n",
      "Training loss for epoch 63: pinn: 23.3267, boundary: 6.8851, total: 92.1781\n",
      "Training loss for epoch 64: pinn: 22.2614, boundary: 6.6631, total: 88.8924\n",
      "Training loss for epoch 65: pinn: 27.5087, boundary: 8.7637, total: 115.1458\n",
      "Training loss for epoch 66: pinn: 26.7554, boundary: 7.0019, total: 96.7746\n",
      "Training loss for epoch 67: pinn: 23.1872, boundary: 6.8486, total: 91.6736\n",
      "Training loss for epoch 68: pinn: 26.7634, boundary: 9.2836, total: 119.5990\n",
      "Training loss for epoch 69: pinn: 25.6933, boundary: 9.3382, total: 119.0750\n",
      "Training loss for epoch 70: pinn: 27.8835, boundary: 7.3656, total: 101.5394\n",
      "Training loss for epoch 71: pinn: 24.3424, boundary: 6.3355, total: 87.6969\n",
      "Training loss for epoch 72: pinn: 24.7135, boundary: 9.4139, total: 118.8525\n",
      "Training loss for epoch 73: pinn: 33.0841, boundary: 12.1204, total: 154.2878\n",
      "Training loss for epoch 74: pinn: 28.6816, boundary: 6.7674, total: 96.3553\n",
      "Training loss for epoch 75: pinn: 27.5868, boundary: 12.2349, total: 149.9355\n",
      "Training loss for epoch 76: pinn: 31.5948, boundary: 8.8194, total: 119.7890\n",
      "Training loss for epoch 77: pinn: 31.8195, boundary: 7.9084, total: 110.9031\n",
      "Training loss for epoch 78: pinn: 28.3593, boundary: 11.1314, total: 139.6737\n",
      "Training loss for epoch 79: pinn: 31.4783, boundary: 8.3874, total: 115.3523\n",
      "Training loss for epoch 80: pinn: 27.4461, boundary: 6.5380, total: 92.8260\n",
      "Training loss for epoch 81: pinn: 25.4309, boundary: 6.5253, total: 90.6844\n",
      "Training loss for epoch 82: pinn: 24.7802, boundary: 4.8710, total: 73.4903\n",
      "Training loss for epoch 83: pinn: 24.0554, boundary: 7.0911, total: 94.9668\n",
      "Training loss for epoch 84: pinn: 24.1110, boundary: 6.6705, total: 90.8164\n",
      "Training loss for epoch 85: pinn: 23.0827, boundary: 7.0991, total: 94.0741\n",
      "Training loss for epoch 86: pinn: 24.9579, boundary: 4.8356, total: 73.3143\n",
      "Training loss for epoch 87: pinn: 24.4452, boundary: 4.8903, total: 73.3481\n",
      "Training loss for epoch 88: pinn: 23.9083, boundary: 8.3055, total: 106.9630\n",
      "Training loss for epoch 89: pinn: 25.6866, boundary: 10.0435, total: 126.1217\n",
      "Training loss for epoch 90: pinn: 28.6797, boundary: 10.0171, total: 128.8504\n",
      "Training loss for epoch 91: pinn: 27.7771, boundary: 8.9482, total: 117.2595\n",
      "Training loss for epoch 92: pinn: 28.5811, boundary: 9.3026, total: 121.6073\n",
      "Training loss for epoch 93: pinn: 27.8890, boundary: 8.6519, total: 114.4081\n",
      "Training loss for epoch 94: pinn: 25.9552, boundary: 8.0594, total: 106.5495\n",
      "Training loss for epoch 95: pinn: 24.8042, boundary: 5.2081, total: 76.8849\n",
      "Training loss for epoch 96: pinn: 22.6519, boundary: 6.3318, total: 85.9702\n",
      "Training loss for epoch 97: pinn: 23.4139, boundary: 6.0435, total: 83.8492\n",
      "Training loss for epoch 98: pinn: 22.8702, boundary: 7.8914, total: 101.7844\n",
      "Training loss for epoch 99: pinn: 23.3506, boundary: 7.1346, total: 94.6966\n",
      "Training loss for epoch 100: pinn: 24.6879, boundary: 8.9971, total: 114.6588\n",
      "Training loss for epoch 101: pinn: 26.4981, boundary: 5.7572, total: 84.0702\n",
      "Training loss for epoch 102: pinn: 26.2061, boundary: 5.8103, total: 84.3091\n",
      "Training loss for epoch 103: pinn: 22.7996, boundary: 6.5743, total: 88.5424\n",
      "Training loss for epoch 104: pinn: 22.6780, boundary: 5.2766, total: 75.4445\n",
      "Training loss for epoch 105: pinn: 23.6351, boundary: 4.7053, total: 70.6884\n",
      "Training loss for epoch 106: pinn: 23.0050, boundary: 4.3823, total: 66.8275\n",
      "Training loss for epoch 107: pinn: 21.1551, boundary: 4.0936, total: 62.0906\n",
      "Training loss for epoch 108: pinn: 22.3786, boundary: 9.4009, total: 116.3880\n",
      "Training loss for epoch 109: pinn: 27.2206, boundary: 6.5387, total: 92.6077\n",
      "Training loss for epoch 110: pinn: 24.1512, boundary: 10.1666, total: 125.8168\n",
      "Training loss for epoch 111: pinn: 27.2484, boundary: 6.4383, total: 91.6310\n",
      "Training loss for epoch 112: pinn: 23.7364, boundary: 6.0607, total: 84.3434\n",
      "Training loss for epoch 113: pinn: 23.5680, boundary: 10.1635, total: 125.2031\n",
      "Training loss for epoch 114: pinn: 28.4762, boundary: 6.7109, total: 95.5856\n",
      "Training loss for epoch 115: pinn: 25.1459, boundary: 5.3045, total: 78.1905\n",
      "Training loss for epoch 116: pinn: 23.6960, boundary: 5.3747, total: 77.4433\n",
      "Training loss for epoch 117: pinn: 23.9773, boundary: 5.7000, total: 80.9772\n",
      "Training loss for epoch 118: pinn: 24.3455, boundary: 5.5191, total: 79.5367\n",
      "Training loss for epoch 119: pinn: 23.3106, boundary: 5.6026, total: 79.3370\n",
      "Training loss for epoch 120: pinn: 22.2136, boundary: 5.5878, total: 78.0913\n",
      "Training loss for epoch 121: pinn: 22.8463, boundary: 9.8491, total: 121.3375\n",
      "Training loss for epoch 122: pinn: 25.0824, boundary: 13.4075, total: 159.1575\n",
      "Training loss for epoch 123: pinn: 26.1813, boundary: 9.0376, total: 116.5576\n",
      "Training loss for epoch 124: pinn: 29.1844, boundary: 9.1241, total: 120.4254\n",
      "Training loss for epoch 125: pinn: 26.3890, boundary: 7.1834, total: 98.2233\n",
      "Training loss for epoch 126: pinn: 24.1255, boundary: 7.4308, total: 98.4330\n",
      "Training loss for epoch 127: pinn: 23.9612, boundary: 8.5077, total: 109.0380\n",
      "Training loss for epoch 128: pinn: 27.4201, boundary: 9.2468, total: 119.8882\n",
      "Training loss for epoch 129: pinn: 25.9579, boundary: 6.7116, total: 93.0734\n",
      "Training loss for epoch 130: pinn: 24.0119, boundary: 8.9010, total: 113.0222\n",
      "Training loss for epoch 131: pinn: 26.0119, boundary: 7.3974, total: 99.9857\n",
      "Training loss for epoch 132: pinn: 24.6543, boundary: 3.7153, total: 61.8076\n",
      "Training loss for epoch 133: pinn: 24.4560, boundary: 4.2826, total: 67.2822\n",
      "Training loss for epoch 134: pinn: 23.7371, boundary: 3.5040, total: 58.7771\n",
      "Training loss for epoch 135: pinn: 23.6625, boundary: 7.5122, total: 98.7845\n",
      "Training loss for epoch 136: pinn: 32.0270, boundary: 6.4784, total: 96.8106\n",
      "Training loss for epoch 137: pinn: 28.1765, boundary: 6.3934, total: 92.1106\n",
      "Training loss for epoch 138: pinn: 29.6884, boundary: 7.8026, total: 107.7147\n",
      "Training loss for epoch 139: pinn: 25.6889, boundary: 5.0857, total: 76.5454\n",
      "Training loss for epoch 140: pinn: 22.8266, boundary: 5.2863, total: 75.6897\n",
      "Training loss for epoch 141: pinn: 23.6406, boundary: 7.9797, total: 103.4371\n",
      "Training loss for epoch 142: pinn: 23.9817, boundary: 5.4405, total: 78.3864\n",
      "Training loss for epoch 143: pinn: 21.1958, boundary: 7.1820, total: 93.0159\n",
      "Training loss for epoch 144: pinn: 22.2933, boundary: 7.3797, total: 96.0904\n",
      "Training loss for epoch 145: pinn: 26.2537, boundary: 8.0775, total: 107.0285\n",
      "Training loss for epoch 146: pinn: 23.0837, boundary: 7.0400, total: 93.4837\n",
      "Training loss for epoch 147: pinn: 23.2985, boundary: 6.0636, total: 83.9348\n",
      "Training loss for epoch 148: pinn: 23.3264, boundary: 6.9270, total: 92.5967\n",
      "Training loss for epoch 149: pinn: 22.6136, boundary: 4.9914, total: 72.5277\n",
      "Training loss for epoch 150: pinn: 20.2405, boundary: 4.8150, total: 68.3906\n",
      "Training loss for epoch 151: pinn: 20.3858, boundary: 5.5122, total: 75.5074\n",
      "Training loss for epoch 152: pinn: 21.8061, boundary: 4.7106, total: 68.9123\n",
      "Training loss for epoch 153: pinn: 22.2847, boundary: 4.5150, total: 67.4343\n",
      "Training loss for epoch 154: pinn: 21.8670, boundary: 5.5382, total: 77.2493\n",
      "Training loss for epoch 155: pinn: 22.2793, boundary: 4.8224, total: 70.5037\n",
      "Training loss for epoch 156: pinn: 22.9050, boundary: 5.8123, total: 81.0284\n",
      "Training loss for epoch 157: pinn: 24.6902, boundary: 9.4144, total: 118.8341\n",
      "Training loss for epoch 158: pinn: 26.2713, boundary: 9.1026, total: 117.2971\n",
      "Training loss for epoch 159: pinn: 26.2481, boundary: 6.8365, total: 94.6132\n",
      "Training loss for epoch 160: pinn: 23.5304, boundary: 4.8081, total: 71.6112\n",
      "Training loss for epoch 161: pinn: 21.6228, boundary: 4.7656, total: 69.2787\n",
      "Training loss for epoch 162: pinn: 22.6025, boundary: 6.6916, total: 89.5187\n",
      "Training loss for epoch 163: pinn: 23.6085, boundary: 5.5662, total: 79.2707\n",
      "Training loss for epoch 164: pinn: 24.2762, boundary: 6.8371, total: 92.6474\n",
      "Training loss for epoch 165: pinn: 24.4245, boundary: 5.3798, total: 78.2228\n",
      "Training loss for epoch 166: pinn: 23.7368, boundary: 7.4554, total: 98.2906\n",
      "Training loss for epoch 167: pinn: 24.2766, boundary: 5.2294, total: 76.5703\n",
      "Training loss for epoch 168: pinn: 23.9289, boundary: 5.1034, total: 74.9629\n",
      "Training loss for epoch 169: pinn: 21.7531, boundary: 6.9950, total: 91.7032\n",
      "Training loss for epoch 170: pinn: 22.6163, boundary: 4.8492, total: 71.1084\n",
      "Training loss for epoch 171: pinn: 24.2884, boundary: 5.0131, total: 74.4196\n",
      "Training loss for epoch 172: pinn: 22.8432, boundary: 4.9521, total: 72.3637\n",
      "Training loss for epoch 173: pinn: 21.0852, boundary: 5.0022, total: 71.1073\n",
      "Training loss for epoch 174: pinn: 21.6290, boundary: 6.4834, total: 86.4630\n",
      "Training loss for epoch 175: pinn: 21.3655, boundary: 5.1308, total: 72.6738\n",
      "Training loss for epoch 176: pinn: 22.3514, boundary: 6.1231, total: 83.5823\n",
      "Training loss for epoch 177: pinn: 22.2050, boundary: 5.2919, total: 75.1244\n",
      "Training loss for epoch 178: pinn: 21.6581, boundary: 4.1659, total: 63.3174\n",
      "Training loss for epoch 179: pinn: 21.7990, boundary: 3.2839, total: 54.6383\n",
      "Training loss for epoch 180: pinn: 22.1580, boundary: 4.3543, total: 65.7014\n",
      "Training loss for epoch 181: pinn: 22.6723, boundary: 5.7798, total: 80.4703\n",
      "Training loss for epoch 182: pinn: 27.5919, boundary: 4.0212, total: 67.8036\n",
      "Training loss for epoch 183: pinn: 22.9246, boundary: 4.2030, total: 64.9549\n",
      "Training loss for epoch 184: pinn: 20.3554, boundary: 5.1282, total: 71.6370\n",
      "Training loss for epoch 185: pinn: 20.5931, boundary: 4.6817, total: 67.4100\n",
      "Training loss for epoch 186: pinn: 23.0503, boundary: 7.0658, total: 93.7086\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m pinn \u001b[38;5;241m=\u001b[39m PINN(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs, lower_bound\u001b[38;5;241m=\u001b[39mlb, upper_bound\u001b[38;5;241m=\u001b[39mub, p\u001b[38;5;241m=\u001b[39mp[:, \u001b[38;5;241m0\u001b[39m], r\u001b[38;5;241m=\u001b[39mr[:, \u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m     18\u001b[0m             f_boundary\u001b[38;5;241m=\u001b[39mf_boundary[:, \u001b[38;5;241m0\u001b[39m], size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m     19\u001b[0m pinn\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[0;32m---> 20\u001b[0m total_loss, pinn_loss, boundary_loss, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP_predict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mboundary_batchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboundary_batchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Predict f at the boundary r_HP\u001b[39;00m\n\u001b[1;32m     24\u001b[0m r_boundary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((p\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPINN.fit\u001b[0;34m(self, P_predict, size, alpha, beta, batchsize, boundary_batchsize, epochs)\u001b[0m\n\u001b[1;32m    115\u001b[0m r_boundary \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(upper_bound, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Pass variables through the model via train_step and get losses\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m sum_loss[step] \u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    120\u001b[0m pinn_loss[step] \u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;241m1\u001b[39m]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPINN.train_step\u001b[0;34m(self, p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\u001b[0m\n\u001b[1;32m     60\u001b[0m     sum_loss \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39mpinn_loss \u001b[38;5;241m+\u001b[39m beta\u001b[38;5;241m*\u001b[39mboundary_loss\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43msum_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Return losses\u001b[39;00m\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1100\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1094\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1095\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1096\u001b[0m           output_gradients))\n\u001b[1;32m   1097\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1098\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1100\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1109\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1205\u001b[0m, in \u001b[0;36m_TapeGradientFunctions._wrap_backward_function.<locals>._backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1203\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m backward_function_inputs:\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremapped_captures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define neural network. Note: 2 inputs- (p, r), 1 output- f(r, p)\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(1000, activation='tanh')(inputs)\n",
    "x_ = tf.keras.layers.Dense(1000, activation='tanh')(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "# Define hyperparameters\n",
    "alpha = 1 # pinn_loss weight\n",
    "beta = 10 # boundary_loss weight\n",
    "lr = 3e-3\n",
    "batchsize = 2048\n",
    "boundary_batchsize = 256\n",
    "epochs = 1000\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Initialize, compile, and fit the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "            f_boundary=f_boundary[:, 0], size=size)\n",
    "pinn.compile(optimizer=optimizer)\n",
    "total_loss, pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, beta=beta, batchsize=batchsize, \n",
    "                                                 boundary_batchsize=boundary_batchsize, epochs=epochs, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict f at the boundary r_HP\n",
    "r_boundary = np.zeros((p.shape[0], 1))\n",
    "r_boundary[:] = ub[1]\n",
    "f_pred_boundary = pinn.predict(p, r_boundary).numpy()\n",
    "\n",
    "# Predict f at the final epoch for all (p, r)\n",
    "f_predict = pinn.tf_call(P_star).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Save PINN outputs\n",
    "with open('./figures/total_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(total_loss, file)\n",
    "    \n",
    "with open('./figures/pinn_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/boundary_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/predictions.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "with open('./figures/f_boundary.pkl', 'wb') as file:\n",
    "    pkl.dump(f_boundary, file)\n",
    "    \n",
    "with open('./figures/p.pkl', 'wb') as file:\n",
    "    pkl.dump(p, file)\n",
    "    \n",
    "with open('./figures/f_pred_boundary.pkl', 'wb') as file:\n",
    "    pkl.dump(f_pred_boundary, file)\n",
    "    \n",
    "with open('./figures/f_predict.pkl', 'wb') as file:\n",
    "    pkl.dump(f_predict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
