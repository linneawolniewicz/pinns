{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Physics Informed Neural Networks\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook adapts the code implementation of Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs, and the code by Michael Ito, to solve the force-field equation for solar modulation of cosmic rays. Michael Ito's code adaption use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure. \n",
    "\n",
    "In this application, our PINN is $h(r, p) = \\frac{\\partial f}{\\partial r} + \\frac{RV}{3k} \\frac{\\partial f}{\\partial p}$ where $k=\\beta(p)k_1(r)k_2(r)$ and $\\beta = \\frac{p}{\\sqrt{p^2 + M^2}}$. We will approximate $f(r, p)$ using a neural network.\n",
    "\n",
    "We have no initial data, but our boundary data will be given by $f(r_{HP}, p) = \\frac{J(r_{HP}, T)}{p^2} = \\frac{(T+M)^\\gamma}{p^2}$, where $r_{HP} = 120$ AU (i.e. the radius of Heliopause), $M=0.938$ GeV, $\\gamma$ is between $-2$ and $-3$, and $T = \\sqrt{p^2 + M^2} - M$. Or, vice versa, $p = \\sqrt{T^2 + 2TM}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The data was gathered from https://github.com/maziarraissi/PINNs/tree/master/main/Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201,) (256,) (256,)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "noise = 0.0   \n",
    "M = 0.938 # GeV\n",
    "gamma = -2.5 # Between -2 and -3\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([0.0433, 1]) # (p, r) in (GeV, AU)\n",
    "ub = np.array([1000.9376, 120]) # (p, r) in (GeV, AU)\n",
    "\n",
    "# Assign number of data points\n",
    "N_b = 50 # number of boundary data points\n",
    "N_h = 20000 # number of collocation data points\n",
    "\n",
    "# Create intial r, p, and T data\n",
    "r = np.linspace(1, 120, 201) # r values\n",
    "p = np.linspace(0.0433, 1000.9376, 256) # p values\n",
    "T = np.linspace(0.001, 1000, 256) # T values\n",
    "# print(r, p, T)\n",
    "print(r.shape, p.shape, T.shape)\n",
    "\n",
    "# Create boundary f data (f at r_HP)\n",
    "f_exact = ((T + M)**gamma)/(p**2)\n",
    "print(f_exact.shape)\n",
    "\n",
    "P, R = np.meshgrid(p, r)\n",
    "# print(len(P), len(R))\n",
    "# print(P, R)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "P_star = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions.\n",
    "\n",
    "Michael advises working through the TF2 API introduction to Gradients and Autodifferentiation in tensorflow https://www.tensorflow.org/guide/autodiff and Advanced Autodifferentiation in tensorflow https://www.tensorflow.org/guide/advanced_autodiff. These are the main data structures for the PINN used in the train_step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, r, f_boundary, n_samples=20000, n_boundary=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.r = r\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.n_boundary = n_boundary\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a temporal variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained.We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights wrt. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MSE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and boundary_f.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "    Outputs: total_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                # Forward pass P (PINN data)\n",
    "                P = tf.concat((p, r), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Forward pass P_boundary (boundary condition data)\n",
    "                P_boundary = tf.concat((p_boundary, r_boundary), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "\n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tf.math.reduce_mean(tf.math.square(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            f_p = t1.gradient(f, p)\n",
    "            f_r = t1.gradient(f, r)\n",
    "\n",
    "        # Calculate resulting loss = PINN loss + boundary loss\n",
    "        pinn_loss = self.pinn_loss(f, r, p, f_p, f_r)\n",
    "        total_loss = pinn_loss + boundary_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        print(gradients)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Return losses\n",
    "        return total_loss.numpy(), pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        initial_batchsize: batchsize for (x_initial, t_initial) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "    \n",
    "    Outputs: Losses for each equation (PDE, Initial Value, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, batchsize=64, boundary_batchsize=16, epochs=20):\n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        total_predictions = np.zeros((51456, 1, epochs)) # why 51456??\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Reset loss variables\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN and boundary variables and pass them to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN p and r variables via uniform distribution sampling between lower and upper bounds\n",
    "                p = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                r = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Get p_boundary and r_boundary variables by randomly sampling along r = r_HP = 120 AU\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = self.p[p_idx]\n",
    "                r_boundary = np.zeros((boundary_batchsize, 1)) + 120\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Pass variables through the model via train_step and get losses\n",
    "                total_loss = self.train_step(p, r, p_boundary, r_boundary, f_boundary)\n",
    "                pinn_loss[step] = total_loss[1]\n",
    "                boundary_loss[step] = total_loss[2]\n",
    "            \n",
    "            # Calculate and print total losses for the epoch\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Training loss for epoch {epoch}: pinn: {total_pinn_loss[epoch]:.4f}, boundary: {total_boundary_loss[epoch]:.4f}')\n",
    "            \n",
    "            # Get prediction variable loss by the predict function (below)\n",
    "            total_predictions[:, :, epoch] = np.expand_dims(self.predict(P_predict)[1], axis=1)\n",
    "        \n",
    "        # Return epoch losses\n",
    "        return total_pinn_loss, total_boundary_loss, total_predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(P.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((P.shape[0], 2))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Get prediction data and calculate h\n",
    "            preds[start_idx: end_idx, :] = self(P[start_idx: end_idx, :]).numpy()\n",
    "            f = np.sqrt(preds[:, 0]**2 + preds[:, 1]**2)\n",
    "        \n",
    "        # Return prediction data and h\n",
    "        return preds, f\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MSE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, f, r, p, f_p, f_r): \n",
    "        R = 1 # unknown\n",
    "        V = 400 # km/s\n",
    "        M = 0.938 # GeV\n",
    "        k_0 = 1 # km^2/s\n",
    "        k = (p/tf.math.sqrt(tf.math.square(p) + tf.math.square(M)))*k_0*r*p\n",
    "        l_f = tf.math.reduce_mean(tf.math.square(f_r + ((R*V)/(3*k))*f_p))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['dense_15/kernel:0', 'dense_15/bias:0', 'dense_16/kernel:0', 'dense_16/bias:0', 'dense_17/kernel:0', 'dense_17/bias:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_15/kernel:0' shape=(2, 100) dtype=float32, numpy=\narray([[-0.18240786, -0.12136189,  0.10202342, -0.11690237,  0.14446056,\n         0.12863079,  0.22357962, -0.21971944, -0.21990113, -0.12652513,\n         0.20022923,  0.12606889,  0.2325086 ,  0.22600067, -0.11639403,\n         0.20897132, -0.17634717,  0.13730332, -0.06520849,  0.13826415,\n        -0.1309739 ,  0.06028718, -0.19411199, -0.12039003, -0.00083759,\n        -0.08642328,  0.20541418, -0.14166847,  0.01872501,  0.20606712,\n        -0.06493215,  0.09048954, -0.22403544,  0.03089866,  0.01542372,\n        -0.22834022,  0.10889909, -0.07259865, -0.06819643, -0.09579815,\n        -0.14678124,  0.23208746, -0.14895663, -0.08917245, -0.03694282,\n        -0.22013792, -0.03379941, -0.18506445, -0.07803114, -0.05392244,\n        -0.1986092 , -0.00962116,  0.12743357, -0.16170211,  0.02628365,\n         0.0855726 ,  0.19176468, -0.11215714,  0.18123165,  0.03882179,\n        -0.14437711,  0.23464036,  0.07651207, -0.01902188, -0.13711515,\n         0.11865118, -0.22223096,  0.05041993, -0.13690352, -0.10643448,\n        -0.13913001, -0.08345483,  0.11185458,  0.03727421, -0.09086262,\n         0.14238718,  0.07577983, -0.08751021,  0.01522961,  0.21507117,\n        -0.23221207,  0.03510118, -0.12265503, -0.21210337,  0.2136909 ,\n        -0.04031588, -0.12203711, -0.16274707,  0.14924112,  0.23469034,\n        -0.15027966, -0.17337975, -0.16581944,  0.02028438, -0.05393384,\n         0.05356005,  0.14159319,  0.19022411,  0.00327446, -0.19063057],\n       [-0.19818014, -0.03633104,  0.1355218 , -0.0988604 , -0.05843106,\n        -0.05829847,  0.2379745 , -0.18181792,  0.07804427,  0.05974802,\n        -0.13634071,  0.00142729, -0.1554177 , -0.15096304, -0.0383005 ,\n         0.03296256, -0.01549768, -0.05857852, -0.05895236, -0.05501661,\n        -0.0762329 , -0.22709745, -0.08174247,  0.1067442 ,  0.02198979,\n         0.12122756,  0.09236231,  0.12500176,  0.24173793, -0.07130244,\n        -0.03861865,  0.03193125,  0.17548424, -0.0363625 ,  0.01076615,\n        -0.08281176, -0.16280872, -0.1361492 ,  0.10658363, -0.08253425,\n        -0.11574292, -0.17396593,  0.12138432,  0.19463381,  0.18570185,\n         0.05762035,  0.15495306, -0.18784647,  0.12981799,  0.02870461,\n         0.21040615,  0.2328408 ,  0.149988  , -0.16402847,  0.17263728,\n        -0.18062973,  0.02093861, -0.08596519, -0.11156483, -0.201929  ,\n        -0.0006943 , -0.1035274 ,  0.2229566 , -0.14806582, -0.21653791,\n         0.1342524 , -0.02518782,  0.17467302, -0.16677499, -0.18262736,\n         0.00466144,  0.0668003 ,  0.2161052 ,  0.18739724, -0.14360845,\n         0.22889581, -0.2400817 , -0.21007395,  0.04951334, -0.07827279,\n        -0.03486454, -0.09049387, -0.16858763,  0.23205107, -0.1601442 ,\n        -0.0532278 ,  0.03511423,  0.0582462 ,  0.13616231, -0.04446609,\n         0.13540196, -0.22981533,  0.11763051, -0.019373  ,  0.04482558,\n        -0.06831653,  0.03943798,  0.01466852, -0.00456402,  0.02080491]],\n      dtype=float32)>), (None, <tf.Variable 'dense_15/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_16/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[ 0.01458852,  0.03009862,  0.08213121, ...,  0.11993459,\n        -0.14862153, -0.05568251],\n       [-0.05748112,  0.14426392,  0.02149165, ...,  0.02441345,\n        -0.00187823, -0.01149641],\n       [-0.16827545, -0.04047048, -0.05867006, ...,  0.10928407,\n         0.10429937, -0.07603721],\n       ...,\n       [-0.13487919,  0.02480936,  0.16686463, ...,  0.08285517,\n        -0.04325308,  0.00786197],\n       [ 0.08684281,  0.04430738, -0.12566963, ...,  0.06895417,\n        -0.08524632,  0.11915737],\n       [-0.14443654, -0.13706549,  0.04043852, ..., -0.13823117,\n        -0.0419887 ,  0.09253296]], dtype=float32)>), (None, <tf.Variable 'dense_16/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_17/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[ 0.04767407, -0.13440417,  0.14239284, ..., -0.05958711,\n         0.05638677,  0.06275433],\n       [ 0.08496922, -0.00571556, -0.07990374, ...,  0.01876575,\n         0.00837618, -0.1407812 ],\n       [ 0.05461115,  0.13466486,  0.0556266 , ..., -0.11159541,\n         0.03832585,  0.15228128],\n       ...,\n       [-0.15925936, -0.06102579,  0.14862186, ...,  0.05930172,\n         0.06116438,  0.06691389],\n       [ 0.02610207,  0.08327788, -0.12502179, ...,  0.15546718,\n         0.11468154,  0.13161245],\n       [-0.1173559 ,  0.13865834,  0.12973508, ...,  0.12454173,\n         0.12299007, -0.15545982]], dtype=float32)>), (None, <tf.Variable 'dense_17/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_18/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[-0.0974521 , -0.11151624, -0.06493737, ..., -0.16232471,\n        -0.08517669,  0.01054113],\n       [-0.06863289, -0.13549425, -0.05443866, ..., -0.14521842,\n        -0.11521205,  0.02580296],\n       [ 0.04989204, -0.11395779,  0.04757406, ...,  0.04287833,\n         0.15914014, -0.10883548],\n       ...,\n       [-0.17153898,  0.01464592,  0.16804117, ...,  0.07534878,\n         0.15140468,  0.15067893],\n       [ 0.14783737, -0.03440022,  0.06975558, ...,  0.14722532,\n        -0.08526973, -0.03517725],\n       [ 0.00612913,  0.01220606,  0.0181298 , ...,  0.04200236,\n        -0.10417832, -0.17194264]], dtype=float32)>), (None, <tf.Variable 'dense_18/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_19/kernel:0' shape=(100, 2) dtype=float32, numpy=\narray([[-0.01284733,  0.06872854],\n       [-0.02350725, -0.06014633],\n       [-0.10827859, -0.008564  ],\n       [-0.2115527 , -0.2312235 ],\n       [-0.23525482,  0.23713574],\n       [ 0.15200746, -0.05958791],\n       [-0.02140415,  0.24143088],\n       [ 0.09832361, -0.063449  ],\n       [-0.07986112,  0.19104564],\n       [ 0.2389296 , -0.12580425],\n       [-0.2389185 ,  0.23192513],\n       [-0.06927712, -0.08337474],\n       [ 0.11592939,  0.00731845],\n       [-0.13990712, -0.14971712],\n       [-0.08225942,  0.11926198],\n       [ 0.2099841 ,  0.22732222],\n       [ 0.05803168, -0.04866858],\n       [ 0.02926847, -0.20371492],\n       [-0.00106572,  0.16663799],\n       [-0.17968239, -0.00087345],\n       [-0.047803  , -0.10260352],\n       [ 0.06157246,  0.22380748],\n       [-0.21343699, -0.03971918],\n       [-0.23312506, -0.02472001],\n       [ 0.22444871,  0.18935144],\n       [ 0.10565802,  0.00053205],\n       [-0.20716962,  0.09781528],\n       [-0.14497109, -0.16938266],\n       [ 0.23132485, -0.00796157],\n       [ 0.20563748,  0.23128062],\n       [ 0.13364995,  0.15346992],\n       [ 0.0806624 , -0.05589543],\n       [ 0.10150483, -0.17740062],\n       [-0.05539368, -0.10250533],\n       [ 0.20588377, -0.12650329],\n       [ 0.11214024, -0.05853896],\n       [ 0.05327955,  0.24078861],\n       [-0.05784222,  0.2002334 ],\n       [ 0.18708271,  0.2376689 ],\n       [-0.06636477, -0.0735283 ],\n       [-0.04323581, -0.20108643],\n       [ 0.13376772, -0.09543057],\n       [ 0.10866082,  0.0823673 ],\n       [-0.12830782,  0.0703381 ],\n       [ 0.01490295,  0.12604463],\n       [ 0.03873071,  0.14190406],\n       [-0.0569649 ,  0.00545967],\n       [-0.1999012 ,  0.2364111 ],\n       [ 0.06373084,  0.21572402],\n       [ 0.04182985, -0.01754931],\n       [-0.01257473, -0.20702581],\n       [-0.12694848, -0.19737916],\n       [-0.11627924, -0.13125366],\n       [-0.16005936, -0.07873984],\n       [ 0.08641535, -0.20089653],\n       [ 0.0109641 ,  0.04573271],\n       [ 0.16538483, -0.23235518],\n       [-0.08671911, -0.12152768],\n       [ 0.12433532,  0.07000238],\n       [ 0.12315613,  0.16278142],\n       [ 0.01010123,  0.00612962],\n       [-0.06647903, -0.19441111],\n       [-0.16921538, -0.01031332],\n       [-0.06291667,  0.11513194],\n       [ 0.16262916, -0.03924079],\n       [ 0.1433805 , -0.08618568],\n       [-0.06499866, -0.20042439],\n       [ 0.06609535, -0.05583304],\n       [ 0.03776631,  0.04200605],\n       [-0.0552792 ,  0.142609  ],\n       [ 0.06582087, -0.1986975 ],\n       [ 0.17633045, -0.21100342],\n       [-0.0367095 , -0.21450739],\n       [-0.01687084, -0.00473449],\n       [ 0.14940718,  0.23252687],\n       [-0.1462403 ,  0.04078656],\n       [ 0.10433012, -0.14835674],\n       [-0.23703998,  0.08951008],\n       [ 0.216656  , -0.13681382],\n       [ 0.14031515, -0.15691039],\n       [ 0.02246848,  0.17735752],\n       [ 0.04455051, -0.08906038],\n       [ 0.08704531,  0.05980632],\n       [ 0.13024941,  0.09846255],\n       [-0.19147003,  0.1072017 ],\n       [-0.03931376, -0.01768468],\n       [ 0.07736695, -0.09180962],\n       [-0.07602125,  0.153029  ],\n       [ 0.10845205,  0.07303563],\n       [-0.08308579, -0.10829628],\n       [-0.02454075, -0.01023242],\n       [-0.13190223, -0.20956439],\n       [-0.04929309, -0.18650661],\n       [ 0.11686784, -0.11632361],\n       [-0.03330934,  0.23671177],\n       [ 0.18725592, -0.16825086],\n       [ 0.16164508, -0.07411951],\n       [ 0.05920741,  0.07127729],\n       [-0.14765069, -0.11798248],\n       [-0.17135194, -0.07588403]], dtype=float32)>), (None, <tf.Variable 'dense_19/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Compile the PINN and get loss and prediction outputs\u001b[39;00m\n\u001b[1;32m     17\u001b[0m pinn\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m pinn_loss, boundary_loss, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP_predict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mboundary_batchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mPINN.fit\u001b[0;34m(self, P_predict, batchsize, boundary_batchsize, epochs)\u001b[0m\n\u001b[1;32m    109\u001b[0m f_boundary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_boundary[p_idx]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Pass variables through the model via train_step and get losses\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_boundary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m pinn_loss[step] \u001b[38;5;241m=\u001b[39m total_loss[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    114\u001b[0m boundary_loss[step] \u001b[38;5;241m=\u001b[39m total_loss[\u001b[38;5;241m2\u001b[39m]\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mPINN.train_step\u001b[0;34m(self, p, r, p_boundary, r_boundary, f_boundary)\u001b[0m\n\u001b[1;32m     62\u001b[0m gradients \u001b[38;5;241m=\u001b[39m t2\u001b[38;5;241m.\u001b[39mgradient(total_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(gradients)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Return losses\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\u001b[38;5;241m.\u001b[39mnumpy(), pinn_loss\u001b[38;5;241m.\u001b[39mnumpy(), boundary_loss\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:640\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    600\u001b[0m                     grads_and_vars,\n\u001b[1;32m    601\u001b[0m                     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    602\u001b[0m                     experimental_aggregate_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m   \u001b[38;5;124;03m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m  This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m   var_list \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m (_, v) \u001b[38;5;129;01min\u001b[39;00m grads_and_vars]\n\u001b[1;32m    643\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name):\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/sadow_lts/personal/linneamw/anaconda3/envs/pinns/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/utils.py:73\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[1;32m     72\u001b[0m   variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 73\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     76\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     77\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m when minimizing the loss. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using `model.compile()`, did you forget to provide a `loss`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument?\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     80\u001b[0m       ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vars_with_empty_grads]))\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['dense_15/kernel:0', 'dense_15/bias:0', 'dense_16/kernel:0', 'dense_16/bias:0', 'dense_17/kernel:0', 'dense_17/bias:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_15/kernel:0' shape=(2, 100) dtype=float32, numpy=\narray([[-0.18240786, -0.12136189,  0.10202342, -0.11690237,  0.14446056,\n         0.12863079,  0.22357962, -0.21971944, -0.21990113, -0.12652513,\n         0.20022923,  0.12606889,  0.2325086 ,  0.22600067, -0.11639403,\n         0.20897132, -0.17634717,  0.13730332, -0.06520849,  0.13826415,\n        -0.1309739 ,  0.06028718, -0.19411199, -0.12039003, -0.00083759,\n        -0.08642328,  0.20541418, -0.14166847,  0.01872501,  0.20606712,\n        -0.06493215,  0.09048954, -0.22403544,  0.03089866,  0.01542372,\n        -0.22834022,  0.10889909, -0.07259865, -0.06819643, -0.09579815,\n        -0.14678124,  0.23208746, -0.14895663, -0.08917245, -0.03694282,\n        -0.22013792, -0.03379941, -0.18506445, -0.07803114, -0.05392244,\n        -0.1986092 , -0.00962116,  0.12743357, -0.16170211,  0.02628365,\n         0.0855726 ,  0.19176468, -0.11215714,  0.18123165,  0.03882179,\n        -0.14437711,  0.23464036,  0.07651207, -0.01902188, -0.13711515,\n         0.11865118, -0.22223096,  0.05041993, -0.13690352, -0.10643448,\n        -0.13913001, -0.08345483,  0.11185458,  0.03727421, -0.09086262,\n         0.14238718,  0.07577983, -0.08751021,  0.01522961,  0.21507117,\n        -0.23221207,  0.03510118, -0.12265503, -0.21210337,  0.2136909 ,\n        -0.04031588, -0.12203711, -0.16274707,  0.14924112,  0.23469034,\n        -0.15027966, -0.17337975, -0.16581944,  0.02028438, -0.05393384,\n         0.05356005,  0.14159319,  0.19022411,  0.00327446, -0.19063057],\n       [-0.19818014, -0.03633104,  0.1355218 , -0.0988604 , -0.05843106,\n        -0.05829847,  0.2379745 , -0.18181792,  0.07804427,  0.05974802,\n        -0.13634071,  0.00142729, -0.1554177 , -0.15096304, -0.0383005 ,\n         0.03296256, -0.01549768, -0.05857852, -0.05895236, -0.05501661,\n        -0.0762329 , -0.22709745, -0.08174247,  0.1067442 ,  0.02198979,\n         0.12122756,  0.09236231,  0.12500176,  0.24173793, -0.07130244,\n        -0.03861865,  0.03193125,  0.17548424, -0.0363625 ,  0.01076615,\n        -0.08281176, -0.16280872, -0.1361492 ,  0.10658363, -0.08253425,\n        -0.11574292, -0.17396593,  0.12138432,  0.19463381,  0.18570185,\n         0.05762035,  0.15495306, -0.18784647,  0.12981799,  0.02870461,\n         0.21040615,  0.2328408 ,  0.149988  , -0.16402847,  0.17263728,\n        -0.18062973,  0.02093861, -0.08596519, -0.11156483, -0.201929  ,\n        -0.0006943 , -0.1035274 ,  0.2229566 , -0.14806582, -0.21653791,\n         0.1342524 , -0.02518782,  0.17467302, -0.16677499, -0.18262736,\n         0.00466144,  0.0668003 ,  0.2161052 ,  0.18739724, -0.14360845,\n         0.22889581, -0.2400817 , -0.21007395,  0.04951334, -0.07827279,\n        -0.03486454, -0.09049387, -0.16858763,  0.23205107, -0.1601442 ,\n        -0.0532278 ,  0.03511423,  0.0582462 ,  0.13616231, -0.04446609,\n         0.13540196, -0.22981533,  0.11763051, -0.019373  ,  0.04482558,\n        -0.06831653,  0.03943798,  0.01466852, -0.00456402,  0.02080491]],\n      dtype=float32)>), (None, <tf.Variable 'dense_15/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_16/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[ 0.01458852,  0.03009862,  0.08213121, ...,  0.11993459,\n        -0.14862153, -0.05568251],\n       [-0.05748112,  0.14426392,  0.02149165, ...,  0.02441345,\n        -0.00187823, -0.01149641],\n       [-0.16827545, -0.04047048, -0.05867006, ...,  0.10928407,\n         0.10429937, -0.07603721],\n       ...,\n       [-0.13487919,  0.02480936,  0.16686463, ...,  0.08285517,\n        -0.04325308,  0.00786197],\n       [ 0.08684281,  0.04430738, -0.12566963, ...,  0.06895417,\n        -0.08524632,  0.11915737],\n       [-0.14443654, -0.13706549,  0.04043852, ..., -0.13823117,\n        -0.0419887 ,  0.09253296]], dtype=float32)>), (None, <tf.Variable 'dense_16/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_17/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[ 0.04767407, -0.13440417,  0.14239284, ..., -0.05958711,\n         0.05638677,  0.06275433],\n       [ 0.08496922, -0.00571556, -0.07990374, ...,  0.01876575,\n         0.00837618, -0.1407812 ],\n       [ 0.05461115,  0.13466486,  0.0556266 , ..., -0.11159541,\n         0.03832585,  0.15228128],\n       ...,\n       [-0.15925936, -0.06102579,  0.14862186, ...,  0.05930172,\n         0.06116438,  0.06691389],\n       [ 0.02610207,  0.08327788, -0.12502179, ...,  0.15546718,\n         0.11468154,  0.13161245],\n       [-0.1173559 ,  0.13865834,  0.12973508, ...,  0.12454173,\n         0.12299007, -0.15545982]], dtype=float32)>), (None, <tf.Variable 'dense_17/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_18/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[-0.0974521 , -0.11151624, -0.06493737, ..., -0.16232471,\n        -0.08517669,  0.01054113],\n       [-0.06863289, -0.13549425, -0.05443866, ..., -0.14521842,\n        -0.11521205,  0.02580296],\n       [ 0.04989204, -0.11395779,  0.04757406, ...,  0.04287833,\n         0.15914014, -0.10883548],\n       ...,\n       [-0.17153898,  0.01464592,  0.16804117, ...,  0.07534878,\n         0.15140468,  0.15067893],\n       [ 0.14783737, -0.03440022,  0.06975558, ...,  0.14722532,\n        -0.08526973, -0.03517725],\n       [ 0.00612913,  0.01220606,  0.0181298 , ...,  0.04200236,\n        -0.10417832, -0.17194264]], dtype=float32)>), (None, <tf.Variable 'dense_18/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_19/kernel:0' shape=(100, 2) dtype=float32, numpy=\narray([[-0.01284733,  0.06872854],\n       [-0.02350725, -0.06014633],\n       [-0.10827859, -0.008564  ],\n       [-0.2115527 , -0.2312235 ],\n       [-0.23525482,  0.23713574],\n       [ 0.15200746, -0.05958791],\n       [-0.02140415,  0.24143088],\n       [ 0.09832361, -0.063449  ],\n       [-0.07986112,  0.19104564],\n       [ 0.2389296 , -0.12580425],\n       [-0.2389185 ,  0.23192513],\n       [-0.06927712, -0.08337474],\n       [ 0.11592939,  0.00731845],\n       [-0.13990712, -0.14971712],\n       [-0.08225942,  0.11926198],\n       [ 0.2099841 ,  0.22732222],\n       [ 0.05803168, -0.04866858],\n       [ 0.02926847, -0.20371492],\n       [-0.00106572,  0.16663799],\n       [-0.17968239, -0.00087345],\n       [-0.047803  , -0.10260352],\n       [ 0.06157246,  0.22380748],\n       [-0.21343699, -0.03971918],\n       [-0.23312506, -0.02472001],\n       [ 0.22444871,  0.18935144],\n       [ 0.10565802,  0.00053205],\n       [-0.20716962,  0.09781528],\n       [-0.14497109, -0.16938266],\n       [ 0.23132485, -0.00796157],\n       [ 0.20563748,  0.23128062],\n       [ 0.13364995,  0.15346992],\n       [ 0.0806624 , -0.05589543],\n       [ 0.10150483, -0.17740062],\n       [-0.05539368, -0.10250533],\n       [ 0.20588377, -0.12650329],\n       [ 0.11214024, -0.05853896],\n       [ 0.05327955,  0.24078861],\n       [-0.05784222,  0.2002334 ],\n       [ 0.18708271,  0.2376689 ],\n       [-0.06636477, -0.0735283 ],\n       [-0.04323581, -0.20108643],\n       [ 0.13376772, -0.09543057],\n       [ 0.10866082,  0.0823673 ],\n       [-0.12830782,  0.0703381 ],\n       [ 0.01490295,  0.12604463],\n       [ 0.03873071,  0.14190406],\n       [-0.0569649 ,  0.00545967],\n       [-0.1999012 ,  0.2364111 ],\n       [ 0.06373084,  0.21572402],\n       [ 0.04182985, -0.01754931],\n       [-0.01257473, -0.20702581],\n       [-0.12694848, -0.19737916],\n       [-0.11627924, -0.13125366],\n       [-0.16005936, -0.07873984],\n       [ 0.08641535, -0.20089653],\n       [ 0.0109641 ,  0.04573271],\n       [ 0.16538483, -0.23235518],\n       [-0.08671911, -0.12152768],\n       [ 0.12433532,  0.07000238],\n       [ 0.12315613,  0.16278142],\n       [ 0.01010123,  0.00612962],\n       [-0.06647903, -0.19441111],\n       [-0.16921538, -0.01031332],\n       [-0.06291667,  0.11513194],\n       [ 0.16262916, -0.03924079],\n       [ 0.1433805 , -0.08618568],\n       [-0.06499866, -0.20042439],\n       [ 0.06609535, -0.05583304],\n       [ 0.03776631,  0.04200605],\n       [-0.0552792 ,  0.142609  ],\n       [ 0.06582087, -0.1986975 ],\n       [ 0.17633045, -0.21100342],\n       [-0.0367095 , -0.21450739],\n       [-0.01687084, -0.00473449],\n       [ 0.14940718,  0.23252687],\n       [-0.1462403 ,  0.04078656],\n       [ 0.10433012, -0.14835674],\n       [-0.23703998,  0.08951008],\n       [ 0.216656  , -0.13681382],\n       [ 0.14031515, -0.15691039],\n       [ 0.02246848,  0.17735752],\n       [ 0.04455051, -0.08906038],\n       [ 0.08704531,  0.05980632],\n       [ 0.13024941,  0.09846255],\n       [-0.19147003,  0.1072017 ],\n       [-0.03931376, -0.01768468],\n       [ 0.07736695, -0.09180962],\n       [-0.07602125,  0.153029  ],\n       [ 0.10845205,  0.07303563],\n       [-0.08308579, -0.10829628],\n       [-0.02454075, -0.01023242],\n       [-0.13190223, -0.20956439],\n       [-0.04929309, -0.18650661],\n       [ 0.11686784, -0.11632361],\n       [-0.03330934,  0.23671177],\n       [ 0.18725592, -0.16825086],\n       [ 0.16164508, -0.07411951],\n       [ 0.05920741,  0.07127729],\n       [-0.14765069, -0.11798248],\n       [-0.17135194, -0.07588403]], dtype=float32)>), (None, <tf.Variable 'dense_19/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "# Define neural network as 6 layers (4 hidden), with activation functions of tanh for hidden layers and linear for the output layer\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(inputs)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='tanh')(x_)\n",
    "outputs = tf.keras.layers.Dense(2, activation='linear')(x_)\n",
    "\n",
    "# Get keras Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "# Define the PINN using the model defined above\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, \n",
    "            p=p, r=r, f_boundary=f_exact)\n",
    "\n",
    "# Compile the PINN and get loss and prediction outputs\n",
    "pinn.compile(optimizer=\"adam\")\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, batchsize=2048, \n",
    "                                               boundary_batchsize=64, epochs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Save loss data, prediction data, and h-function approximation\n",
    "with open('./figures/pinn_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/boundary_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/predictions.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "with open('./figures/true.pkl', 'wb') as file:\n",
    "    pkl.dump(h_star, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
