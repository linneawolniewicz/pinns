{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Physics Informed Neural Networks\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook adapts the code implementation of Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs, and the code by Michael Ito, to solve the force-field equation for solar modulation of cosmic rays. Michael Ito's code adaption use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure. \n",
    "\n",
    "In this application, our PINN is $h(r, p) = \\frac{\\partial f}{\\partial r} + \\frac{RV}{3k} \\frac{\\partial f}{\\partial p}$ where $k=\\beta(p)k_1(r)k_2(r)$ and $\\beta = \\frac{p}{\\sqrt{p^2 + M^2}}$. We will approximate $f(r, p)$ using a neural network.\n",
    "\n",
    "We have no initial data, but our boundary data will be given by $f(r_{HP}, p) = \\frac{J(r_{HP}, T)}{p^2} = \\frac{(T+M)^\\gamma}{p^2}$, where $r_{HP} = 120$ AU (i.e. the radius of Heliopause), $M=0.938$ GeV, $\\gamma$ is between $-2$ and $-3$, and $T = \\sqrt{p^2 + M^2} - M$. Or, vice versa, $p = \\sqrt{T^2 + 2TM}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants  \n",
    "m = 0.938 # GeV/c^2\n",
    "gamma = -3 # Between -2 and -3\n",
    "size = 512 # size of r, T, p, and f_boundary\n",
    "au = 150e6 # 150e6 m/AU\n",
    "r_limits = [119, 120]\n",
    "T_limits = [0.001, 1000]\n",
    "\n",
    "# Create data\n",
    "T = np.logspace(np.log10(T_limits[0]), np.log10(T_limits[1]), size).flatten()[:, None] # GeV\n",
    "p = (np.sqrt((T+m)**2-m**2)).flatten()[:,None] # GeV/c\n",
    "r = np.logspace(np.log10(r_limits[0]*au), np.log10(r_limits[1]*au), size).flatten()[:, None] # km\n",
    "f_boundary = ((T + m)**gamma)/(p**2) # particles/(m^3 (GeV/c)^3)\n",
    "\n",
    "# Take the log of all input data\n",
    "r = np.log(r)\n",
    "T = np.log(T)\n",
    "p = np.log(p)\n",
    "f_boundary = np.log(f_boundary)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([p[0], r[0]]) # (p, r) in (GeV, AU)\n",
    "ub = np.array([p[-1], r[-1]]) # (p, r) in (GeV, AU)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "P, R = np.meshgrid(p, r)\n",
    "P_star = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))\n",
    "\n",
    "# Check inputs\n",
    "# print(f'r: {r.shape}, p: {p.shape}, T: {T.shape}, f_boundary: {f_boundary.shape}, P_star: {P_star.shape}, lb: {lb}, ub:{ub}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, r, f_boundary, size, n_samples=20000, n_boundary=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.r = r\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.n_boundary = n_boundary\n",
    "        self.size = size\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and boundary_f.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "        alpha = weight on pinn_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                # Forward pass P (PINN data)\n",
    "                P = tf.concat((p, r), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Forward pass P_boundary (boundary condition data)\n",
    "                P_boundary = tf.concat((p_boundary, r_boundary), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "\n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tf.math.reduce_mean(tf.math.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            f_p = t1.gradient(f, p)\n",
    "            f_r = t1.gradient(f, r)\n",
    "            \n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r)\n",
    "            total_loss = (1-alpha)*pinn_loss + alpha*boundary_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        alpha: weight on pinn_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_decay: If -1, learning rate will not be decayed. Otherwise, lr = lr_decay*lr if loss hasn't \n",
    "        decreased\n",
    "        \n",
    "        alpha_decay: If -1, alpha will not be changed. Otherwise, alpha = alpha_decay*alpha if loss \n",
    "        hasn't decreased\n",
    "        \n",
    "        patience: Number of epochs to check whether loss has decreased before updating lr or alpha\n",
    "        \n",
    "        filename: Name for the checkpoint file\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, alpha=1, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, size=256, \n",
    "            save=False, load_epoch=-1, lr_decay=-1, alpha_decay=-1, patience=3, filename=''):\n",
    "        \n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            name = './ckpts/pinn_' + filename + '_epoch_' + str(load_epoch)\n",
    "            self.load_weights(name)\n",
    "        \n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            # Reset loss variables\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN and boundary variables and pass them to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN p and r variables via uniform distribution sampling between lower and upper bounds\n",
    "                p = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                r = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = self.p[p_idx]\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound[:] = self.upper_bound[1]\n",
    "                r_boundary = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                \n",
    "                # Train and get loss\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Calculate total epoch loss\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Training loss for epoch {epoch}: pinn: {total_pinn_loss[epoch]:.4f}, boundary: {total_boundary_loss[epoch]:.4f}, total: {(total_boundary_loss[epoch]+total_pinn_loss[epoch]):.4f}')\n",
    "            \n",
    "            # Predict\n",
    "            predictions[:, :, epoch] = self.predict(P_predict, size)\n",
    "            \n",
    "            # Determine if loss has decreased since the last patience epoch\n",
    "            if (epoch > patience):\n",
    "                hasntDecreased = True\n",
    "                if (total_pinn_loss[epoch] + total_boundary_loss[epoch]) < (total_pinn_loss[epoch-patience] + total_boundary_loss[epoch-patience]):\n",
    "                    hasntDecreased = False\n",
    "                        \n",
    "                if (lr_decay != -1) & hasntDecreased:\n",
    "                    lr = lr_decay*lr\n",
    "\n",
    "                if (alpha_decay != -1) & hasntDecreased:\n",
    "                    alpha = alpha_decay*alpha\n",
    "                    print(f'New learning rate {lr} and alpha {alpha}') \n",
    "\n",
    "            # If the epoch is a multiple of 10, save to a checkpoint\n",
    "            if (epoch%10 == 0) & (save == True):\n",
    "                name = './ckpts/pinn_' + filename + '_epoch_' + str(epoch)\n",
    "                self.save_weights(name, overwrite=True, save_format=None, options=None)\n",
    "        \n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, size, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(P.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((size**2, 1))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Pass prediction data through the model\n",
    "            preds[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        # Return f\n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r): # To-do: add loss pass-through!\n",
    "        # Note: p and r are taken out of logspace for the PINN calculation\n",
    "        p = tf.math.exp(p) # GeV/c\n",
    "        r = tf.math.exp(r) # km\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tf.math.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tf.math.divide(p, tf.math.sqrt(tf.math.square(p) + tf.math.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tf.math.reduce_mean(tf.math.abs(f_r + (tf.math.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 15:17:49.043017: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 0: pinn: 1.1353, boundary: 159.4946, total: 160.6299\n",
      "Training loss for epoch 1: pinn: 1.1325, boundary: 63.5595, total: 64.6919\n",
      "Training loss for epoch 2: pinn: 1.9710, boundary: 30.9645, total: 32.9355\n",
      "Training loss for epoch 3: pinn: 2.6749, boundary: 24.4765, total: 27.1514\n",
      "Training loss for epoch 4: pinn: 3.2396, boundary: 18.8528, total: 22.0924\n",
      "Training loss for epoch 5: pinn: 3.5692, boundary: 16.5883, total: 20.1574\n",
      "Training loss for epoch 6: pinn: 3.7424, boundary: 19.2003, total: 22.9427\n",
      "Training loss for epoch 7: pinn: 3.6920, boundary: 12.6691, total: 16.3611\n",
      "Training loss for epoch 8: pinn: 3.7718, boundary: 15.8031, total: 19.5749\n",
      "Training loss for epoch 9: pinn: 3.7940, boundary: 17.5081, total: 21.3021\n",
      "Training loss for epoch 10: pinn: 3.7669, boundary: 9.9408, total: 13.7077\n",
      "Training loss for epoch 11: pinn: 3.8045, boundary: 16.8563, total: 20.6608\n",
      "New learning rate 0.00027 and alpha 0.81\n",
      "Training loss for epoch 12: pinn: 3.6944, boundary: 16.2986, total: 19.9930\n",
      "Training loss for epoch 13: pinn: 3.6678, boundary: 10.8975, total: 14.5653\n",
      "New learning rate 0.000243 and alpha 0.7290000000000001\n",
      "Training loss for epoch 14: pinn: 3.5124, boundary: 10.8500, total: 14.3624\n",
      "Training loss for epoch 15: pinn: 3.3619, boundary: 11.1904, total: 14.5523\n",
      "Training loss for epoch 16: pinn: 3.2601, boundary: 13.0546, total: 16.3147\n",
      "New learning rate 0.0002187 and alpha 0.6561000000000001\n",
      "Training loss for epoch 17: pinn: 3.1520, boundary: 12.5266, total: 15.6787\n",
      "New learning rate 0.00019683 and alpha 0.5904900000000002\n",
      "Training loss for epoch 18: pinn: 2.8738, boundary: 10.2375, total: 13.1113\n",
      "Training loss for epoch 19: pinn: 2.6629, boundary: 7.7537, total: 10.4165\n",
      "Training loss for epoch 20: pinn: 2.5370, boundary: 10.0131, total: 12.5500\n",
      "Training loss for epoch 21: pinn: 2.3503, boundary: 12.5341, total: 14.8844\n",
      "New learning rate 0.000177147 and alpha 0.5314410000000002\n",
      "Training loss for epoch 22: pinn: 2.1593, boundary: 7.7128, total: 9.8721\n",
      "Training loss for epoch 23: pinn: 1.9726, boundary: 7.1817, total: 9.1543\n",
      "Training loss for epoch 24: pinn: 1.8630, boundary: 6.0587, total: 7.9217\n",
      "Training loss for epoch 25: pinn: 1.8732, boundary: 7.2509, total: 9.1242\n",
      "Training loss for epoch 26: pinn: 1.7084, boundary: 9.7411, total: 11.4494\n",
      "New learning rate 0.0001594323 and alpha 0.47829690000000014\n",
      "Training loss for epoch 27: pinn: 1.6277, boundary: 9.1503, total: 10.7780\n",
      "New learning rate 0.00014348907 and alpha 0.43046721000000016\n",
      "Training loss for epoch 28: pinn: 1.4574, boundary: 6.0489, total: 7.5063\n",
      "Training loss for epoch 29: pinn: 1.3798, boundary: 7.0768, total: 8.4566\n",
      "Training loss for epoch 30: pinn: 1.3208, boundary: 7.6533, total: 8.9741\n",
      "Training loss for epoch 31: pinn: 1.2471, boundary: 6.6956, total: 7.9427\n",
      "New learning rate 0.000129140163 and alpha 0.38742048900000015\n",
      "Training loss for epoch 32: pinn: 1.1989, boundary: 7.3356, total: 8.5345\n",
      "New learning rate 0.0001162261467 and alpha 0.34867844010000015\n",
      "Training loss for epoch 33: pinn: 1.1207, boundary: 5.0773, total: 6.1980\n",
      "Training loss for epoch 34: pinn: 1.0669, boundary: 5.1169, total: 6.1838\n",
      "Training loss for epoch 35: pinn: 1.0102, boundary: 4.8464, total: 5.8567\n",
      "Training loss for epoch 36: pinn: 0.9447, boundary: 4.9074, total: 5.8522\n",
      "Training loss for epoch 37: pinn: 0.9110, boundary: 5.9114, total: 6.8224\n",
      "New learning rate 0.00010460353203 and alpha 0.31381059609000017\n",
      "Training loss for epoch 38: pinn: 0.8205, boundary: 3.7527, total: 4.5732\n",
      "Training loss for epoch 39: pinn: 0.7674, boundary: 5.3723, total: 6.1397\n",
      "New learning rate 9.4143178827e-05 and alpha 0.28242953648100017\n",
      "Training loss for epoch 40: pinn: 0.7021, boundary: 5.1923, total: 5.8944\n",
      "Training loss for epoch 41: pinn: 0.6304, boundary: 3.8819, total: 4.5123\n",
      "Training loss for epoch 42: pinn: 0.5746, boundary: 3.9928, total: 4.5674\n",
      "Training loss for epoch 43: pinn: 0.5496, boundary: 4.6952, total: 5.2448\n",
      "Training loss for epoch 44: pinn: 0.5186, boundary: 3.9729, total: 4.4915\n",
      "Training loss for epoch 45: pinn: 0.5228, boundary: 5.6129, total: 6.1357\n",
      "New learning rate 8.47288609443e-05 and alpha 0.25418658283290013\n",
      "Training loss for epoch 46: pinn: 0.4764, boundary: 3.7952, total: 4.2716\n",
      "Training loss for epoch 47: pinn: 0.4519, boundary: 3.7574, total: 4.2093\n",
      "Training loss for epoch 48: pinn: 0.4384, boundary: 3.7352, total: 4.1736\n",
      "Training loss for epoch 49: pinn: 0.4163, boundary: 3.3984, total: 3.8147\n",
      "Training loss for epoch 50: pinn: 0.4062, boundary: 3.7515, total: 4.1577\n",
      "Training loss for epoch 51: pinn: 0.3781, boundary: 3.4130, total: 3.7911\n",
      "Training loss for epoch 52: pinn: 0.3775, boundary: 4.1573, total: 4.5348\n",
      "New learning rate 7.625597484987e-05 and alpha 0.22876792454961012\n",
      "Training loss for epoch 53: pinn: 0.3547, boundary: 3.8738, total: 4.2286\n",
      "New learning rate 6.8630377364883e-05 and alpha 0.2058911320946491\n",
      "Training loss for epoch 54: pinn: 0.3310, boundary: 3.0294, total: 3.3604\n",
      "Training loss for epoch 55: pinn: 0.3128, boundary: 3.3693, total: 3.6821\n",
      "Training loss for epoch 56: pinn: 0.2943, boundary: 2.3013, total: 2.5956\n",
      "Training loss for epoch 57: pinn: 0.2896, boundary: 3.2438, total: 3.5334\n",
      "New learning rate 6.176733962839471e-05 and alpha 0.1853020188851842\n",
      "Training loss for epoch 58: pinn: 0.2802, boundary: 3.0191, total: 3.2993\n",
      "Training loss for epoch 59: pinn: 0.2690, boundary: 2.5712, total: 2.8401\n",
      "New learning rate 5.559060566555524e-05 and alpha 0.16677181699666577\n",
      "Training loss for epoch 60: pinn: 0.2596, boundary: 2.6703, total: 2.9299\n",
      "Training loss for epoch 61: pinn: 0.2408, boundary: 1.1568, total: 1.3977\n",
      "Training loss for epoch 62: pinn: 0.2383, boundary: 2.2531, total: 2.4914\n",
      "Training loss for epoch 63: pinn: 0.2347, boundary: 2.3818, total: 2.6165\n",
      "Training loss for epoch 64: pinn: 0.2283, boundary: 2.6349, total: 2.8632\n",
      "New learning rate 5.003154509899972e-05 and alpha 0.1500946352969992\n",
      "Training loss for epoch 65: pinn: 0.2063, boundary: 1.8160, total: 2.0223\n",
      "Training loss for epoch 66: pinn: 0.2168, boundary: 2.0605, total: 2.2773\n",
      "Training loss for epoch 67: pinn: 0.2106, boundary: 2.3080, total: 2.5185\n",
      "Training loss for epoch 68: pinn: 0.2006, boundary: 1.7370, total: 1.9376\n",
      "Training loss for epoch 69: pinn: 0.2091, boundary: 2.7072, total: 2.9163\n",
      "New learning rate 4.502839058909975e-05 and alpha 0.13508517176729928\n",
      "Training loss for epoch 70: pinn: 0.1956, boundary: 2.0224, total: 2.2180\n",
      "Training loss for epoch 71: pinn: 0.1909, boundary: 1.6005, total: 1.7914\n",
      "Training loss for epoch 72: pinn: 0.1970, boundary: 2.1279, total: 2.3249\n",
      "Training loss for epoch 73: pinn: 0.1862, boundary: 1.5969, total: 1.7831\n",
      "Training loss for epoch 74: pinn: 0.1885, boundary: 1.9336, total: 2.1222\n",
      "New learning rate 4.052555153018977e-05 and alpha 0.12157665459056936\n",
      "Training loss for epoch 75: pinn: 0.1881, boundary: 2.1006, total: 2.2887\n",
      "Training loss for epoch 76: pinn: 0.1761, boundary: 1.6671, total: 1.8432\n",
      "New learning rate 3.64729963771708e-05 and alpha 0.10941898913151243\n",
      "Training loss for epoch 77: pinn: 0.1716, boundary: 1.6221, total: 1.7938\n",
      "Training loss for epoch 78: pinn: 0.1642, boundary: 1.4180, total: 1.5821\n",
      "Training loss for epoch 79: pinn: 0.1639, boundary: 1.6419, total: 1.8058\n",
      "Training loss for epoch 80: pinn: 0.1641, boundary: 1.7064, total: 1.8705\n",
      "New learning rate 3.282569673945372e-05 and alpha 0.0984770902183612\n",
      "Training loss for epoch 81: pinn: 0.1584, boundary: 1.6005, total: 1.7589\n",
      "New learning rate 2.9543127065508348e-05 and alpha 0.08862938119652508\n",
      "Training loss for epoch 82: pinn: 0.1566, boundary: 1.4206, total: 1.5772\n",
      "Training loss for epoch 83: pinn: 0.1430, boundary: 1.1387, total: 1.2818\n",
      "Training loss for epoch 84: pinn: 0.1439, boundary: 1.2009, total: 1.3449\n",
      "Training loss for epoch 85: pinn: 0.1450, boundary: 1.4459, total: 1.5909\n",
      "New learning rate 2.6588814358957513e-05 and alpha 0.07976644307687257\n",
      "Training loss for epoch 86: pinn: 0.1385, boundary: 1.1431, total: 1.2815\n",
      "Training loss for epoch 87: pinn: 0.1406, boundary: 1.2503, total: 1.3909\n",
      "New learning rate 2.392993292306176e-05 and alpha 0.07178979876918531\n",
      "Training loss for epoch 88: pinn: 0.1355, boundary: 1.1944, total: 1.3299\n",
      "Training loss for epoch 89: pinn: 0.1284, boundary: 1.0978, total: 1.2262\n",
      "Training loss for epoch 90: pinn: 0.1274, boundary: 1.1839, total: 1.3113\n",
      "Training loss for epoch 91: pinn: 0.1203, boundary: 1.0599, total: 1.1802\n",
      "Training loss for epoch 92: pinn: 0.1212, boundary: 1.1541, total: 1.2753\n",
      "New learning rate 2.1536939630755587e-05 and alpha 0.06461081889226679\n",
      "Training loss for epoch 93: pinn: 0.1236, boundary: 1.0401, total: 1.1637\n",
      "Training loss for epoch 94: pinn: 0.1225, boundary: 1.1307, total: 1.2532\n",
      "New learning rate 1.938324566768003e-05 and alpha 0.05814973700304011\n",
      "Training loss for epoch 95: pinn: 0.1173, boundary: 0.9638, total: 1.0812\n",
      "Training loss for epoch 96: pinn: 0.1198, boundary: 1.0427, total: 1.1625\n",
      "Training loss for epoch 97: pinn: 0.1206, boundary: 0.9552, total: 1.0759\n",
      "Training loss for epoch 98: pinn: 0.1191, boundary: 0.9577, total: 1.0767\n",
      "Training loss for epoch 99: pinn: 0.1240, boundary: 1.0251, total: 1.1491\n",
      "Training loss for epoch 100: pinn: 0.1254, boundary: 1.0778, total: 1.2032\n",
      "New learning rate 1.7444921100912026e-05 and alpha 0.0523347633027361\n",
      "Training loss for epoch 101: pinn: 0.1250, boundary: 0.9592, total: 1.0842\n",
      "New learning rate 1.5700428990820825e-05 and alpha 0.04710128697246249\n",
      "Training loss for epoch 102: pinn: 0.1241, boundary: 0.8206, total: 0.9447\n",
      "Training loss for epoch 103: pinn: 0.1211, boundary: 0.9560, total: 1.0771\n",
      "Training loss for epoch 104: pinn: 0.1208, boundary: 0.8786, total: 0.9995\n",
      "Training loss for epoch 105: pinn: 0.1190, boundary: 0.8628, total: 0.9818\n",
      "New learning rate 1.4130386091738743e-05 and alpha 0.042391158275216244\n",
      "Training loss for epoch 106: pinn: 0.1174, boundary: 0.8565, total: 0.9739\n",
      "Training loss for epoch 107: pinn: 0.1165, boundary: 0.8544, total: 0.9709\n",
      "Training loss for epoch 108: pinn: 0.1146, boundary: 0.8621, total: 0.9767\n",
      "Training loss for epoch 109: pinn: 0.1134, boundary: 0.8539, total: 0.9673\n",
      "Training loss for epoch 110: pinn: 0.1135, boundary: 0.8399, total: 0.9533\n",
      "Training loss for epoch 111: pinn: 0.1120, boundary: 0.8025, total: 0.9145\n",
      "Training loss for epoch 112: pinn: 0.1129, boundary: 0.8248, total: 0.9377\n",
      "Training loss for epoch 113: pinn: 0.1134, boundary: 0.8829, total: 0.9964\n",
      "New learning rate 1.271734748256487e-05 and alpha 0.03815204244769462\n",
      "Training loss for epoch 114: pinn: 0.1113, boundary: 0.7943, total: 0.9055\n",
      "Training loss for epoch 115: pinn: 0.1106, boundary: 0.7921, total: 0.9027\n",
      "Training loss for epoch 116: pinn: 0.1106, boundary: 0.7500, total: 0.8606\n",
      "Training loss for epoch 117: pinn: 0.1116, boundary: 0.7327, total: 0.8443\n",
      "Training loss for epoch 118: pinn: 0.1122, boundary: 0.7781, total: 0.8903\n",
      "Training loss for epoch 119: pinn: 0.1142, boundary: 0.7864, total: 0.9006\n",
      "New learning rate 1.1445612734308383e-05 and alpha 0.03433683820292516\n",
      "Training loss for epoch 120: pinn: 0.1106, boundary: 0.6493, total: 0.7599\n",
      "Training loss for epoch 121: pinn: 0.1122, boundary: 0.6874, total: 0.7996\n",
      "Training loss for epoch 122: pinn: 0.1124, boundary: 0.7080, total: 0.8204\n",
      "Training loss for epoch 123: pinn: 0.1142, boundary: 0.7298, total: 0.8440\n",
      "New learning rate 1.0301051460877545e-05 and alpha 0.030903154382632643\n",
      "Training loss for epoch 124: pinn: 0.1124, boundary: 0.6214, total: 0.7338\n",
      "Training loss for epoch 125: pinn: 0.1114, boundary: 0.6354, total: 0.7468\n",
      "Training loss for epoch 126: pinn: 0.1080, boundary: 0.6490, total: 0.7570\n",
      "Training loss for epoch 127: pinn: 0.1082, boundary: 0.6511, total: 0.7593\n",
      "New learning rate 9.27094631478979e-06 and alpha 0.02781283894436938\n",
      "Training loss for epoch 128: pinn: 0.1113, boundary: 0.5868, total: 0.6981\n",
      "Training loss for epoch 129: pinn: 0.1156, boundary: 0.5939, total: 0.7095\n",
      "Training loss for epoch 130: pinn: 0.1145, boundary: 0.5245, total: 0.6391\n",
      "Training loss for epoch 131: pinn: 0.1168, boundary: 0.6185, total: 0.7353\n",
      "New learning rate 8.343851683310812e-06 and alpha 0.025031555049932444\n",
      "Training loss for epoch 132: pinn: 0.1200, boundary: 0.5459, total: 0.6660\n",
      "Training loss for epoch 133: pinn: 0.1173, boundary: 0.4987, total: 0.6159\n",
      "Training loss for epoch 134: pinn: 0.1187, boundary: 0.4371, total: 0.5557\n",
      "Training loss for epoch 135: pinn: 0.1370, boundary: 0.4249, total: 0.5618\n",
      "Training loss for epoch 136: pinn: 0.1359, boundary: 0.5249, total: 0.6607\n",
      "New learning rate 7.509466514979731e-06 and alpha 0.0225283995449392\n",
      "Training loss for epoch 137: pinn: 0.1343, boundary: 0.4884, total: 0.6227\n",
      "New learning rate 6.758519863481758e-06 and alpha 0.020275559590445278\n",
      "Training loss for epoch 138: pinn: 0.1325, boundary: 0.4866, total: 0.6191\n",
      "New learning rate 6.082667877133583e-06 and alpha 0.01824800363140075\n",
      "Training loss for epoch 139: pinn: 0.1378, boundary: 0.4432, total: 0.5810\n",
      "Training loss for epoch 140: pinn: 0.1305, boundary: 0.4931, total: 0.6236\n",
      "New learning rate 5.4744010894202246e-06 and alpha 0.016423203268260675\n",
      "Training loss for epoch 141: pinn: 0.1194, boundary: 0.4639, total: 0.5832\n",
      "Training loss for epoch 142: pinn: 0.1098, boundary: 0.5053, total: 0.6152\n",
      "New learning rate 4.9269609804782025e-06 and alpha 0.014780882941434608\n",
      "Training loss for epoch 143: pinn: 0.1045, boundary: 0.4743, total: 0.5788\n",
      "Training loss for epoch 144: pinn: 0.1042, boundary: 0.5033, total: 0.6074\n",
      "New learning rate 4.434264882430382e-06 and alpha 0.013302794647291147\n",
      "Training loss for epoch 145: pinn: 0.0915, boundary: 0.4791, total: 0.5706\n",
      "Training loss for epoch 146: pinn: 0.0909, boundary: 0.5221, total: 0.6129\n",
      "New learning rate 3.990838394187344e-06 and alpha 0.011972515182562033\n",
      "Training loss for epoch 147: pinn: 0.0929, boundary: 0.5160, total: 0.6089\n",
      "New learning rate 3.59175455476861e-06 and alpha 0.01077526366430583\n",
      "Training loss for epoch 148: pinn: 0.0913, boundary: 0.4740, total: 0.5653\n",
      "Training loss for epoch 149: pinn: 0.0903, boundary: 0.4864, total: 0.5766\n",
      "Training loss for epoch 150: pinn: 0.0868, boundary: 0.4902, total: 0.5770\n",
      "Training loss for epoch 151: pinn: 0.0891, boundary: 0.4758, total: 0.5649\n",
      "Training loss for epoch 152: pinn: 0.0890, boundary: 0.4812, total: 0.5703\n",
      "Training loss for epoch 153: pinn: 0.0896, boundary: 0.5126, total: 0.6023\n",
      "New learning rate 3.2325790992917493e-06 and alpha 0.009697737297875247\n",
      "Training loss for epoch 154: pinn: 0.0939, boundary: 0.5033, total: 0.5972\n",
      "New learning rate 2.9093211893625745e-06 and alpha 0.008727963568087723\n",
      "Training loss for epoch 155: pinn: 0.0974, boundary: 0.5016, total: 0.5990\n",
      "New learning rate 2.618389070426317e-06 and alpha 0.00785516721127895\n",
      "Training loss for epoch 156: pinn: 0.0997, boundary: 0.4874, total: 0.5872\n",
      "Training loss for epoch 157: pinn: 0.1010, boundary: 0.5204, total: 0.6214\n",
      "New learning rate 2.3565501633836854e-06 and alpha 0.007069650490151055\n",
      "Training loss for epoch 158: pinn: 0.0996, boundary: 0.5155, total: 0.6151\n",
      "New learning rate 2.120895147045317e-06 and alpha 0.00636268544113595\n",
      "Training loss for epoch 159: pinn: 0.0996, boundary: 0.5232, total: 0.6228\n",
      "New learning rate 1.9088056323407854e-06 and alpha 0.005726416897022355\n",
      "Training loss for epoch 160: pinn: 0.0992, boundary: 0.5491, total: 0.6483\n",
      "New learning rate 1.717925069106707e-06 and alpha 0.00515377520732012\n",
      "Training loss for epoch 161: pinn: 0.0986, boundary: 0.5570, total: 0.6557\n",
      "New learning rate 1.5461325621960363e-06 and alpha 0.004638397686588107\n",
      "Training loss for epoch 162: pinn: 0.1017, boundary: 0.5664, total: 0.6681\n",
      "New learning rate 1.3915193059764327e-06 and alpha 0.0041745579179292966\n",
      "Training loss for epoch 163: pinn: 0.1036, boundary: 0.5650, total: 0.6686\n",
      "New learning rate 1.2523673753787895e-06 and alpha 0.003757102126136367\n",
      "Training loss for epoch 164: pinn: 0.1037, boundary: 0.6033, total: 0.7070\n",
      "New learning rate 1.1271306378409106e-06 and alpha 0.00338139191352273\n",
      "Training loss for epoch 165: pinn: 0.1049, boundary: 0.6078, total: 0.7127\n",
      "New learning rate 1.0144175740568196e-06 and alpha 0.0030432527221704573\n",
      "Training loss for epoch 166: pinn: 0.1093, boundary: 0.6070, total: 0.7163\n",
      "New learning rate 9.129758166511377e-07 and alpha 0.0027389274499534117\n",
      "Training loss for epoch 167: pinn: 0.1072, boundary: 0.6294, total: 0.7366\n",
      "New learning rate 8.216782349860239e-07 and alpha 0.0024650347049580707\n",
      "Training loss for epoch 168: pinn: 0.1088, boundary: 0.6221, total: 0.7309\n",
      "New learning rate 7.395104114874216e-07 and alpha 0.0022185312344622636\n",
      "Training loss for epoch 169: pinn: 0.1087, boundary: 0.6513, total: 0.7600\n",
      "New learning rate 6.655593703386794e-07 and alpha 0.001996678111016037\n",
      "Training loss for epoch 170: pinn: 0.1099, boundary: 0.6371, total: 0.7470\n",
      "New learning rate 5.990034333048114e-07 and alpha 0.0017970102999144335\n",
      "Training loss for epoch 171: pinn: 0.1118, boundary: 0.6685, total: 0.7803\n",
      "New learning rate 5.391030899743303e-07 and alpha 0.0016173092699229901\n",
      "Training loss for epoch 172: pinn: 0.1135, boundary: 0.6706, total: 0.7840\n",
      "New learning rate 4.851927809768973e-07 and alpha 0.0014555783429306911\n",
      "Training loss for epoch 173: pinn: 0.1155, boundary: 0.6523, total: 0.7678\n",
      "New learning rate 4.3667350287920755e-07 and alpha 0.001310020508637622\n",
      "Training loss for epoch 174: pinn: 0.1169, boundary: 0.6661, total: 0.7830\n",
      "New learning rate 3.930061525912868e-07 and alpha 0.0011790184577738598\n",
      "Training loss for epoch 175: pinn: 0.1186, boundary: 0.6745, total: 0.7932\n",
      "New learning rate 3.5370553733215817e-07 and alpha 0.0010611166119964739\n",
      "Training loss for epoch 176: pinn: 0.1149, boundary: 0.6822, total: 0.7971\n",
      "New learning rate 3.1833498359894233e-07 and alpha 0.0009550049507968265\n",
      "Training loss for epoch 177: pinn: 0.1144, boundary: 0.7025, total: 0.8169\n",
      "New learning rate 2.865014852390481e-07 and alpha 0.0008595044557171439\n",
      "Training loss for epoch 178: pinn: 0.1121, boundary: 0.7117, total: 0.8238\n",
      "New learning rate 2.578513367151433e-07 and alpha 0.0007735540101454295\n",
      "Training loss for epoch 179: pinn: 0.1130, boundary: 0.7155, total: 0.8285\n",
      "New learning rate 2.32066203043629e-07 and alpha 0.0006961986091308866\n",
      "Training loss for epoch 180: pinn: 0.1133, boundary: 0.7397, total: 0.8531\n",
      "New learning rate 2.088595827392661e-07 and alpha 0.0006265787482177979\n",
      "Training loss for epoch 181: pinn: 0.1124, boundary: 0.7453, total: 0.8577\n",
      "New learning rate 1.879736244653395e-07 and alpha 0.0005639208733960181\n",
      "Training loss for epoch 182: pinn: 0.1130, boundary: 0.7374, total: 0.8504\n",
      "New learning rate 1.6917626201880554e-07 and alpha 0.0005075287860564164\n",
      "Training loss for epoch 183: pinn: 0.1129, boundary: 0.7427, total: 0.8556\n",
      "New learning rate 1.52258635816925e-07 and alpha 0.00045677590745077476\n",
      "Training loss for epoch 184: pinn: 0.1125, boundary: 0.7497, total: 0.8621\n",
      "New learning rate 1.3703277223523248e-07 and alpha 0.0004110983167056973\n",
      "Training loss for epoch 185: pinn: 0.1148, boundary: 0.7703, total: 0.8851\n",
      "New learning rate 1.2332949501170925e-07 and alpha 0.0003699884850351276\n",
      "Training loss for epoch 186: pinn: 0.1122, boundary: 0.7668, total: 0.8790\n",
      "New learning rate 1.1099654551053832e-07 and alpha 0.00033298963653161486\n",
      "Training loss for epoch 187: pinn: 0.1136, boundary: 0.7857, total: 0.8993\n",
      "New learning rate 9.98968909594845e-08 and alpha 0.0002996906728784534\n",
      "Training loss for epoch 188: pinn: 0.1152, boundary: 0.7911, total: 0.9063\n",
      "New learning rate 8.990720186353605e-08 and alpha 0.00026972160559060804\n",
      "Training loss for epoch 189: pinn: 0.1118, boundary: 0.8022, total: 0.9140\n",
      "New learning rate 8.091648167718244e-08 and alpha 0.00024274944503154723\n",
      "Training loss for epoch 190: pinn: 0.1133, boundary: 0.7744, total: 0.8877\n",
      "Training loss for epoch 191: pinn: 0.1136, boundary: 0.7997, total: 0.9133\n",
      "New learning rate 7.28248335094642e-08 and alpha 0.00021847450052839252\n",
      "Training loss for epoch 192: pinn: 0.1123, boundary: 0.7867, total: 0.8990\n",
      "Training loss for epoch 193: pinn: 0.1138, boundary: 0.8330, total: 0.9468\n",
      "New learning rate 6.554235015851779e-08 and alpha 0.00019662705047555326\n",
      "Training loss for epoch 194: pinn: 0.1138, boundary: 0.8177, total: 0.9315\n",
      "New learning rate 5.898811514266601e-08 and alpha 0.00017696434542799794\n",
      "Training loss for epoch 195: pinn: 0.1139, boundary: 0.8144, total: 0.9283\n",
      "New learning rate 5.308930362839941e-08 and alpha 0.00015926791088519815\n",
      "Training loss for epoch 196: pinn: 0.1127, boundary: 0.8267, total: 0.9394\n",
      "Training loss for epoch 197: pinn: 0.1114, boundary: 0.8296, total: 0.9410\n",
      "New learning rate 4.778037326555947e-08 and alpha 0.00014334111979667834\n",
      "Training loss for epoch 198: pinn: 0.1126, boundary: 0.8377, total: 0.9503\n",
      "New learning rate 4.300233593900353e-08 and alpha 0.00012900700781701051\n",
      "Training loss for epoch 199: pinn: 0.1107, boundary: 0.8412, total: 0.9519\n",
      "New learning rate 3.870210234510318e-08 and alpha 0.00011610630703530947\n",
      "Training loss for epoch 200: pinn: 0.1129, boundary: 0.8373, total: 0.9502\n",
      "New learning rate 3.483189211059286e-08 and alpha 0.00010449567633177853\n",
      "Training loss for epoch 201: pinn: 0.1107, boundary: 0.8528, total: 0.9636\n",
      "New learning rate 3.1348702899533574e-08 and alpha 9.404610869860067e-05\n",
      "Training loss for epoch 202: pinn: 0.1114, boundary: 0.8423, total: 0.9537\n",
      "New learning rate 2.8213832609580217e-08 and alpha 8.464149782874061e-05\n",
      "Training loss for epoch 203: pinn: 0.1118, boundary: 0.8574, total: 0.9692\n",
      "New learning rate 2.5392449348622195e-08 and alpha 7.617734804586655e-05\n",
      "Training loss for epoch 204: pinn: 0.1141, boundary: 0.8441, total: 0.9582\n",
      "Training loss for epoch 205: pinn: 0.1138, boundary: 0.8313, total: 0.9451\n",
      "Training loss for epoch 206: pinn: 0.1108, boundary: 0.8539, total: 0.9647\n",
      "Training loss for epoch 207: pinn: 0.1131, boundary: 0.8712, total: 0.9842\n",
      "New learning rate 2.2853204413759976e-08 and alpha 6.85596132412799e-05\n",
      "Training loss for epoch 208: pinn: 0.1119, boundary: 0.8620, total: 0.9739\n",
      "New learning rate 2.056788397238398e-08 and alpha 6.170365191715192e-05\n",
      "Training loss for epoch 209: pinn: 0.1129, boundary: 0.8678, total: 0.9808\n",
      "New learning rate 1.8511095575145582e-08 and alpha 5.5533286725436733e-05\n",
      "Training loss for epoch 210: pinn: 0.1121, boundary: 0.8847, total: 0.9968\n",
      "New learning rate 1.6659986017631024e-08 and alpha 4.997995805289306e-05\n",
      "Training loss for epoch 211: pinn: 0.1133, boundary: 0.8499, total: 0.9632\n",
      "Training loss for epoch 212: pinn: 0.1135, boundary: 0.8562, total: 0.9696\n",
      "Training loss for epoch 213: pinn: 0.1120, boundary: 0.8651, total: 0.9771\n",
      "Training loss for epoch 214: pinn: 0.1107, boundary: 0.8856, total: 0.9963\n",
      "New learning rate 1.4993987415867923e-08 and alpha 4.4981962247603756e-05\n",
      "Training loss for epoch 215: pinn: 0.1148, boundary: 0.8655, total: 0.9803\n",
      "New learning rate 1.349458867428113e-08 and alpha 4.048376602284338e-05\n",
      "Training loss for epoch 216: pinn: 0.1136, boundary: 0.8768, total: 0.9904\n",
      "New learning rate 1.2145129806853018e-08 and alpha 3.6435389420559045e-05\n",
      "Training loss for epoch 217: pinn: 0.1117, boundary: 0.8505, total: 0.9622\n",
      "Training loss for epoch 218: pinn: 0.1129, boundary: 0.8810, total: 0.9939\n",
      "New learning rate 1.0930616826167717e-08 and alpha 3.279185047850314e-05\n",
      "Training loss for epoch 219: pinn: 0.1107, boundary: 0.8952, total: 1.0058\n",
      "New learning rate 9.837555143550946e-09 and alpha 2.9512665430652825e-05\n",
      "Training loss for epoch 220: pinn: 0.1125, boundary: 0.8796, total: 0.9920\n",
      "New learning rate 8.853799629195851e-09 and alpha 2.6561398887587544e-05\n",
      "Training loss for epoch 221: pinn: 0.1113, boundary: 0.8779, total: 0.9892\n",
      "Training loss for epoch 222: pinn: 0.1129, boundary: 0.8696, total: 0.9826\n",
      "Training loss for epoch 223: pinn: 0.1114, boundary: 0.8985, total: 1.0098\n",
      "New learning rate 7.968419666276265e-09 and alpha 2.390525899882879e-05\n",
      "Training loss for epoch 224: pinn: 0.1114, boundary: 0.8638, total: 0.9752\n",
      "Training loss for epoch 225: pinn: 0.1122, boundary: 0.8799, total: 0.9921\n",
      "New learning rate 7.171577699648639e-09 and alpha 2.1514733098945913e-05\n",
      "Training loss for epoch 226: pinn: 0.1138, boundary: 0.8915, total: 1.0053\n",
      "Training loss for epoch 227: pinn: 0.1139, boundary: 0.8835, total: 0.9975\n",
      "New learning rate 6.454419929683775e-09 and alpha 1.9363259789051322e-05\n",
      "Training loss for epoch 228: pinn: 0.1118, boundary: 0.8876, total: 0.9994\n",
      "New learning rate 5.808977936715398e-09 and alpha 1.742693381014619e-05\n",
      "Training loss for epoch 229: pinn: 0.1126, boundary: 0.8674, total: 0.9800\n",
      "Training loss for epoch 230: pinn: 0.1116, boundary: 0.8664, total: 0.9780\n",
      "Training loss for epoch 231: pinn: 0.1126, boundary: 0.8826, total: 0.9952\n",
      "Training loss for epoch 232: pinn: 0.1143, boundary: 0.8792, total: 0.9934\n",
      "New learning rate 5.228080143043858e-09 and alpha 1.5684240429131574e-05\n",
      "Training loss for epoch 233: pinn: 0.1132, boundary: 0.8801, total: 0.9933\n",
      "New learning rate 4.705272128739473e-09 and alpha 1.4115816386218418e-05\n",
      "Training loss for epoch 234: pinn: 0.1137, boundary: 0.8833, total: 0.9970\n",
      "New learning rate 4.234744915865525e-09 and alpha 1.2704234747596576e-05\n",
      "Training loss for epoch 235: pinn: 0.1131, boundary: 0.8935, total: 1.0065\n",
      "New learning rate 3.811270424278973e-09 and alpha 1.1433811272836918e-05\n",
      "Training loss for epoch 236: pinn: 0.1127, boundary: 0.8641, total: 0.9768\n",
      "Training loss for epoch 237: pinn: 0.1129, boundary: 0.8949, total: 1.0078\n",
      "New learning rate 3.430143381851076e-09 and alpha 1.0290430145553226e-05\n",
      "Training loss for epoch 238: pinn: 0.1130, boundary: 0.8992, total: 1.0122\n",
      "New learning rate 3.0871290436659687e-09 and alpha 9.261387130997904e-06\n",
      "Training loss for epoch 239: pinn: 0.1128, boundary: 0.8641, total: 0.9769\n",
      "New learning rate 2.778416139299372e-09 and alpha 8.335248417898115e-06\n",
      "Training loss for epoch 240: pinn: 0.1130, boundary: 0.8692, total: 0.9822\n",
      "Training loss for epoch 241: pinn: 0.1128, boundary: 0.8643, total: 0.9770\n",
      "Training loss for epoch 242: pinn: 0.1119, boundary: 0.8961, total: 1.0080\n",
      "New learning rate 2.500574525369435e-09 and alpha 7.501723576108304e-06\n",
      "Training loss for epoch 243: pinn: 0.1124, boundary: 0.8714, total: 0.9838\n",
      "New learning rate 2.2505170728324916e-09 and alpha 6.751551218497473e-06\n",
      "Training loss for epoch 244: pinn: 0.1136, boundary: 0.9076, total: 1.0212\n",
      "New learning rate 2.0254653655492425e-09 and alpha 6.076396096647726e-06\n",
      "Training loss for epoch 245: pinn: 0.1116, boundary: 0.8981, total: 1.0097\n",
      "New learning rate 1.8229188289943183e-09 and alpha 5.468756486982954e-06\n",
      "Training loss for epoch 246: pinn: 0.1119, boundary: 0.8817, total: 0.9936\n",
      "New learning rate 1.6406269460948865e-09 and alpha 4.9218808382846585e-06\n",
      "Training loss for epoch 247: pinn: 0.1127, boundary: 0.8987, total: 1.0115\n",
      "Training loss for epoch 248: pinn: 0.1128, boundary: 0.8824, total: 0.9951\n",
      "Training loss for epoch 249: pinn: 0.1125, boundary: 0.8893, total: 1.0019\n",
      "New learning rate 1.4765642514853978e-09 and alpha 4.429692754456193e-06\n",
      "Training loss for epoch 250: pinn: 0.1127, boundary: 0.8896, total: 1.0023\n",
      "Training loss for epoch 251: pinn: 0.1131, boundary: 0.9080, total: 1.0212\n",
      "New learning rate 1.3289078263368581e-09 and alpha 3.986723479010574e-06\n",
      "Training loss for epoch 252: pinn: 0.1130, boundary: 0.8868, total: 0.9998\n",
      "Training loss for epoch 253: pinn: 0.1118, boundary: 0.8853, total: 0.9971\n",
      "Training loss for epoch 254: pinn: 0.1128, boundary: 0.8929, total: 1.0057\n",
      "Training loss for epoch 255: pinn: 0.1135, boundary: 0.8938, total: 1.0073\n",
      "New learning rate 1.1960170437031724e-09 and alpha 3.5880511311095166e-06\n",
      "Training loss for epoch 256: pinn: 0.1125, boundary: 0.8926, total: 1.0051\n",
      "New learning rate 1.0764153393328553e-09 and alpha 3.229246017998565e-06\n",
      "Training loss for epoch 257: pinn: 0.1138, boundary: 0.8712, total: 0.9850\n",
      "Training loss for epoch 258: pinn: 0.1114, boundary: 0.8841, total: 0.9955\n",
      "Training loss for epoch 259: pinn: 0.1120, boundary: 0.8803, total: 0.9923\n",
      "Training loss for epoch 260: pinn: 0.1131, boundary: 0.8877, total: 1.0008\n",
      "New learning rate 9.687738053995699e-10 and alpha 2.9063214161987086e-06\n",
      "Training loss for epoch 261: pinn: 0.1120, boundary: 0.8835, total: 0.9955\n",
      "Training loss for epoch 262: pinn: 0.1125, boundary: 0.8928, total: 1.0054\n",
      "New learning rate 8.718964248596129e-10 and alpha 2.615689274578838e-06\n",
      "Training loss for epoch 263: pinn: 0.1115, boundary: 0.9000, total: 1.0114\n",
      "New learning rate 7.847067823736516e-10 and alpha 2.354120347120954e-06\n",
      "Training loss for epoch 264: pinn: 0.1123, boundary: 0.9072, total: 1.0195\n",
      "New learning rate 7.062361041362865e-10 and alpha 2.118708312408859e-06\n",
      "Training loss for epoch 265: pinn: 0.1122, boundary: 0.8799, total: 0.9921\n",
      "Training loss for epoch 266: pinn: 0.1129, boundary: 0.8829, total: 0.9958\n",
      "Training loss for epoch 267: pinn: 0.1111, boundary: 0.8958, total: 1.0068\n",
      "Training loss for epoch 268: pinn: 0.1125, boundary: 0.9040, total: 1.0165\n",
      "New learning rate 6.356124937226579e-10 and alpha 1.906837481167973e-06\n",
      "Training loss for epoch 269: pinn: 0.1135, boundary: 0.9005, total: 1.0140\n",
      "New learning rate 5.720512443503921e-10 and alpha 1.7161537330511758e-06\n",
      "Training loss for epoch 270: pinn: 0.1121, boundary: 0.8905, total: 1.0027\n",
      "Training loss for epoch 271: pinn: 0.1132, boundary: 0.8755, total: 0.9887\n",
      "Training loss for epoch 272: pinn: 0.1133, boundary: 0.8839, total: 0.9973\n",
      "Training loss for epoch 273: pinn: 0.1126, boundary: 0.8821, total: 0.9947\n",
      "Training loss for epoch 274: pinn: 0.1132, boundary: 0.9117, total: 1.0249\n",
      "New learning rate 5.148461199153529e-10 and alpha 1.5445383597460583e-06\n",
      "Training loss for epoch 275: pinn: 0.1116, boundary: 0.8777, total: 0.9893\n",
      "Training loss for epoch 276: pinn: 0.1133, boundary: 0.8733, total: 0.9866\n",
      "Training loss for epoch 277: pinn: 0.1144, boundary: 0.8870, total: 1.0014\n",
      "Training loss for epoch 278: pinn: 0.1129, boundary: 0.9174, total: 1.0303\n",
      "New learning rate 4.633615079238176e-10 and alpha 1.3900845237714525e-06\n",
      "Training loss for epoch 279: pinn: 0.1141, boundary: 0.8880, total: 1.0022\n",
      "New learning rate 4.1702535713143583e-10 and alpha 1.2510760713943073e-06\n",
      "Training loss for epoch 280: pinn: 0.1118, boundary: 0.9062, total: 1.0181\n",
      "New learning rate 3.7532282141829226e-10 and alpha 1.1259684642548765e-06\n",
      "Training loss for epoch 281: pinn: 0.1122, boundary: 0.8789, total: 0.9911\n",
      "Training loss for epoch 282: pinn: 0.1104, boundary: 0.8814, total: 0.9918\n",
      "Training loss for epoch 283: pinn: 0.1124, boundary: 0.8741, total: 0.9866\n",
      "Training loss for epoch 284: pinn: 0.1156, boundary: 0.8797, total: 0.9954\n",
      "New learning rate 3.3779053927646306e-10 and alpha 1.0133716178293888e-06\n",
      "Training loss for epoch 285: pinn: 0.1133, boundary: 0.9021, total: 1.0154\n",
      "New learning rate 3.0401148534881676e-10 and alpha 9.1203445604645e-07\n",
      "Training loss for epoch 286: pinn: 0.1119, boundary: 0.9034, total: 1.0152\n",
      "New learning rate 2.736103368139351e-10 and alpha 8.20831010441805e-07\n",
      "Training loss for epoch 287: pinn: 0.1138, boundary: 0.9020, total: 1.0157\n",
      "New learning rate 2.462493031325416e-10 and alpha 7.387479093976244e-07\n",
      "Training loss for epoch 288: pinn: 0.1123, boundary: 0.8998, total: 1.0121\n",
      "Training loss for epoch 289: pinn: 0.1128, boundary: 0.8897, total: 1.0025\n",
      "Training loss for epoch 290: pinn: 0.1128, boundary: 0.9034, total: 1.0162\n",
      "New learning rate 2.2162437281928744e-10 and alpha 6.64873118457862e-07\n",
      "Training loss for epoch 291: pinn: 0.1130, boundary: 0.8776, total: 0.9906\n",
      "Training loss for epoch 292: pinn: 0.1126, boundary: 0.8697, total: 0.9823\n",
      "Training loss for epoch 293: pinn: 0.1118, boundary: 0.8864, total: 0.9982\n",
      "Training loss for epoch 294: pinn: 0.1129, boundary: 0.8783, total: 0.9912\n",
      "New learning rate 1.994619355373587e-10 and alpha 5.983858066120759e-07\n",
      "Training loss for epoch 295: pinn: 0.1130, boundary: 0.8982, total: 1.0112\n",
      "New learning rate 1.7951574198362283e-10 and alpha 5.385472259508683e-07\n",
      "Training loss for epoch 296: pinn: 0.1128, boundary: 0.9115, total: 1.0243\n",
      "New learning rate 1.6156416778526056e-10 and alpha 4.846925033557815e-07\n",
      "Training loss for epoch 297: pinn: 0.1116, boundary: 0.8735, total: 0.9851\n",
      "Training loss for epoch 298: pinn: 0.1117, boundary: 0.9102, total: 1.0219\n",
      "New learning rate 1.454077510067345e-10 and alpha 4.3622325302020334e-07\n",
      "Training loss for epoch 299: pinn: 0.1120, boundary: 0.8821, total: 0.9941\n",
      "Training loss for epoch 300: pinn: 0.1122, boundary: 0.8930, total: 1.0053\n",
      "New learning rate 1.3086697590606107e-10 and alpha 3.92600927718183e-07\n",
      "Training loss for epoch 301: pinn: 0.1120, boundary: 0.9004, total: 1.0124\n",
      "Training loss for epoch 302: pinn: 0.1141, boundary: 0.8892, total: 1.0032\n",
      "New learning rate 1.1778027831545497e-10 and alpha 3.5334083494636473e-07\n",
      "Training loss for epoch 303: pinn: 0.1128, boundary: 0.8939, total: 1.0068\n",
      "New learning rate 1.0600225048390948e-10 and alpha 3.180067514517283e-07\n",
      "Training loss for epoch 304: pinn: 0.1121, boundary: 0.9043, total: 1.0164\n",
      "New learning rate 9.540202543551853e-11 and alpha 2.8620607630655544e-07\n",
      "Training loss for epoch 305: pinn: 0.1136, boundary: 0.8904, total: 1.0040\n",
      "New learning rate 8.586182289196668e-11 and alpha 2.575854686758999e-07\n",
      "Training loss for epoch 306: pinn: 0.1139, boundary: 0.8979, total: 1.0118\n",
      "New learning rate 7.727564060277002e-11 and alpha 2.318269218083099e-07\n",
      "Training loss for epoch 307: pinn: 0.1119, boundary: 0.8791, total: 0.9910\n",
      "Training loss for epoch 308: pinn: 0.1111, boundary: 0.8791, total: 0.9902\n",
      "Training loss for epoch 309: pinn: 0.1121, boundary: 0.8739, total: 0.9860\n",
      "Training loss for epoch 310: pinn: 0.1116, boundary: 0.8759, total: 0.9876\n",
      "Training loss for epoch 311: pinn: 0.1123, boundary: 0.8861, total: 0.9985\n",
      "New learning rate 6.954807654249302e-11 and alpha 2.086442296274789e-07\n",
      "Training loss for epoch 312: pinn: 0.1126, boundary: 0.8917, total: 1.0043\n",
      "New learning rate 6.259326888824372e-11 and alpha 1.87779806664731e-07\n",
      "Training loss for epoch 313: pinn: 0.1120, boundary: 0.9010, total: 1.0131\n",
      "New learning rate 5.633394199941935e-11 and alpha 1.6900182599825792e-07\n",
      "Training loss for epoch 314: pinn: 0.1129, boundary: 0.8924, total: 1.0053\n",
      "New learning rate 5.070054779947741e-11 and alpha 1.5210164339843212e-07\n",
      "Training loss for epoch 315: pinn: 0.1136, boundary: 0.8791, total: 0.9927\n",
      "Training loss for epoch 316: pinn: 0.1124, boundary: 0.8795, total: 0.9919\n",
      "Training loss for epoch 317: pinn: 0.1116, boundary: 0.8812, total: 0.9929\n",
      "Training loss for epoch 318: pinn: 0.1120, boundary: 0.9028, total: 1.0149\n",
      "New learning rate 4.563049301952967e-11 and alpha 1.3689147905858892e-07\n",
      "Training loss for epoch 319: pinn: 0.1123, boundary: 0.8950, total: 1.0073\n",
      "New learning rate 4.1067443717576704e-11 and alpha 1.2320233115273002e-07\n",
      "Training loss for epoch 320: pinn: 0.1134, boundary: 0.8722, total: 0.9856\n",
      "Training loss for epoch 321: pinn: 0.1122, boundary: 0.9048, total: 1.0169\n",
      "New learning rate 3.6960699345819034e-11 and alpha 1.1088209803745703e-07\n",
      "Training loss for epoch 322: pinn: 0.1116, boundary: 0.8760, total: 0.9876\n",
      "Training loss for epoch 323: pinn: 0.1112, boundary: 0.8898, total: 1.0010\n",
      "New learning rate 3.326462941123713e-11 and alpha 9.979388823371132e-08\n",
      "Training loss for epoch 324: pinn: 0.1125, boundary: 0.9050, total: 1.0175\n",
      "New learning rate 2.993816647011342e-11 and alpha 8.98144994103402e-08\n",
      "Training loss for epoch 325: pinn: 0.1108, boundary: 0.8837, total: 0.9945\n",
      "New learning rate 2.6944349823102082e-11 and alpha 8.083304946930617e-08\n",
      "Training loss for epoch 326: pinn: 0.1116, boundary: 0.8923, total: 1.0039\n",
      "New learning rate 2.4249914840791875e-11 and alpha 7.274974452237555e-08\n",
      "Training loss for epoch 327: pinn: 0.1121, boundary: 0.8924, total: 1.0044\n",
      "Training loss for epoch 328: pinn: 0.1126, boundary: 0.8937, total: 1.0063\n",
      "New learning rate 2.182492335671269e-11 and alpha 6.5474770070138e-08\n",
      "Training loss for epoch 329: pinn: 0.1117, boundary: 0.8905, total: 1.0022\n",
      "Training loss for epoch 330: pinn: 0.1115, boundary: 0.8971, total: 1.0085\n",
      "New learning rate 1.964243102104142e-11 and alpha 5.8927293063124204e-08\n",
      "Training loss for epoch 331: pinn: 0.1131, boundary: 0.9065, total: 1.0195\n",
      "New learning rate 1.7678187918937277e-11 and alpha 5.303456375681178e-08\n",
      "Training loss for epoch 332: pinn: 0.1131, boundary: 0.8866, total: 0.9997\n",
      "Training loss for epoch 333: pinn: 0.1120, boundary: 0.8933, total: 1.0053\n",
      "Training loss for epoch 334: pinn: 0.1114, boundary: 0.8910, total: 1.0023\n",
      "Training loss for epoch 335: pinn: 0.1138, boundary: 0.9069, total: 1.0207\n",
      "New learning rate 1.591036912704355e-11 and alpha 4.7731107381130604e-08\n",
      "Training loss for epoch 336: pinn: 0.1102, boundary: 0.9019, total: 1.0121\n",
      "New learning rate 1.4319332214339195e-11 and alpha 4.295799664301754e-08\n",
      "Training loss for epoch 337: pinn: 0.1128, boundary: 0.8580, total: 0.9708\n",
      "Training loss for epoch 338: pinn: 0.1149, boundary: 0.9118, total: 1.0268\n",
      "New learning rate 1.2887398992905275e-11 and alpha 3.866219697871579e-08\n",
      "Training loss for epoch 339: pinn: 0.1123, boundary: 0.9070, total: 1.0192\n",
      "New learning rate 1.1598659093614748e-11 and alpha 3.479597728084421e-08\n",
      "Training loss for epoch 340: pinn: 0.1125, boundary: 0.8871, total: 0.9996\n",
      "New learning rate 1.0438793184253272e-11 and alpha 3.131637955275979e-08\n",
      "Training loss for epoch 341: pinn: 0.1125, boundary: 0.8705, total: 0.9829\n",
      "Training loss for epoch 342: pinn: 0.1126, boundary: 0.8674, total: 0.9801\n",
      "Training loss for epoch 343: pinn: 0.1133, boundary: 0.9034, total: 1.0167\n",
      "New learning rate 9.394913865827945e-12 and alpha 2.8184741597483813e-08\n",
      "Training loss for epoch 344: pinn: 0.1116, boundary: 0.8931, total: 1.0047\n",
      "New learning rate 8.455422479245151e-12 and alpha 2.5366267437735433e-08\n",
      "Training loss for epoch 345: pinn: 0.1129, boundary: 0.8867, total: 0.9996\n",
      "New learning rate 7.609880231320637e-12 and alpha 2.282964069396189e-08\n",
      "Training loss for epoch 346: pinn: 0.1125, boundary: 0.8792, total: 0.9917\n",
      "Training loss for epoch 347: pinn: 0.1120, boundary: 0.8882, total: 1.0002\n",
      "Training loss for epoch 348: pinn: 0.1123, boundary: 0.8716, total: 0.9840\n",
      "Training loss for epoch 349: pinn: 0.1132, boundary: 0.8784, total: 0.9916\n",
      "Training loss for epoch 350: pinn: 0.1133, boundary: 0.8697, total: 0.9830\n",
      "Training loss for epoch 351: pinn: 0.1124, boundary: 0.8649, total: 0.9773\n",
      "Training loss for epoch 352: pinn: 0.1119, boundary: 0.8884, total: 1.0003\n",
      "New learning rate 6.848892208188573e-12 and alpha 2.05466766245657e-08\n",
      "Training loss for epoch 353: pinn: 0.1129, boundary: 0.8937, total: 1.0066\n",
      "New learning rate 6.1640029873697155e-12 and alpha 1.849200896210913e-08\n",
      "Training loss for epoch 354: pinn: 0.1126, boundary: 0.8976, total: 1.0103\n",
      "New learning rate 5.547602688632744e-12 and alpha 1.6642808065898218e-08\n",
      "Training loss for epoch 355: pinn: 0.1138, boundary: 0.8812, total: 0.9950\n",
      "Training loss for epoch 356: pinn: 0.1146, boundary: 0.8974, total: 1.0121\n",
      "New learning rate 4.992842419769469e-12 and alpha 1.4978527259308396e-08\n",
      "Training loss for epoch 357: pinn: 0.1125, boundary: 0.9055, total: 1.0180\n",
      "New learning rate 4.4935581777925224e-12 and alpha 1.3480674533377557e-08\n",
      "Training loss for epoch 358: pinn: 0.1128, boundary: 0.8856, total: 0.9984\n",
      "New learning rate 4.04420236001327e-12 and alpha 1.2132607080039802e-08\n",
      "Training loss for epoch 359: pinn: 0.1124, boundary: 0.8722, total: 0.9847\n",
      "Training loss for epoch 360: pinn: 0.1133, boundary: 0.8792, total: 0.9925\n",
      "Training loss for epoch 361: pinn: 0.1116, boundary: 0.8988, total: 1.0104\n",
      "New learning rate 3.639782124011943e-12 and alpha 1.0919346372035822e-08\n",
      "Training loss for epoch 362: pinn: 0.1108, boundary: 0.8970, total: 1.0078\n",
      "New learning rate 3.275803911610749e-12 and alpha 9.82741173483224e-09\n",
      "Training loss for epoch 363: pinn: 0.1139, boundary: 0.8741, total: 0.9879\n",
      "Training loss for epoch 364: pinn: 0.1127, boundary: 0.9063, total: 1.0190\n",
      "New learning rate 2.9482235204496743e-12 and alpha 8.844670561349016e-09\n",
      "Training loss for epoch 365: pinn: 0.1109, boundary: 0.8947, total: 1.0056\n",
      "Training loss for epoch 366: pinn: 0.1134, boundary: 0.8930, total: 1.0065\n",
      "New learning rate 2.653401168404707e-12 and alpha 7.960203505214115e-09\n",
      "Training loss for epoch 367: pinn: 0.1128, boundary: 0.8599, total: 0.9727\n",
      "Training loss for epoch 368: pinn: 0.1115, boundary: 0.9135, total: 1.0250\n",
      "New learning rate 2.3880610515642365e-12 and alpha 7.164183154692703e-09\n",
      "Training loss for epoch 369: pinn: 0.1139, boundary: 0.8801, total: 0.9939\n",
      "Training loss for epoch 370: pinn: 0.1131, boundary: 0.8465, total: 0.9596\n",
      "Training loss for epoch 371: pinn: 0.1128, boundary: 0.8681, total: 0.9809\n",
      "Training loss for epoch 372: pinn: 0.1130, boundary: 0.8683, total: 0.9813\n",
      "Training loss for epoch 373: pinn: 0.1134, boundary: 0.8777, total: 0.9911\n",
      "New learning rate 2.149254946407813e-12 and alpha 6.4477648392234335e-09\n",
      "Training loss for epoch 374: pinn: 0.1129, boundary: 0.8835, total: 0.9964\n",
      "New learning rate 1.934329451767032e-12 and alpha 5.80298835530109e-09\n",
      "Training loss for epoch 375: pinn: 0.1122, boundary: 0.8859, total: 0.9980\n",
      "New learning rate 1.7408965065903287e-12 and alpha 5.222689519770981e-09\n",
      "Training loss for epoch 376: pinn: 0.1124, boundary: 0.8561, total: 0.9685\n",
      "Training loss for epoch 377: pinn: 0.1135, boundary: 0.8810, total: 0.9946\n",
      "Training loss for epoch 378: pinn: 0.1112, boundary: 0.8871, total: 0.9983\n",
      "New learning rate 1.5668068559312958e-12 and alpha 4.7004205677938825e-09\n",
      "Training loss for epoch 379: pinn: 0.1116, boundary: 0.9198, total: 1.0313\n",
      "New learning rate 1.4101261703381664e-12 and alpha 4.230378511014494e-09\n",
      "Training loss for epoch 380: pinn: 0.1111, boundary: 0.8835, total: 0.9946\n",
      "New learning rate 1.2691135533043497e-12 and alpha 3.807340659913045e-09\n",
      "Training loss for epoch 381: pinn: 0.1131, boundary: 0.8898, total: 1.0029\n",
      "New learning rate 1.1422021979739148e-12 and alpha 3.4266065939217406e-09\n",
      "Training loss for epoch 382: pinn: 0.1113, boundary: 0.8778, total: 0.9891\n",
      "Training loss for epoch 383: pinn: 0.1135, boundary: 0.8913, total: 1.0048\n",
      "New learning rate 1.0279819781765234e-12 and alpha 3.0839459345295667e-09\n",
      "Training loss for epoch 384: pinn: 0.1133, boundary: 0.8550, total: 0.9683\n",
      "Training loss for epoch 385: pinn: 0.1134, boundary: 0.8750, total: 0.9884\n",
      "Training loss for epoch 386: pinn: 0.1122, boundary: 0.8893, total: 1.0015\n",
      "Training loss for epoch 387: pinn: 0.1113, boundary: 0.8799, total: 0.9911\n",
      "New learning rate 9.25183780358871e-13 and alpha 2.77555134107661e-09\n",
      "Training loss for epoch 388: pinn: 0.1134, boundary: 0.8897, total: 1.0031\n",
      "New learning rate 8.32665402322984e-13 and alpha 2.497996206968949e-09\n",
      "Training loss for epoch 389: pinn: 0.1143, boundary: 0.8786, total: 0.9929\n",
      "Training loss for epoch 390: pinn: 0.1136, boundary: 0.9036, total: 1.0172\n",
      "New learning rate 7.493988620906856e-13 and alpha 2.248196586272054e-09\n",
      "Training loss for epoch 391: pinn: 0.1118, boundary: 0.8896, total: 1.0014\n",
      "Training loss for epoch 392: pinn: 0.1117, boundary: 0.8968, total: 1.0085\n",
      "New learning rate 6.744589758816171e-13 and alpha 2.0233769276448487e-09\n",
      "Training loss for epoch 393: pinn: 0.1140, boundary: 0.8695, total: 0.9835\n",
      "Training loss for epoch 394: pinn: 0.1099, boundary: 0.9004, total: 1.0104\n",
      "New learning rate 6.070130782934554e-13 and alpha 1.821039234880364e-09\n",
      "Training loss for epoch 395: pinn: 0.1114, boundary: 0.8977, total: 1.0092\n",
      "New learning rate 5.463117704641098e-13 and alpha 1.6389353113923277e-09\n",
      "Training loss for epoch 396: pinn: 0.1121, boundary: 0.9246, total: 1.0367\n",
      "New learning rate 4.916805934176989e-13 and alpha 1.475041780253095e-09\n",
      "Training loss for epoch 397: pinn: 0.1119, boundary: 0.8852, total: 0.9971\n",
      "Training loss for epoch 398: pinn: 0.1138, boundary: 0.8867, total: 1.0005\n",
      "Training loss for epoch 399: pinn: 0.1119, boundary: 0.9021, total: 1.0140\n"
     ]
    }
   ],
   "source": [
    "# Neural network. Note: 2 inputs- (p, r), 1 output- f(r, p)\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(140, activation='selu')(inputs)\n",
    "x_ = tf.keras.layers.Dense(140, activation='selu')(x_)\n",
    "x_ = tf.keras.layers.Dense(140, activation='selu')(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.95\n",
    "alpha_decay = 0.99\n",
    "lr = 3e-2\n",
    "lr_decay = 0.99\n",
    "batchsize = 1032\n",
    "boundary_batchsize = 256\n",
    "epochs = 300\n",
    "save = False\n",
    "load_epoch = -1\n",
    "patience = 8\n",
    "filename = ''\n",
    "\n",
    "# Initialize and fit the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "            f_boundary=f_boundary[:, 0], size=size)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, batchsize=batchsize, boundary_batchsize=boundary_batchsize,\n",
    "                                                         epochs=epochs, lr=lr, size=size, save=save, load_epoch=load_epoch, lr_decay=lr_decay,\n",
    "                                                         alpha_decay=alpha_decay, patience=patience, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the outputs to a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open('./figures/pickles/pinn_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/pickles/boundary_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/pickles/predictions_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "# with open('./figures/pickles/f_boundary.pkl', 'wb') as file:\n",
    "#     pkl.dump(f_boundary, file)\n",
    "    \n",
    "# with open('./figures/pickles/p.pkl', 'wb') as file:\n",
    "#     pkl.dump(p, file)\n",
    "    \n",
    "# with open('./figures/pickles/T.pkl', 'wb') as file:\n",
    "#     pkl.dump(T, file)\n",
    "    \n",
    "# with open('./figures/pickles/r.pkl', 'wb') as file:\n",
    "#     pkl.dump(r, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
