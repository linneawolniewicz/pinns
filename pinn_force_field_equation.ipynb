{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Physics Informed Neural Networks\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook adapts the code implementation of Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs, and the code by Michael Ito, to solve the force-field equation for solar modulation of cosmic rays. Michael Ito's code adaption use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure. \n",
    "\n",
    "In this application, our PINN is $h(r, p) = \\frac{\\partial f}{\\partial r} + \\frac{RV}{3k} \\frac{\\partial f}{\\partial p}$ where $k=\\beta(p)k_1(r)k_2(r)$ and $\\beta = \\frac{p}{\\sqrt{p^2 + M^2}}$. We will approximate $f(r, p)$ using a neural network.\n",
    "\n",
    "We have no initial data, but our boundary data will be given by $f(r_{HP}, p) = \\frac{J(r_{HP}, T)}{p^2} = \\frac{(T+M)^\\gamma}{p^2}$, where $r_{HP} = 120$ AU (i.e. the radius of Heliopause), $M=0.938$ GeV, $\\gamma$ is between $-2$ and $-3$, and $T = \\sqrt{p^2 + M^2} - M$. Or, vice versa, $p = \\sqrt{T^2 + 2TM}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfm = tf.math\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants  \n",
    "m = 0.938 # GeV/c^2\n",
    "gamma = -3 # Between -2 and -3\n",
    "size = 512 # size of r, T, p, and f_boundary\n",
    "au = 150e6 # 150e6 m/AU\n",
    "r_limits = [119, 120]\n",
    "T_limits = [0.001, 1000]\n",
    "\n",
    "# Create boundary data\n",
    "T = np.logspace(np.log10(T_limits[0]), np.log10(T_limits[1]), size).flatten()[:, None]\n",
    "p = (np.sqrt((T+m)**2-m**2)).flatten()[:,None] # GeV/c\n",
    "r = np.logspace(np.log10(r_limits[0]*au), np.log10(r_limits[1]*au), size).flatten()[:, None] # km\n",
    "f_boundary = ((T + m)**gamma)/(p**2) # particles/(m^3 (GeV/c)^3)\n",
    "\n",
    "# Get upper and lower bounds\n",
    "lb = np.array([p[0], r[0]], dtype='float32')\n",
    "ub = np.array([p[-1], r[-1]], dtype='float32')\n",
    "\n",
    "# Create test data\n",
    "p_predict = np.log(p)\n",
    "r_predict = np.log(r)\n",
    "\n",
    "p_predict = (p_predict - lb[0])/np.abs(ub[0] - lb[0])\n",
    "r_predict = (r_predict - lb[1])/np.abs(ub[1] - lb[1])\n",
    "    \n",
    "P, R = np.meshgrid(p_predict, r_predict)\n",
    "P_predict = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: (512, 1), p: (512, 1), T: (512, 1), f_boundary: (512, 1), P_predict: (262144, 2)\n",
      "lb: [[4.332436e-02]\n",
      " [1.785000e+10]], ub:[[1.00093756e+03]\n",
      " [1.80000010e+10]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x1500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check inputs\n",
    "print(f'r: {r.shape}, p: {p.shape}, T: {T.shape}, f_boundary: {f_boundary.shape}, P_predict: {P_predict.shape}')\n",
    "print(f'lb: {lb}, ub:{ub}')\n",
    "\n",
    "# Visualize\n",
    "fig, axs = plt.subplots(4, figsize=(5, 15))\n",
    "for ax, data, name in zip(axs, [T, f_boundary, r, p], ['T', 'f_boundary', 'r', 'p']):\n",
    "    ax.set_title(name)\n",
    "    pd.Series(data[:, 0]).hist(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, f_boundary, size, n_samples=20000):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.size = size\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and f_boundary.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "        beta: boundary_loss scale factor\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha, beta):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                t1.watch(p)\n",
    "                t1.watch(r)\n",
    "                \n",
    "                lb = tfm.log(self.lower_bound)\n",
    "                ub = tfm.log(self.upper_bound)\n",
    "                \n",
    "                # PINN loss\n",
    "                p_scaled = (tfm.log(p) - lb[0])/tfm.abs(ub[0] - lb[0])\n",
    "                r_scaled = (tfm.log(r) - lb[1])/tfm.abs(ub[1] - lb[1])\n",
    "                \n",
    "                P = tf.concat((p_scaled, r_scaled), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Boundary loss\n",
    "                p_boundary_scaled = (tfm.log(p_boundary) - lb[0])/tfm.abs(ub[0] - lb[0])\n",
    "                r_boundary_scaled = (tfm.log(r_boundary) - lb[1])/tfm.abs(ub[1] - lb[1])\n",
    "                \n",
    "                P_boundary = tf.concat((p_boundary_scaled, r_boundary_scaled), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "\n",
    "                boundary_loss = tfm.reduce_mean(tfm.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            f_p = t1.gradient(f, p)\n",
    "            f_r = t1.gradient(f, r)\n",
    "            \n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r)\n",
    "            total_loss = (1-alpha)*pinn_loss + alpha*beta*boundary_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        alpha: weight on boundary_loss, 1-alpha weight on pinn_loss\n",
    "        \n",
    "        beta: boundary_loss scale factor\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_decay: If -1, learning rate will not be decayed. Otherwise, lr = lr_decay*lr if loss hasn't \n",
    "        decreased\n",
    "        \n",
    "        alpha_decay: If -1, alpha will not be changed. Otherwise, alpha = alpha_decay*alpha if loss \n",
    "        hasn't decreased\n",
    "        \n",
    "        alpha_limit = Minimum alpha value to decay to\n",
    "        \n",
    "        patience: Number of epochs to check whether loss has decreased before updating lr or alpha\n",
    "        \n",
    "        filename: Name for the checkpoint file\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, alpha=0.5, beta=0.01, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, size=256, \n",
    "            save=False, load_epoch=-1, lr_decay=-1, alpha_decay=-1, alpha_limit = 0.5, patience=3, filename=''):\n",
    "        \n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            name = './ckpts/pinn_' + filename + '_epoch_' + str(load_epoch)\n",
    "            self.load_weights(name)\n",
    "        \n",
    "        # Initialize\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs,))\n",
    "        total_boundary_loss = np.zeros((epochs,))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, sample data and pass to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Sample p and r according to a beta distribution between upper and lower bounds\n",
    "                dist = tfd.Beta(1, 5)\n",
    "                p = (dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[0] - self.lower_bound[0])) + self.lower_bound[0]\n",
    "                r = (dist.sample((batchsize, 1))*tfm.abs(self.upper_bound[1] - self.lower_bound[1])) + self.lower_bound[1]\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = tf.Variable(self.p[p_idx], dtype=tf.float32)\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound[:] = self.upper_bound[1]\n",
    "                r_boundary = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                \n",
    "                # Train and get loss\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Sum losses\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Epoch {epoch}. Current alpha: {alpha:.4f}, lr: {lr:.6f}. Training losses: pinn: {total_pinn_loss[epoch]:.10f}, ' +\n",
    "                  f'boundary: {total_boundary_loss[epoch]:.6f}, weighted total: {((alpha*beta*total_boundary_loss[epoch])+((1-alpha)*total_pinn_loss[epoch])):.4f}')\n",
    "            \n",
    "            predictions[:, :, epoch] = self.predict(P_predict, batchsize)\n",
    "            \n",
    "            # Decay lr if loss hasn't decreased since current epoch - patience\n",
    "            if (epoch > patience):\n",
    "                hasntDecreased = False\n",
    "                if (total_pinn_loss[epoch] + total_boundary_loss[epoch]) > (total_pinn_loss[epoch-patience] + total_boundary_loss[epoch-patience]):\n",
    "                    hasntDecreased = True\n",
    "                        \n",
    "                if (lr_decay != -1) & hasntDecreased:\n",
    "                    lr = lr_decay*lr\n",
    "\n",
    "            # Decrease alpha each epoch\n",
    "            if if (alpha_decay != -1) & (alpha >= alpha_limit):\n",
    "                alpha = alpha_decay*alpha\n",
    "\n",
    "            # If the epoch is a multiple of 10, save to a checkpoint\n",
    "            if (epoch%10 == 0) & (save == True):\n",
    "                name = './ckpts/pinn_' + filename + '_epoch_' + str(epoch)\n",
    "                self.save_weights(name, overwrite=True, save_format=None, options=None)\n",
    "        \n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, batchsize):\n",
    "        P_size = P.shape[0]\n",
    "        steps_per_epoch = np.ceil(P_size / batchsize).astype(int)\n",
    "        predictions = np.zeros((P_size, 1))\n",
    "        \n",
    "        # For each step predict on data between start and end indices\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # Calculate end_idx\n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P_size - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Predict\n",
    "            predictions[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r):\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tfm.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tfm.divide(p, tfm.sqrt(tfm.square(p) + tfm.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tfm.reduce_mean(tfm.abs(f_r + (tfm.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 12:04:08.912629: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Current alpha: 0.9000, lr: 0.000300. Training losses: pinn: 0.000000, boundary: 930.456779, weighted total: 0.0837\n",
      "Epoch 1. Current alpha: 0.8982, lr: 0.000300. Training losses: pinn: 0.000000, boundary: 887.663490, weighted total: 0.0797\n",
      "Epoch 2. Current alpha: 0.8964, lr: 0.000300. Training losses: pinn: 0.000000, boundary: 895.732655, weighted total: 0.0803\n",
      "Epoch 3. Current alpha: 0.8946, lr: 0.000300. Training losses: pinn: 0.000001, boundary: 908.502922, weighted total: 0.0813\n",
      "Epoch 4. Current alpha: 0.8928, lr: 0.000300. Training losses: pinn: 0.000001, boundary: 870.246075, weighted total: 0.0777\n",
      "Epoch 5. Current alpha: 0.8910, lr: 0.000300. Training losses: pinn: 0.000001, boundary: 908.540840, weighted total: 0.0810\n",
      "Epoch 6. Current alpha: 0.8893, lr: 0.000300. Training losses: pinn: 0.000001, boundary: 863.616199, weighted total: 0.0768\n",
      "Epoch 7. Current alpha: 0.8875, lr: 0.000300. Training losses: pinn: 0.000001, boundary: 912.773079, weighted total: 0.0810\n",
      "Epoch 8. Current alpha: 0.8857, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 871.242550, weighted total: 0.0772\n",
      "Epoch 9. Current alpha: 0.8839, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 871.896210, weighted total: 0.0771\n",
      "Epoch 10. Current alpha: 0.8822, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 883.835991, weighted total: 0.0780\n",
      "Epoch 11. Current alpha: 0.8804, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 812.642670, weighted total: 0.0715\n",
      "Epoch 12. Current alpha: 0.8786, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 855.390022, weighted total: 0.0752\n",
      "Epoch 13. Current alpha: 0.8769, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 811.124107, weighted total: 0.0711\n",
      "Epoch 14. Current alpha: 0.8751, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 804.354630, weighted total: 0.0704\n",
      "Epoch 15. Current alpha: 0.8734, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 816.675564, weighted total: 0.0713\n",
      "Epoch 16. Current alpha: 0.8716, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 764.155094, weighted total: 0.0666\n",
      "Epoch 17. Current alpha: 0.8699, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 759.144501, weighted total: 0.0660\n",
      "Epoch 18. Current alpha: 0.8681, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 708.653570, weighted total: 0.0615\n",
      "Epoch 19. Current alpha: 0.8664, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 724.522844, weighted total: 0.0628\n",
      "Epoch 20. Current alpha: 0.8647, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 676.009264, weighted total: 0.0585\n",
      "Epoch 21. Current alpha: 0.8629, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 656.599785, weighted total: 0.0567\n",
      "Epoch 22. Current alpha: 0.8612, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 651.395782, weighted total: 0.0561\n",
      "Epoch 23. Current alpha: 0.8595, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 594.797606, weighted total: 0.0511\n",
      "Epoch 24. Current alpha: 0.8578, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 562.577637, weighted total: 0.0483\n",
      "Epoch 25. Current alpha: 0.8561, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 575.194996, weighted total: 0.0492\n",
      "Epoch 26. Current alpha: 0.8544, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 530.192450, weighted total: 0.0453\n",
      "Epoch 27. Current alpha: 0.8526, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 541.766636, weighted total: 0.0462\n",
      "Epoch 28. Current alpha: 0.8509, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 528.012617, weighted total: 0.0449\n",
      "Epoch 29. Current alpha: 0.8492, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 510.421745, weighted total: 0.0433\n",
      "Epoch 30. Current alpha: 0.8475, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 482.610390, weighted total: 0.0409\n",
      "Epoch 31. Current alpha: 0.8458, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 458.395195, weighted total: 0.0388\n",
      "Epoch 32. Current alpha: 0.8442, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 417.725372, weighted total: 0.0353\n",
      "Epoch 33. Current alpha: 0.8425, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 383.533092, weighted total: 0.0323\n",
      "Epoch 34. Current alpha: 0.8408, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 391.488423, weighted total: 0.0329\n",
      "Epoch 35. Current alpha: 0.8391, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 356.333113, weighted total: 0.0299\n",
      "Epoch 36. Current alpha: 0.8374, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 349.988217, weighted total: 0.0293\n",
      "Epoch 37. Current alpha: 0.8357, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 329.400543, weighted total: 0.0275\n",
      "Epoch 38. Current alpha: 0.8341, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 300.104327, weighted total: 0.0250\n",
      "Epoch 39. Current alpha: 0.8324, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 306.265151, weighted total: 0.0255\n",
      "Epoch 40. Current alpha: 0.8307, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 249.096903, weighted total: 0.0207\n",
      "Epoch 41. Current alpha: 0.8291, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 249.369609, weighted total: 0.0207\n",
      "Epoch 42. Current alpha: 0.8274, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 224.542249, weighted total: 0.0186\n",
      "Epoch 43. Current alpha: 0.8258, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 223.790863, weighted total: 0.0185\n",
      "Epoch 44. Current alpha: 0.8241, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 203.632221, weighted total: 0.0168\n",
      "Epoch 45. Current alpha: 0.8225, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 191.134549, weighted total: 0.0157\n",
      "Epoch 46. Current alpha: 0.8208, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 186.176076, weighted total: 0.0153\n",
      "Epoch 47. Current alpha: 0.8192, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 154.991721, weighted total: 0.0127\n",
      "Epoch 48. Current alpha: 0.8175, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 166.940223, weighted total: 0.0136\n",
      "Epoch 49. Current alpha: 0.8159, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 149.277444, weighted total: 0.0122\n",
      "Epoch 50. Current alpha: 0.8143, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 141.179420, weighted total: 0.0115\n",
      "Epoch 51. Current alpha: 0.8126, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 144.545382, weighted total: 0.0117\n",
      "Epoch 52. Current alpha: 0.8110, lr: 0.000300. Training losses: pinn: 0.000003, boundary: 123.730333, weighted total: 0.0100\n",
      "Epoch 53. Current alpha: 0.8094, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 129.504550, weighted total: 0.0105\n",
      "Epoch 54. Current alpha: 0.8078, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 111.641079, weighted total: 0.0090\n",
      "Epoch 55. Current alpha: 0.8062, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 104.270730, weighted total: 0.0084\n",
      "Epoch 56. Current alpha: 0.8045, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 102.967647, weighted total: 0.0083\n",
      "Epoch 57. Current alpha: 0.8029, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 94.088934, weighted total: 0.0076\n",
      "Epoch 58. Current alpha: 0.8013, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 92.334082, weighted total: 0.0074\n",
      "Epoch 59. Current alpha: 0.7997, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 83.018623, weighted total: 0.0066\n",
      "Epoch 60. Current alpha: 0.7981, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 85.211738, weighted total: 0.0068\n",
      "Epoch 61. Current alpha: 0.7965, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 74.212559, weighted total: 0.0059\n",
      "Epoch 62. Current alpha: 0.7949, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 75.745643, weighted total: 0.0060\n",
      "Epoch 63. Current alpha: 0.7934, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 74.313374, weighted total: 0.0059\n",
      "Epoch 64. Current alpha: 0.7918, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 75.176578, weighted total: 0.0060\n",
      "Epoch 65. Current alpha: 0.7902, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 66.663754, weighted total: 0.0053\n",
      "Epoch 66. Current alpha: 0.7886, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 62.267748, weighted total: 0.0049\n",
      "Epoch 67. Current alpha: 0.7870, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 63.525191, weighted total: 0.0050\n",
      "Epoch 68. Current alpha: 0.7855, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 58.216009, weighted total: 0.0046\n",
      "Epoch 69. Current alpha: 0.7839, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 65.062350, weighted total: 0.0051\n",
      "Epoch 70. Current alpha: 0.7823, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 61.627012, weighted total: 0.0048\n",
      "Epoch 71. Current alpha: 0.7807, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 57.936793, weighted total: 0.0045\n",
      "Epoch 72. Current alpha: 0.7792, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 57.007190, weighted total: 0.0044\n",
      "Epoch 73. Current alpha: 0.7776, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 48.579236, weighted total: 0.0038\n",
      "Epoch 74. Current alpha: 0.7761, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 43.544860, weighted total: 0.0034\n",
      "Epoch 75. Current alpha: 0.7745, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 44.907516, weighted total: 0.0035\n",
      "Epoch 76. Current alpha: 0.7730, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 45.214160, weighted total: 0.0035\n",
      "Epoch 77. Current alpha: 0.7714, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 46.661407, weighted total: 0.0036\n",
      "Epoch 78. Current alpha: 0.7699, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 40.896117, weighted total: 0.0031\n",
      "Epoch 79. Current alpha: 0.7683, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 42.159051, weighted total: 0.0032\n",
      "Epoch 80. Current alpha: 0.7668, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 42.119971, weighted total: 0.0032\n",
      "Epoch 81. Current alpha: 0.7653, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 45.780556, weighted total: 0.0035\n",
      "Epoch 82. Current alpha: 0.7637, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 42.060453, weighted total: 0.0032\n",
      "Epoch 83. Current alpha: 0.7622, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 38.875775, weighted total: 0.0030\n",
      "Epoch 84. Current alpha: 0.7607, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 31.754501, weighted total: 0.0024\n",
      "Epoch 85. Current alpha: 0.7592, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 34.109933, weighted total: 0.0026\n",
      "Epoch 86. Current alpha: 0.7577, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 38.480630, weighted total: 0.0029\n",
      "Epoch 87. Current alpha: 0.7561, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 46.085212, weighted total: 0.0035\n",
      "Epoch 88. Current alpha: 0.7546, lr: 0.000300. Training losses: pinn: 0.000002, boundary: 45.415850, weighted total: 0.0034\n",
      "Epoch 89. Current alpha: 0.7531, lr: 0.000285. Training losses: pinn: 0.000002, boundary: 39.459831, weighted total: 0.0030\n",
      "Epoch 90. Current alpha: 0.7516, lr: 0.000285. Training losses: pinn: 0.000002, boundary: 29.992296, weighted total: 0.0023\n",
      "Epoch 91. Current alpha: 0.7501, lr: 0.000285. Training losses: pinn: 0.000002, boundary: 48.324649, weighted total: 0.0036\n",
      "Epoch 92. Current alpha: 0.7486, lr: 0.000271. Training losses: pinn: 0.000002, boundary: 39.417632, weighted total: 0.0030\n",
      "Epoch 93. Current alpha: 0.7471, lr: 0.000271. Training losses: pinn: 0.000002, boundary: 36.491127, weighted total: 0.0027\n",
      "Epoch 94. Current alpha: 0.7456, lr: 0.000271. Training losses: pinn: 0.000002, boundary: 38.502181, weighted total: 0.0029\n",
      "Epoch 95. Current alpha: 0.7441, lr: 0.000257. Training losses: pinn: 0.000002, boundary: 41.523021, weighted total: 0.0031\n",
      "Epoch 96. Current alpha: 0.7426, lr: 0.000244. Training losses: pinn: 0.000002, boundary: 38.716645, weighted total: 0.0029\n",
      "Epoch 97. Current alpha: 0.7411, lr: 0.000232. Training losses: pinn: 0.000002, boundary: 28.681104, weighted total: 0.0021\n",
      "Epoch 98. Current alpha: 0.7397, lr: 0.000232. Training losses: pinn: 0.000002, boundary: 33.531902, weighted total: 0.0025\n",
      "Epoch 99. Current alpha: 0.7382, lr: 0.000232. Training losses: pinn: 0.000002, boundary: 35.219400, weighted total: 0.0026\n",
      "Epoch 100. Current alpha: 0.7367, lr: 0.000232. Training losses: pinn: 0.000002, boundary: 33.297266, weighted total: 0.0025\n",
      "Epoch 101. Current alpha: 0.7352, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 23.929646, weighted total: 0.0018\n",
      "Epoch 102. Current alpha: 0.7338, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 26.510637, weighted total: 0.0019\n",
      "Epoch 103. Current alpha: 0.7323, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 33.939220, weighted total: 0.0025\n",
      "Epoch 104. Current alpha: 0.7308, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 21.669888, weighted total: 0.0016\n",
      "Epoch 105. Current alpha: 0.7294, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 22.740859, weighted total: 0.0017\n",
      "Epoch 106. Current alpha: 0.7279, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 27.749085, weighted total: 0.0020\n",
      "Epoch 107. Current alpha: 0.7265, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 27.064375, weighted total: 0.0020\n",
      "Epoch 108. Current alpha: 0.7250, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 23.333353, weighted total: 0.0017\n",
      "Epoch 109. Current alpha: 0.7236, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 23.685944, weighted total: 0.0017\n",
      "Epoch 110. Current alpha: 0.7221, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 25.905479, weighted total: 0.0019\n",
      "Epoch 111. Current alpha: 0.7207, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 22.592735, weighted total: 0.0016\n",
      "Epoch 112. Current alpha: 0.7192, lr: 0.000221. Training losses: pinn: 0.000002, boundary: 35.103395, weighted total: 0.0025\n",
      "Epoch 113. Current alpha: 0.7178, lr: 0.000210. Training losses: pinn: 0.000002, boundary: 22.467865, weighted total: 0.0016\n",
      "Epoch 114. Current alpha: 0.7163, lr: 0.000210. Training losses: pinn: 0.000002, boundary: 20.278985, weighted total: 0.0015\n",
      "Epoch 115. Current alpha: 0.7149, lr: 0.000210. Training losses: pinn: 0.000002, boundary: 35.056810, weighted total: 0.0025\n",
      "Epoch 116. Current alpha: 0.7135, lr: 0.000199. Training losses: pinn: 0.000002, boundary: 26.713490, weighted total: 0.0019\n",
      "Epoch 117. Current alpha: 0.7121, lr: 0.000199. Training losses: pinn: 0.000002, boundary: 30.698829, weighted total: 0.0022\n",
      "Epoch 118. Current alpha: 0.7106, lr: 0.000189. Training losses: pinn: 0.000002, boundary: 22.878785, weighted total: 0.0016\n",
      "Epoch 119. Current alpha: 0.7092, lr: 0.000189. Training losses: pinn: 0.000002, boundary: 25.644541, weighted total: 0.0018\n",
      "Epoch 120. Current alpha: 0.7078, lr: 0.000180. Training losses: pinn: 0.000002, boundary: 29.035634, weighted total: 0.0021\n",
      "Epoch 121. Current alpha: 0.7064, lr: 0.000171. Training losses: pinn: 0.000002, boundary: 16.148732, weighted total: 0.0011\n",
      "Epoch 122. Current alpha: 0.7050, lr: 0.000171. Training losses: pinn: 0.000002, boundary: 26.408665, weighted total: 0.0019\n",
      "Epoch 123. Current alpha: 0.7036, lr: 0.000171. Training losses: pinn: 0.000002, boundary: 24.254278, weighted total: 0.0017\n",
      "Epoch 124. Current alpha: 0.7021, lr: 0.000162. Training losses: pinn: 0.000002, boundary: 23.961056, weighted total: 0.0017\n",
      "Epoch 125. Current alpha: 0.7007, lr: 0.000154. Training losses: pinn: 0.000002, boundary: 15.664856, weighted total: 0.0011\n",
      "Epoch 126. Current alpha: 0.6993, lr: 0.000154. Training losses: pinn: 0.000002, boundary: 14.941256, weighted total: 0.0010\n",
      "Epoch 127. Current alpha: 0.6979, lr: 0.000154. Training losses: pinn: 0.000002, boundary: 19.246345, weighted total: 0.0013\n",
      "Epoch 128. Current alpha: 0.6965, lr: 0.000154. Training losses: pinn: 0.000002, boundary: 19.279698, weighted total: 0.0013\n",
      "Epoch 129. Current alpha: 0.6952, lr: 0.000154. Training losses: pinn: 0.000002, boundary: 22.221459, weighted total: 0.0015\n",
      "Epoch 130. Current alpha: 0.6938, lr: 0.000154. Training losses: pinn: 0.000002, boundary: 21.476438, weighted total: 0.0015\n",
      "Epoch 131. Current alpha: 0.6924, lr: 0.000154. Training losses: pinn: 0.000002, boundary: 22.914434, weighted total: 0.0016\n",
      "Epoch 132. Current alpha: 0.6910, lr: 0.000146. Training losses: pinn: 0.000002, boundary: 15.952119, weighted total: 0.0011\n",
      "Epoch 133. Current alpha: 0.6896, lr: 0.000146. Training losses: pinn: 0.000002, boundary: 21.418716, weighted total: 0.0015\n",
      "Epoch 134. Current alpha: 0.6882, lr: 0.000146. Training losses: pinn: 0.000002, boundary: 18.265030, weighted total: 0.0013\n",
      "Epoch 135. Current alpha: 0.6869, lr: 0.000146. Training losses: pinn: 0.000002, boundary: 15.313513, weighted total: 0.0011\n",
      "Epoch 136. Current alpha: 0.6855, lr: 0.000146. Training losses: pinn: 0.000002, boundary: 15.788866, weighted total: 0.0011\n",
      "Epoch 137. Current alpha: 0.6841, lr: 0.000139. Training losses: pinn: 0.000002, boundary: 18.427248, weighted total: 0.0013\n",
      "Epoch 138. Current alpha: 0.6827, lr: 0.000139. Training losses: pinn: 0.000002, boundary: 20.033000, weighted total: 0.0014\n",
      "Epoch 139. Current alpha: 0.6814, lr: 0.000132. Training losses: pinn: 0.000002, boundary: 16.477825, weighted total: 0.0011\n",
      "Epoch 140. Current alpha: 0.6800, lr: 0.000132. Training losses: pinn: 0.000002, boundary: 11.523601, weighted total: 0.0008\n",
      "Epoch 141. Current alpha: 0.6787, lr: 0.000132. Training losses: pinn: 0.000002, boundary: 19.089284, weighted total: 0.0013\n",
      "Epoch 142. Current alpha: 0.6773, lr: 0.000132. Training losses: pinn: 0.000002, boundary: 11.536838, weighted total: 0.0008\n",
      "Epoch 143. Current alpha: 0.6759, lr: 0.000132. Training losses: pinn: 0.000002, boundary: 19.742068, weighted total: 0.0013\n",
      "Epoch 144. Current alpha: 0.6746, lr: 0.000132. Training losses: pinn: 0.000002, boundary: 23.648917, weighted total: 0.0016\n",
      "Epoch 145. Current alpha: 0.6732, lr: 0.000125. Training losses: pinn: 0.000002, boundary: 16.807826, weighted total: 0.0011\n",
      "Epoch 146. Current alpha: 0.6719, lr: 0.000119. Training losses: pinn: 0.000002, boundary: 20.087065, weighted total: 0.0014\n",
      "Epoch 147. Current alpha: 0.6706, lr: 0.000113. Training losses: pinn: 0.000002, boundary: 14.869805, weighted total: 0.0010\n",
      "Epoch 148. Current alpha: 0.6692, lr: 0.000113. Training losses: pinn: 0.000002, boundary: 16.907891, weighted total: 0.0011\n",
      "Epoch 149. Current alpha: 0.6679, lr: 0.000113. Training losses: pinn: 0.000002, boundary: 18.879690, weighted total: 0.0013\n",
      "Epoch 150. Current alpha: 0.6665, lr: 0.000108. Training losses: pinn: 0.000002, boundary: 15.837685, weighted total: 0.0011\n",
      "Epoch 151. Current alpha: 0.6652, lr: 0.000102. Training losses: pinn: 0.000002, boundary: 16.111333, weighted total: 0.0011\n",
      "Epoch 152. Current alpha: 0.6639, lr: 0.000102. Training losses: pinn: 0.000002, boundary: 14.598323, weighted total: 0.0010\n",
      "Epoch 153. Current alpha: 0.6625, lr: 0.000097. Training losses: pinn: 0.000002, boundary: 17.223758, weighted total: 0.0011\n",
      "Epoch 154. Current alpha: 0.6612, lr: 0.000097. Training losses: pinn: 0.000002, boundary: 12.251481, weighted total: 0.0008\n",
      "Epoch 155. Current alpha: 0.6599, lr: 0.000097. Training losses: pinn: 0.000002, boundary: 12.668374, weighted total: 0.0008\n",
      "Epoch 156. Current alpha: 0.6586, lr: 0.000097. Training losses: pinn: 0.000002, boundary: 11.304748, weighted total: 0.0007\n",
      "Epoch 157. Current alpha: 0.6573, lr: 0.000097. Training losses: pinn: 0.000002, boundary: 12.153802, weighted total: 0.0008\n",
      "Epoch 158. Current alpha: 0.6559, lr: 0.000097. Training losses: pinn: 0.000002, boundary: 11.215825, weighted total: 0.0007\n",
      "Epoch 159. Current alpha: 0.6546, lr: 0.000097. Training losses: pinn: 0.000002, boundary: 9.123720, weighted total: 0.0006\n",
      "Epoch 160. Current alpha: 0.6533, lr: 0.000097. Training losses: pinn: 0.000002, boundary: 17.823368, weighted total: 0.0012\n",
      "Epoch 161. Current alpha: 0.6520, lr: 0.000092. Training losses: pinn: 0.000002, boundary: 13.027182, weighted total: 0.0009\n",
      "Epoch 162. Current alpha: 0.6507, lr: 0.000092. Training losses: pinn: 0.000002, boundary: 10.541450, weighted total: 0.0007\n",
      "Epoch 163. Current alpha: 0.6494, lr: 0.000092. Training losses: pinn: 0.000002, boundary: 12.092848, weighted total: 0.0008\n",
      "Epoch 164. Current alpha: 0.6481, lr: 0.000092. Training losses: pinn: 0.000002, boundary: 15.242164, weighted total: 0.0010\n",
      "Epoch 165. Current alpha: 0.6468, lr: 0.000088. Training losses: pinn: 0.000002, boundary: 14.637112, weighted total: 0.0009\n",
      "Epoch 166. Current alpha: 0.6455, lr: 0.000083. Training losses: pinn: 0.000002, boundary: 11.142528, weighted total: 0.0007\n",
      "Epoch 167. Current alpha: 0.6442, lr: 0.000083. Training losses: pinn: 0.000002, boundary: 13.122302, weighted total: 0.0008\n",
      "Epoch 168. Current alpha: 0.6429, lr: 0.000079. Training losses: pinn: 0.000002, boundary: 9.770331, weighted total: 0.0006\n",
      "Epoch 169. Current alpha: 0.6417, lr: 0.000079. Training losses: pinn: 0.000002, boundary: 14.025990, weighted total: 0.0009\n",
      "Epoch 170. Current alpha: 0.6404, lr: 0.000075. Training losses: pinn: 0.000002, boundary: 9.971926, weighted total: 0.0006\n",
      "Epoch 171. Current alpha: 0.6391, lr: 0.000075. Training losses: pinn: 0.000002, boundary: 12.495665, weighted total: 0.0008\n",
      "Epoch 172. Current alpha: 0.6378, lr: 0.000075. Training losses: pinn: 0.000002, boundary: 11.612332, weighted total: 0.0007\n",
      "Epoch 173. Current alpha: 0.6365, lr: 0.000071. Training losses: pinn: 0.000002, boundary: 10.208827, weighted total: 0.0007\n",
      "Epoch 174. Current alpha: 0.6353, lr: 0.000071. Training losses: pinn: 0.000002, boundary: 10.599751, weighted total: 0.0007\n",
      "Epoch 175. Current alpha: 0.6340, lr: 0.000071. Training losses: pinn: 0.000002, boundary: 10.760306, weighted total: 0.0007\n",
      "Epoch 176. Current alpha: 0.6327, lr: 0.000071. Training losses: pinn: 0.000002, boundary: 9.513580, weighted total: 0.0006\n",
      "Epoch 177. Current alpha: 0.6315, lr: 0.000071. Training losses: pinn: 0.000002, boundary: 13.271240, weighted total: 0.0008\n",
      "Epoch 178. Current alpha: 0.6302, lr: 0.000068. Training losses: pinn: 0.000002, boundary: 9.225656, weighted total: 0.0006\n",
      "Epoch 179. Current alpha: 0.6289, lr: 0.000068. Training losses: pinn: 0.000002, boundary: 8.933318, weighted total: 0.0006\n",
      "Epoch 180. Current alpha: 0.6277, lr: 0.000068. Training losses: pinn: 0.000002, boundary: 9.434616, weighted total: 0.0006\n",
      "Epoch 181. Current alpha: 0.6264, lr: 0.000068. Training losses: pinn: 0.000002, boundary: 7.730695, weighted total: 0.0005\n",
      "Epoch 182. Current alpha: 0.6252, lr: 0.000068. Training losses: pinn: 0.000002, boundary: 9.617930, weighted total: 0.0006\n",
      "Epoch 183. Current alpha: 0.6239, lr: 0.000068. Training losses: pinn: 0.000002, boundary: 9.032604, weighted total: 0.0006\n",
      "Epoch 184. Current alpha: 0.6227, lr: 0.000068. Training losses: pinn: 0.000002, boundary: 12.022552, weighted total: 0.0007\n",
      "Epoch 185. Current alpha: 0.6214, lr: 0.000064. Training losses: pinn: 0.000002, boundary: 10.340835, weighted total: 0.0006\n",
      "Epoch 186. Current alpha: 0.6202, lr: 0.000064. Training losses: pinn: 0.000002, boundary: 8.274916, weighted total: 0.0005\n",
      "Epoch 187. Current alpha: 0.6189, lr: 0.000064. Training losses: pinn: 0.000002, boundary: 8.433824, weighted total: 0.0005\n",
      "Epoch 188. Current alpha: 0.6177, lr: 0.000064. Training losses: pinn: 0.000002, boundary: 8.516193, weighted total: 0.0005\n",
      "Epoch 189. Current alpha: 0.6165, lr: 0.000064. Training losses: pinn: 0.000002, boundary: 9.882782, weighted total: 0.0006\n",
      "Epoch 190. Current alpha: 0.6152, lr: 0.000061. Training losses: pinn: 0.000002, boundary: 9.261841, weighted total: 0.0006\n",
      "Epoch 191. Current alpha: 0.6140, lr: 0.000061. Training losses: pinn: 0.000002, boundary: 10.815984, weighted total: 0.0007\n",
      "Epoch 192. Current alpha: 0.6128, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 8.556527, weighted total: 0.0005\n",
      "Epoch 193. Current alpha: 0.6116, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 8.471936, weighted total: 0.0005\n",
      "Epoch 194. Current alpha: 0.6103, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 10.260115, weighted total: 0.0006\n",
      "Epoch 195. Current alpha: 0.6091, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 6.938225, weighted total: 0.0004\n",
      "Epoch 196. Current alpha: 0.6079, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 6.116910, weighted total: 0.0004\n",
      "Epoch 197. Current alpha: 0.6067, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 7.798268, weighted total: 0.0005\n",
      "Epoch 198. Current alpha: 0.6055, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 6.086086, weighted total: 0.0004\n",
      "Epoch 199. Current alpha: 0.6043, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 8.551602, weighted total: 0.0005\n",
      "Epoch 200. Current alpha: 0.6030, lr: 0.000058. Training losses: pinn: 0.000002, boundary: 9.881621, weighted total: 0.0006\n",
      "Epoch 201. Current alpha: 0.6018, lr: 0.000055. Training losses: pinn: 0.000002, boundary: 8.169023, weighted total: 0.0005\n",
      "Epoch 202. Current alpha: 0.6006, lr: 0.000055. Training losses: pinn: 0.000002, boundary: 8.712247, weighted total: 0.0005\n",
      "Epoch 203. Current alpha: 0.5994, lr: 0.000052. Training losses: pinn: 0.000002, boundary: 7.724104, weighted total: 0.0005\n",
      "Epoch 204. Current alpha: 0.5982, lr: 0.000052. Training losses: pinn: 0.000002, boundary: 8.747317, weighted total: 0.0005\n",
      "Epoch 205. Current alpha: 0.5970, lr: 0.000052. Training losses: pinn: 0.000002, boundary: 7.547283, weighted total: 0.0005\n",
      "Epoch 206. Current alpha: 0.5958, lr: 0.000050. Training losses: pinn: 0.000002, boundary: 8.508277, weighted total: 0.0005\n",
      "Epoch 207. Current alpha: 0.5947, lr: 0.000047. Training losses: pinn: 0.000002, boundary: 7.042752, weighted total: 0.0004\n",
      "Epoch 208. Current alpha: 0.5935, lr: 0.000047. Training losses: pinn: 0.000002, boundary: 7.910082, weighted total: 0.0005\n",
      "Epoch 209. Current alpha: 0.5923, lr: 0.000045. Training losses: pinn: 0.000002, boundary: 6.600140, weighted total: 0.0004\n",
      "Epoch 210. Current alpha: 0.5911, lr: 0.000045. Training losses: pinn: 0.000002, boundary: 7.863118, weighted total: 0.0005\n",
      "Epoch 211. Current alpha: 0.5899, lr: 0.000045. Training losses: pinn: 0.000002, boundary: 7.317952, weighted total: 0.0004\n",
      "Epoch 212. Current alpha: 0.5887, lr: 0.000045. Training losses: pinn: 0.000002, boundary: 6.741926, weighted total: 0.0004\n",
      "Epoch 213. Current alpha: 0.5876, lr: 0.000045. Training losses: pinn: 0.000002, boundary: 5.878015, weighted total: 0.0003\n",
      "Epoch 214. Current alpha: 0.5864, lr: 0.000045. Training losses: pinn: 0.000002, boundary: 6.735785, weighted total: 0.0004\n",
      "Epoch 215. Current alpha: 0.5852, lr: 0.000045. Training losses: pinn: 0.000002, boundary: 8.223437, weighted total: 0.0005\n",
      "Epoch 216. Current alpha: 0.5840, lr: 0.000043. Training losses: pinn: 0.000002, boundary: 6.269168, weighted total: 0.0004\n",
      "Epoch 217. Current alpha: 0.5829, lr: 0.000043. Training losses: pinn: 0.000002, boundary: 7.284110, weighted total: 0.0004\n",
      "Epoch 218. Current alpha: 0.5817, lr: 0.000041. Training losses: pinn: 0.000002, boundary: 5.434044, weighted total: 0.0003\n",
      "Epoch 219. Current alpha: 0.5805, lr: 0.000041. Training losses: pinn: 0.000002, boundary: 5.770960, weighted total: 0.0003\n",
      "Epoch 220. Current alpha: 0.5794, lr: 0.000041. Training losses: pinn: 0.000002, boundary: 6.039513, weighted total: 0.0004\n",
      "Epoch 221. Current alpha: 0.5782, lr: 0.000041. Training losses: pinn: 0.000002, boundary: 6.099109, weighted total: 0.0004\n",
      "Epoch 222. Current alpha: 0.5771, lr: 0.000041. Training losses: pinn: 0.000002, boundary: 7.748304, weighted total: 0.0004\n",
      "Epoch 223. Current alpha: 0.5759, lr: 0.000039. Training losses: pinn: 0.000002, boundary: 6.334891, weighted total: 0.0004\n",
      "Epoch 224. Current alpha: 0.5748, lr: 0.000037. Training losses: pinn: 0.000002, boundary: 5.428267, weighted total: 0.0003\n",
      "Epoch 225. Current alpha: 0.5736, lr: 0.000037. Training losses: pinn: 0.000002, boundary: 5.828138, weighted total: 0.0003\n",
      "Epoch 226. Current alpha: 0.5725, lr: 0.000037. Training losses: pinn: 0.000002, boundary: 4.220557, weighted total: 0.0002\n",
      "Epoch 227. Current alpha: 0.5713, lr: 0.000037. Training losses: pinn: 0.000002, boundary: 5.542328, weighted total: 0.0003\n",
      "Epoch 228. Current alpha: 0.5702, lr: 0.000037. Training losses: pinn: 0.000002, boundary: 4.725289, weighted total: 0.0003\n",
      "Epoch 229. Current alpha: 0.5690, lr: 0.000037. Training losses: pinn: 0.000002, boundary: 6.424405, weighted total: 0.0004\n",
      "Epoch 230. Current alpha: 0.5679, lr: 0.000035. Training losses: pinn: 0.000002, boundary: 6.041751, weighted total: 0.0003\n",
      "Epoch 231. Current alpha: 0.5668, lr: 0.000033. Training losses: pinn: 0.000002, boundary: 4.473309, weighted total: 0.0003\n",
      "Epoch 232. Current alpha: 0.5656, lr: 0.000033. Training losses: pinn: 0.000002, boundary: 4.972988, weighted total: 0.0003\n",
      "Epoch 233. Current alpha: 0.5645, lr: 0.000033. Training losses: pinn: 0.000002, boundary: 4.980439, weighted total: 0.0003\n",
      "Epoch 234. Current alpha: 0.5634, lr: 0.000033. Training losses: pinn: 0.000002, boundary: 4.379539, weighted total: 0.0002\n",
      "Epoch 235. Current alpha: 0.5622, lr: 0.000033. Training losses: pinn: 0.000002, boundary: 5.096488, weighted total: 0.0003\n",
      "Epoch 236. Current alpha: 0.5611, lr: 0.000033. Training losses: pinn: 0.000002, boundary: 5.366979, weighted total: 0.0003\n",
      "Epoch 237. Current alpha: 0.5600, lr: 0.000031. Training losses: pinn: 0.000002, boundary: 4.565886, weighted total: 0.0003\n",
      "Epoch 238. Current alpha: 0.5589, lr: 0.000031. Training losses: pinn: 0.000002, boundary: 5.882064, weighted total: 0.0003\n",
      "Epoch 239. Current alpha: 0.5578, lr: 0.000030. Training losses: pinn: 0.000002, boundary: 4.870145, weighted total: 0.0003\n",
      "Epoch 240. Current alpha: 0.5566, lr: 0.000030. Training losses: pinn: 0.000002, boundary: 5.822603, weighted total: 0.0003\n",
      "Epoch 241. Current alpha: 0.5555, lr: 0.000030. Training losses: pinn: 0.000002, boundary: 5.468236, weighted total: 0.0003\n",
      "Epoch 242. Current alpha: 0.5544, lr: 0.000028. Training losses: pinn: 0.000002, boundary: 5.475893, weighted total: 0.0003\n",
      "Epoch 243. Current alpha: 0.5533, lr: 0.000027. Training losses: pinn: 0.000002, boundary: 4.380932, weighted total: 0.0002\n",
      "Epoch 244. Current alpha: 0.5522, lr: 0.000027. Training losses: pinn: 0.000002, boundary: 5.034384, weighted total: 0.0003\n",
      "Epoch 245. Current alpha: 0.5511, lr: 0.000026. Training losses: pinn: 0.000002, boundary: 3.761588, weighted total: 0.0002\n",
      "Epoch 246. Current alpha: 0.5500, lr: 0.000026. Training losses: pinn: 0.000002, boundary: 4.844707, weighted total: 0.0003\n",
      "Epoch 247. Current alpha: 0.5489, lr: 0.000026. Training losses: pinn: 0.000002, boundary: 4.690104, weighted total: 0.0003\n",
      "Epoch 248. Current alpha: 0.5478, lr: 0.000024. Training losses: pinn: 0.000002, boundary: 4.684274, weighted total: 0.0003\n",
      "Epoch 249. Current alpha: 0.5467, lr: 0.000024. Training losses: pinn: 0.000002, boundary: 4.102942, weighted total: 0.0002\n",
      "Epoch 250. Current alpha: 0.5456, lr: 0.000024. Training losses: pinn: 0.000002, boundary: 3.891017, weighted total: 0.0002\n",
      "Epoch 251. Current alpha: 0.5445, lr: 0.000024. Training losses: pinn: 0.000002, boundary: 3.802940, weighted total: 0.0002\n",
      "Epoch 252. Current alpha: 0.5434, lr: 0.000024. Training losses: pinn: 0.000002, boundary: 4.207725, weighted total: 0.0002\n",
      "Epoch 253. Current alpha: 0.5423, lr: 0.000024. Training losses: pinn: 0.000002, boundary: 3.868387, weighted total: 0.0002\n",
      "Epoch 254. Current alpha: 0.5413, lr: 0.000024. Training losses: pinn: 0.000002, boundary: 3.882120, weighted total: 0.0002\n",
      "Epoch 255. Current alpha: 0.5402, lr: 0.000024. Training losses: pinn: 0.000002, boundary: 4.470931, weighted total: 0.0002\n",
      "Epoch 256. Current alpha: 0.5391, lr: 0.000023. Training losses: pinn: 0.000002, boundary: 4.065627, weighted total: 0.0002\n",
      "Epoch 257. Current alpha: 0.5380, lr: 0.000023. Training losses: pinn: 0.000002, boundary: 3.916673, weighted total: 0.0002\n",
      "Epoch 258. Current alpha: 0.5369, lr: 0.000023. Training losses: pinn: 0.000002, boundary: 4.062705, weighted total: 0.0002\n",
      "Epoch 259. Current alpha: 0.5359, lr: 0.000023. Training losses: pinn: 0.000002, boundary: 3.882908, weighted total: 0.0002\n",
      "Epoch 260. Current alpha: 0.5348, lr: 0.000023. Training losses: pinn: 0.000002, boundary: 3.493385, weighted total: 0.0002\n",
      "Epoch 261. Current alpha: 0.5337, lr: 0.000023. Training losses: pinn: 0.000002, boundary: 3.754405, weighted total: 0.0002\n",
      "Epoch 262. Current alpha: 0.5327, lr: 0.000023. Training losses: pinn: 0.000002, boundary: 3.828951, weighted total: 0.0002\n",
      "Epoch 263. Current alpha: 0.5316, lr: 0.000023. Training losses: pinn: 0.000002, boundary: 3.874745, weighted total: 0.0002\n",
      "Epoch 264. Current alpha: 0.5305, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.708270, weighted total: 0.0002\n",
      "Epoch 265. Current alpha: 0.5295, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.467351, weighted total: 0.0002\n",
      "Epoch 266. Current alpha: 0.5284, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.471712, weighted total: 0.0002\n",
      "Epoch 267. Current alpha: 0.5273, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.849586, weighted total: 0.0002\n",
      "Epoch 268. Current alpha: 0.5263, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.805068, weighted total: 0.0002\n",
      "Epoch 269. Current alpha: 0.5252, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.563775, weighted total: 0.0002\n",
      "Epoch 270. Current alpha: 0.5242, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.303169, weighted total: 0.0002\n",
      "Epoch 271. Current alpha: 0.5231, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.426607, weighted total: 0.0002\n",
      "Epoch 272. Current alpha: 0.5221, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.497016, weighted total: 0.0002\n",
      "Epoch 273. Current alpha: 0.5211, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.429148, weighted total: 0.0002\n",
      "Epoch 274. Current alpha: 0.5200, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.154823, weighted total: 0.0002\n",
      "Epoch 275. Current alpha: 0.5190, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.266578, weighted total: 0.0002\n",
      "Epoch 276. Current alpha: 0.5179, lr: 0.000022. Training losses: pinn: 0.000002, boundary: 3.596317, weighted total: 0.0002\n",
      "Epoch 277. Current alpha: 0.5169, lr: 0.000021. Training losses: pinn: 0.000002, boundary: 3.373057, weighted total: 0.0002\n",
      "Epoch 278. Current alpha: 0.5159, lr: 0.000021. Training losses: pinn: 0.000002, boundary: 2.970662, weighted total: 0.0002\n",
      "Epoch 279. Current alpha: 0.5148, lr: 0.000021. Training losses: pinn: 0.000002, boundary: 3.682468, weighted total: 0.0002\n",
      "Epoch 280. Current alpha: 0.5138, lr: 0.000020. Training losses: pinn: 0.000002, boundary: 3.330354, weighted total: 0.0002\n",
      "Epoch 281. Current alpha: 0.5128, lr: 0.000019. Training losses: pinn: 0.000002, boundary: 3.095255, weighted total: 0.0002\n",
      "Epoch 282. Current alpha: 0.5117, lr: 0.000019. Training losses: pinn: 0.000002, boundary: 3.494829, weighted total: 0.0002\n",
      "Epoch 283. Current alpha: 0.5107, lr: 0.000019. Training losses: pinn: 0.000002, boundary: 3.961916, weighted total: 0.0002\n",
      "Epoch 284. Current alpha: 0.5097, lr: 0.000018. Training losses: pinn: 0.000002, boundary: 3.402161, weighted total: 0.0002\n",
      "Epoch 285. Current alpha: 0.5087, lr: 0.000017. Training losses: pinn: 0.000002, boundary: 3.496126, weighted total: 0.0002\n",
      "Epoch 286. Current alpha: 0.5077, lr: 0.000016. Training losses: pinn: 0.000002, boundary: 3.126856, weighted total: 0.0002\n",
      "Epoch 287. Current alpha: 0.5066, lr: 0.000016. Training losses: pinn: 0.000002, boundary: 3.394627, weighted total: 0.0002\n",
      "Epoch 288. Current alpha: 0.5056, lr: 0.000015. Training losses: pinn: 0.000002, boundary: 3.664857, weighted total: 0.0002\n",
      "Epoch 289. Current alpha: 0.5046, lr: 0.000015. Training losses: pinn: 0.000002, boundary: 3.312637, weighted total: 0.0002\n",
      "Epoch 290. Current alpha: 0.5036, lr: 0.000015. Training losses: pinn: 0.000002, boundary: 2.754712, weighted total: 0.0001\n",
      "Epoch 291. Current alpha: 0.5026, lr: 0.000015. Training losses: pinn: 0.000002, boundary: 3.364006, weighted total: 0.0002\n",
      "Epoch 292. Current alpha: 0.5016, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 3.122488, weighted total: 0.0002\n",
      "Epoch 293. Current alpha: 0.5006, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 2.763328, weighted total: 0.0001\n",
      "Epoch 294. Current alpha: 0.4996, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 2.933061, weighted total: 0.0001\n",
      "Epoch 295. Current alpha: 0.4986, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 2.689880, weighted total: 0.0001\n",
      "Epoch 296. Current alpha: 0.4976, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 2.945329, weighted total: 0.0001\n",
      "Epoch 297. Current alpha: 0.4966, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 3.014736, weighted total: 0.0002\n",
      "Epoch 298. Current alpha: 0.4956, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 3.033207, weighted total: 0.0002\n",
      "Epoch 299. Current alpha: 0.4946, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 2.764473, weighted total: 0.0001\n",
      "Epoch 300. Current alpha: 0.4936, lr: 0.000014. Training losses: pinn: 0.000002, boundary: 3.139182, weighted total: 0.0002\n",
      "Epoch 301. Current alpha: 0.4926, lr: 0.000013. Training losses: pinn: 0.000002, boundary: 2.702806, weighted total: 0.0001\n",
      "Epoch 302. Current alpha: 0.4917, lr: 0.000013. Training losses: pinn: 0.000002, boundary: 2.601573, weighted total: 0.0001\n",
      "Epoch 303. Current alpha: 0.4907, lr: 0.000013. Training losses: pinn: 0.000002, boundary: 2.564445, weighted total: 0.0001\n",
      "Epoch 304. Current alpha: 0.4897, lr: 0.000013. Training losses: pinn: 0.000002, boundary: 3.072557, weighted total: 0.0002\n",
      "Epoch 305. Current alpha: 0.4887, lr: 0.000012. Training losses: pinn: 0.000002, boundary: 3.081276, weighted total: 0.0002\n",
      "Epoch 306. Current alpha: 0.4877, lr: 0.000012. Training losses: pinn: 0.000002, boundary: 3.098561, weighted total: 0.0002\n",
      "Epoch 307. Current alpha: 0.4868, lr: 0.000011. Training losses: pinn: 0.000002, boundary: 2.993791, weighted total: 0.0001\n",
      "Epoch 308. Current alpha: 0.4858, lr: 0.000011. Training losses: pinn: 0.000002, boundary: 2.761920, weighted total: 0.0001\n",
      "Epoch 309. Current alpha: 0.4848, lr: 0.000011. Training losses: pinn: 0.000002, boundary: 2.475955, weighted total: 0.0001\n",
      "Epoch 310. Current alpha: 0.4838, lr: 0.000011. Training losses: pinn: 0.000002, boundary: 3.205909, weighted total: 0.0002\n",
      "Epoch 311. Current alpha: 0.4829, lr: 0.000011. Training losses: pinn: 0.000002, boundary: 2.619231, weighted total: 0.0001\n",
      "Epoch 312. Current alpha: 0.4819, lr: 0.000011. Training losses: pinn: 0.000002, boundary: 2.679898, weighted total: 0.0001\n",
      "Epoch 313. Current alpha: 0.4810, lr: 0.000010. Training losses: pinn: 0.000002, boundary: 2.043257, weighted total: 0.0001\n",
      "Epoch 314. Current alpha: 0.4800, lr: 0.000010. Training losses: pinn: 0.000002, boundary: 2.527098, weighted total: 0.0001\n",
      "Epoch 315. Current alpha: 0.4790, lr: 0.000010. Training losses: pinn: 0.000002, boundary: 2.661418, weighted total: 0.0001\n",
      "Epoch 316. Current alpha: 0.4781, lr: 0.000010. Training losses: pinn: 0.000002, boundary: 2.476478, weighted total: 0.0001\n",
      "Epoch 317. Current alpha: 0.4771, lr: 0.000010. Training losses: pinn: 0.000002, boundary: 2.448112, weighted total: 0.0001\n",
      "Epoch 318. Current alpha: 0.4762, lr: 0.000010. Training losses: pinn: 0.000002, boundary: 2.849648, weighted total: 0.0001\n",
      "Epoch 319. Current alpha: 0.4752, lr: 0.000010. Training losses: pinn: 0.000002, boundary: 2.520088, weighted total: 0.0001\n",
      "Epoch 320. Current alpha: 0.4743, lr: 0.000009. Training losses: pinn: 0.000002, boundary: 2.561062, weighted total: 0.0001\n",
      "Epoch 321. Current alpha: 0.4733, lr: 0.000009. Training losses: pinn: 0.000002, boundary: 2.554767, weighted total: 0.0001\n",
      "Epoch 322. Current alpha: 0.4724, lr: 0.000009. Training losses: pinn: 0.000002, boundary: 2.399238, weighted total: 0.0001\n",
      "Epoch 323. Current alpha: 0.4714, lr: 0.000009. Training losses: pinn: 0.000002, boundary: 2.599461, weighted total: 0.0001\n",
      "Epoch 324. Current alpha: 0.4705, lr: 0.000009. Training losses: pinn: 0.000002, boundary: 2.683730, weighted total: 0.0001\n",
      "Epoch 325. Current alpha: 0.4695, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.564506, weighted total: 0.0001\n",
      "Epoch 326. Current alpha: 0.4686, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.300357, weighted total: 0.0001\n",
      "Epoch 327. Current alpha: 0.4677, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.209546, weighted total: 0.0001\n",
      "Epoch 328. Current alpha: 0.4667, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.230374, weighted total: 0.0001\n",
      "Epoch 329. Current alpha: 0.4658, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.771097, weighted total: 0.0001\n",
      "Epoch 330. Current alpha: 0.4649, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.064450, weighted total: 0.0001\n",
      "Epoch 331. Current alpha: 0.4639, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.523600, weighted total: 0.0001\n",
      "Epoch 332. Current alpha: 0.4630, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.170816, weighted total: 0.0001\n",
      "Epoch 333. Current alpha: 0.4621, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.295798, weighted total: 0.0001\n",
      "Epoch 334. Current alpha: 0.4612, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.279202, weighted total: 0.0001\n",
      "Epoch 335. Current alpha: 0.4602, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.296843, weighted total: 0.0001\n",
      "Epoch 336. Current alpha: 0.4593, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.228208, weighted total: 0.0001\n",
      "Epoch 337. Current alpha: 0.4584, lr: 0.000008. Training losses: pinn: 0.000002, boundary: 2.565828, weighted total: 0.0001\n",
      "Epoch 338. Current alpha: 0.4575, lr: 0.000007. Training losses: pinn: 0.000002, boundary: 2.578275, weighted total: 0.0001\n",
      "Epoch 339. Current alpha: 0.4566, lr: 0.000007. Training losses: pinn: 0.000002, boundary: 2.280125, weighted total: 0.0001\n",
      "Epoch 340. Current alpha: 0.4556, lr: 0.000007. Training losses: pinn: 0.000002, boundary: 2.018850, weighted total: 0.0001\n",
      "Epoch 341. Current alpha: 0.4547, lr: 0.000007. Training losses: pinn: 0.000002, boundary: 2.357550, weighted total: 0.0001\n",
      "Epoch 342. Current alpha: 0.4538, lr: 0.000007. Training losses: pinn: 0.000002, boundary: 2.172732, weighted total: 0.0001\n",
      "Epoch 343. Current alpha: 0.4529, lr: 0.000007. Training losses: pinn: 0.000002, boundary: 2.466405, weighted total: 0.0001\n",
      "Epoch 344. Current alpha: 0.4520, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 1.958772, weighted total: 0.0001\n",
      "Epoch 345. Current alpha: 0.4511, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 2.127341, weighted total: 0.0001\n",
      "Epoch 346. Current alpha: 0.4502, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 2.090424, weighted total: 0.0001\n",
      "Epoch 347. Current alpha: 0.4493, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 2.169184, weighted total: 0.0001\n",
      "Epoch 348. Current alpha: 0.4484, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 2.226995, weighted total: 0.0001\n",
      "Epoch 349. Current alpha: 0.4475, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 2.347843, weighted total: 0.0001\n",
      "Epoch 350. Current alpha: 0.4466, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 2.351865, weighted total: 0.0001\n",
      "Epoch 351. Current alpha: 0.4457, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 2.199980, weighted total: 0.0001\n",
      "Epoch 352. Current alpha: 0.4448, lr: 0.000006. Training losses: pinn: 0.000002, boundary: 2.273709, weighted total: 0.0001\n",
      "Epoch 353. Current alpha: 0.4439, lr: 0.000005. Training losses: pinn: 0.000002, boundary: 2.021094, weighted total: 0.0001\n",
      "Epoch 354. Current alpha: 0.4431, lr: 0.000005. Training losses: pinn: 0.000002, boundary: 2.170541, weighted total: 0.0001\n",
      "Epoch 355. Current alpha: 0.4422, lr: 0.000005. Training losses: pinn: 0.000002, boundary: 1.966640, weighted total: 0.0001\n",
      "Epoch 356. Current alpha: 0.4413, lr: 0.000005. Training losses: pinn: 0.000002, boundary: 2.182460, weighted total: 0.0001\n",
      "Epoch 357. Current alpha: 0.4404, lr: 0.000005. Training losses: pinn: 0.000002, boundary: 2.264693, weighted total: 0.0001\n",
      "Epoch 358. Current alpha: 0.4395, lr: 0.000005. Training losses: pinn: 0.000002, boundary: 2.052998, weighted total: 0.0001\n",
      "Epoch 359. Current alpha: 0.4386, lr: 0.000005. Training losses: pinn: 0.000002, boundary: 2.368104, weighted total: 0.0001\n",
      "Epoch 360. Current alpha: 0.4378, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.071553, weighted total: 0.0001\n",
      "Epoch 361. Current alpha: 0.4369, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.969785, weighted total: 0.0001\n",
      "Epoch 362. Current alpha: 0.4360, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.146157, weighted total: 0.0001\n",
      "Epoch 363. Current alpha: 0.4351, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.731608, weighted total: 0.0001\n",
      "Epoch 364. Current alpha: 0.4343, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.929378, weighted total: 0.0001\n",
      "Epoch 365. Current alpha: 0.4334, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.099950, weighted total: 0.0001\n",
      "Epoch 366. Current alpha: 0.4325, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.166248, weighted total: 0.0001\n",
      "Epoch 367. Current alpha: 0.4317, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.028636, weighted total: 0.0001\n",
      "Epoch 368. Current alpha: 0.4308, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.945740, weighted total: 0.0001\n",
      "Epoch 369. Current alpha: 0.4299, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.079382, weighted total: 0.0001\n",
      "Epoch 370. Current alpha: 0.4291, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.937048, weighted total: 0.0001\n",
      "Epoch 371. Current alpha: 0.4282, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.956000, weighted total: 0.0001\n",
      "Epoch 372. Current alpha: 0.4274, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.980821, weighted total: 0.0001\n",
      "Epoch 373. Current alpha: 0.4265, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.167258, weighted total: 0.0001\n",
      "Epoch 374. Current alpha: 0.4257, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.418122, weighted total: 0.0001\n",
      "Epoch 375. Current alpha: 0.4248, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.039500, weighted total: 0.0001\n",
      "Epoch 376. Current alpha: 0.4240, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.274441, weighted total: 0.0001\n",
      "Epoch 377. Current alpha: 0.4231, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.839105, weighted total: 0.0001\n",
      "Epoch 378. Current alpha: 0.4223, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.709441, weighted total: 0.0001\n",
      "Epoch 379. Current alpha: 0.4214, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.697702, weighted total: 0.0001\n",
      "Epoch 380. Current alpha: 0.4206, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.746819, weighted total: 0.0001\n",
      "Epoch 381. Current alpha: 0.4197, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.937923, weighted total: 0.0001\n",
      "Epoch 382. Current alpha: 0.4189, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.829032, weighted total: 0.0001\n",
      "Epoch 383. Current alpha: 0.4181, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.123504, weighted total: 0.0001\n",
      "Epoch 384. Current alpha: 0.4172, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.082767, weighted total: 0.0001\n",
      "Epoch 385. Current alpha: 0.4164, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 2.039224, weighted total: 0.0001\n",
      "Epoch 386. Current alpha: 0.4156, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.995324, weighted total: 0.0001\n",
      "Epoch 387. Current alpha: 0.4147, lr: 0.000004. Training losses: pinn: 0.000002, boundary: 1.960114, weighted total: 0.0001\n",
      "Epoch 388. Current alpha: 0.4139, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 1.806364, weighted total: 0.0001\n",
      "Epoch 389. Current alpha: 0.4131, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 1.913575, weighted total: 0.0001\n",
      "Epoch 390. Current alpha: 0.4122, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 2.042631, weighted total: 0.0001\n",
      "Epoch 391. Current alpha: 0.4114, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 2.173927, weighted total: 0.0001\n",
      "Epoch 392. Current alpha: 0.4106, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 2.099265, weighted total: 0.0001\n",
      "Epoch 393. Current alpha: 0.4098, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 1.770264, weighted total: 0.0001\n",
      "Epoch 394. Current alpha: 0.4090, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 1.853376, weighted total: 0.0001\n",
      "Epoch 395. Current alpha: 0.4081, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 1.808536, weighted total: 0.0001\n",
      "Epoch 396. Current alpha: 0.4073, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 2.103103, weighted total: 0.0001\n",
      "Epoch 397. Current alpha: 0.4065, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 1.694364, weighted total: 0.0001\n",
      "Epoch 398. Current alpha: 0.4057, lr: 0.000003. Training losses: pinn: 0.000002, boundary: 2.131520, weighted total: 0.0001\n",
      "Epoch 399. Current alpha: 0.4049, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.799243, weighted total: 0.0001\n",
      "Epoch 400. Current alpha: 0.4041, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.953706, weighted total: 0.0001\n",
      "Epoch 401. Current alpha: 0.4033, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.731041, weighted total: 0.0001\n",
      "Epoch 402. Current alpha: 0.4025, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.869728, weighted total: 0.0001\n",
      "Epoch 403. Current alpha: 0.4017, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.664721, weighted total: 0.0001\n",
      "Epoch 404. Current alpha: 0.4008, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.779395, weighted total: 0.0001\n",
      "Epoch 405. Current alpha: 0.4000, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.717958, weighted total: 0.0001\n",
      "Epoch 406. Current alpha: 0.3992, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.822700, weighted total: 0.0001\n",
      "Epoch 407. Current alpha: 0.3984, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 2.164219, weighted total: 0.0001\n",
      "Epoch 408. Current alpha: 0.3977, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 2.179436, weighted total: 0.0001\n",
      "Epoch 409. Current alpha: 0.3969, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.957531, weighted total: 0.0001\n",
      "Epoch 410. Current alpha: 0.3961, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.834278, weighted total: 0.0001\n",
      "Epoch 411. Current alpha: 0.3953, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 2.344290, weighted total: 0.0001\n",
      "Epoch 412. Current alpha: 0.3945, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.828249, weighted total: 0.0001\n",
      "Epoch 413. Current alpha: 0.3937, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.649142, weighted total: 0.0001\n",
      "Epoch 414. Current alpha: 0.3929, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.903152, weighted total: 0.0001\n",
      "Epoch 415. Current alpha: 0.3921, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.905920, weighted total: 0.0001\n",
      "Epoch 416. Current alpha: 0.3913, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.885653, weighted total: 0.0001\n",
      "Epoch 417. Current alpha: 0.3906, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.811670, weighted total: 0.0001\n",
      "Epoch 418. Current alpha: 0.3898, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.973803, weighted total: 0.0001\n",
      "Epoch 419. Current alpha: 0.3890, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.834445, weighted total: 0.0001\n",
      "Epoch 420. Current alpha: 0.3882, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.961261, weighted total: 0.0001\n",
      "Epoch 421. Current alpha: 0.3874, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.969000, weighted total: 0.0001\n",
      "Epoch 422. Current alpha: 0.3867, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.711010, weighted total: 0.0001\n",
      "Epoch 423. Current alpha: 0.3859, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.699244, weighted total: 0.0001\n",
      "Epoch 424. Current alpha: 0.3851, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 1.746116, weighted total: 0.0001\n",
      "Epoch 425. Current alpha: 0.3843, lr: 0.000002. Training losses: pinn: 0.000002, boundary: 2.145701, weighted total: 0.0001\n",
      "Epoch 426. Current alpha: 0.3836, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.958851, weighted total: 0.0001\n",
      "Epoch 427. Current alpha: 0.3828, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.927700, weighted total: 0.0001\n",
      "Epoch 428. Current alpha: 0.3820, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.978329, weighted total: 0.0001\n",
      "Epoch 429. Current alpha: 0.3813, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.963934, weighted total: 0.0001\n",
      "Epoch 430. Current alpha: 0.3805, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.833261, weighted total: 0.0001\n",
      "Epoch 431. Current alpha: 0.3798, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.815533, weighted total: 0.0001\n",
      "Epoch 432. Current alpha: 0.3790, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.772422, weighted total: 0.0001\n",
      "Epoch 433. Current alpha: 0.3782, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.850662, weighted total: 0.0001\n",
      "Epoch 434. Current alpha: 0.3775, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.778252, weighted total: 0.0001\n",
      "Epoch 435. Current alpha: 0.3767, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.955691, weighted total: 0.0001\n",
      "Epoch 436. Current alpha: 0.3760, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.993584, weighted total: 0.0001\n",
      "Epoch 437. Current alpha: 0.3752, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 2.067302, weighted total: 0.0001\n",
      "Epoch 438. Current alpha: 0.3745, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.739206, weighted total: 0.0001\n",
      "Epoch 439. Current alpha: 0.3737, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.920201, weighted total: 0.0001\n",
      "Epoch 440. Current alpha: 0.3730, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.905183, weighted total: 0.0001\n",
      "Epoch 441. Current alpha: 0.3722, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.757811, weighted total: 0.0001\n",
      "Epoch 442. Current alpha: 0.3715, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.628097, weighted total: 0.0001\n",
      "Epoch 443. Current alpha: 0.3707, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.916110, weighted total: 0.0001\n",
      "Epoch 444. Current alpha: 0.3700, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.699287, weighted total: 0.0001\n",
      "Epoch 445. Current alpha: 0.3693, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.863345, weighted total: 0.0001\n",
      "Epoch 446. Current alpha: 0.3685, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.968309, weighted total: 0.0001\n",
      "Epoch 447. Current alpha: 0.3678, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.775977, weighted total: 0.0001\n",
      "Epoch 448. Current alpha: 0.3670, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.716856, weighted total: 0.0001\n",
      "Epoch 449. Current alpha: 0.3663, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.483531, weighted total: 0.0001\n",
      "Epoch 450. Current alpha: 0.3656, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.913605, weighted total: 0.0001\n",
      "Epoch 451. Current alpha: 0.3649, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.843137, weighted total: 0.0001\n",
      "Epoch 452. Current alpha: 0.3641, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.756983, weighted total: 0.0001\n",
      "Epoch 453. Current alpha: 0.3634, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.960264, weighted total: 0.0001\n",
      "Epoch 454. Current alpha: 0.3627, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.935877, weighted total: 0.0001\n",
      "Epoch 455. Current alpha: 0.3619, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.863986, weighted total: 0.0001\n",
      "Epoch 456. Current alpha: 0.3612, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.629655, weighted total: 0.0001\n",
      "Epoch 457. Current alpha: 0.3605, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.857861, weighted total: 0.0001\n",
      "Epoch 458. Current alpha: 0.3598, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.698602, weighted total: 0.0001\n",
      "Epoch 459. Current alpha: 0.3591, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.627205, weighted total: 0.0001\n",
      "Epoch 460. Current alpha: 0.3583, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.896166, weighted total: 0.0001\n",
      "Epoch 461. Current alpha: 0.3576, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.903481, weighted total: 0.0001\n",
      "Epoch 462. Current alpha: 0.3569, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.724976, weighted total: 0.0001\n",
      "Epoch 463. Current alpha: 0.3562, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.865119, weighted total: 0.0001\n",
      "Epoch 464. Current alpha: 0.3555, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.881198, weighted total: 0.0001\n",
      "Epoch 465. Current alpha: 0.3548, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 1.801037, weighted total: 0.0001\n",
      "Epoch 466. Current alpha: 0.3541, lr: 0.000001. Training losses: pinn: 0.000002, boundary: 2.017967, weighted total: 0.0001\n",
      "Epoch 467. Current alpha: 0.3534, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.428105, weighted total: 0.0001\n",
      "Epoch 468. Current alpha: 0.3526, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.894985, weighted total: 0.0001\n",
      "Epoch 469. Current alpha: 0.3519, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.863659, weighted total: 0.0001\n",
      "Epoch 470. Current alpha: 0.3512, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.557339, weighted total: 0.0001\n",
      "Epoch 471. Current alpha: 0.3505, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.823586, weighted total: 0.0001\n",
      "Epoch 472. Current alpha: 0.3498, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.639528, weighted total: 0.0001\n",
      "Epoch 473. Current alpha: 0.3491, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.807369, weighted total: 0.0001\n",
      "Epoch 474. Current alpha: 0.3484, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.982745, weighted total: 0.0001\n",
      "Epoch 475. Current alpha: 0.3477, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.959815, weighted total: 0.0001\n",
      "Epoch 476. Current alpha: 0.3470, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.867845, weighted total: 0.0001\n",
      "Epoch 477. Current alpha: 0.3463, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.716902, weighted total: 0.0001\n",
      "Epoch 478. Current alpha: 0.3457, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.706890, weighted total: 0.0001\n",
      "Epoch 479. Current alpha: 0.3450, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.818981, weighted total: 0.0001\n",
      "Epoch 480. Current alpha: 0.3443, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.533566, weighted total: 0.0001\n",
      "Epoch 481. Current alpha: 0.3436, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 2.134382, weighted total: 0.0001\n",
      "Epoch 482. Current alpha: 0.3429, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.885292, weighted total: 0.0001\n",
      "Epoch 483. Current alpha: 0.3422, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.834904, weighted total: 0.0001\n",
      "Epoch 484. Current alpha: 0.3415, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.944211, weighted total: 0.0001\n",
      "Epoch 485. Current alpha: 0.3408, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.534773, weighted total: 0.0001\n",
      "Epoch 486. Current alpha: 0.3402, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.697545, weighted total: 0.0001\n",
      "Epoch 487. Current alpha: 0.3395, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.606514, weighted total: 0.0001\n",
      "Epoch 488. Current alpha: 0.3388, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.941017, weighted total: 0.0001\n",
      "Epoch 489. Current alpha: 0.3381, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.751187, weighted total: 0.0001\n",
      "Epoch 490. Current alpha: 0.3374, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.653726, weighted total: 0.0001\n",
      "Epoch 491. Current alpha: 0.3368, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.556067, weighted total: 0.0001\n",
      "Epoch 492. Current alpha: 0.3361, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.815844, weighted total: 0.0001\n",
      "Epoch 493. Current alpha: 0.3354, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.671193, weighted total: 0.0001\n",
      "Epoch 494. Current alpha: 0.3348, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.909819, weighted total: 0.0001\n",
      "Epoch 495. Current alpha: 0.3341, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.850261, weighted total: 0.0001\n",
      "Epoch 496. Current alpha: 0.3334, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 2.087968, weighted total: 0.0001\n",
      "Epoch 497. Current alpha: 0.3328, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.688080, weighted total: 0.0001\n",
      "Epoch 498. Current alpha: 0.3321, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.592180, weighted total: 0.0001\n",
      "Epoch 499. Current alpha: 0.3314, lr: 0.000000. Training losses: pinn: 0.000002, boundary: 1.790077, weighted total: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Neural network. Note: 2 inputs- (p, r), 1 output- f(r, p)\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(100, activation='selu')(inputs)\n",
    "x_ = tf.keras.layers.Dense(100, activation='selu')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='selu')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='selu')(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.9\n",
    "alpha_decay = 0.998\n",
    "beta = 1e-4\n",
    "lr = 3e-4\n",
    "lr_decay = 0.95\n",
    "patience = 10\n",
    "batchsize = 1032\n",
    "boundary_batchsize = 256\n",
    "epochs = 500\n",
    "save = False\n",
    "load_epoch = -1\n",
    "filename = ''\n",
    "\n",
    "# Initialize and fit the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], f_boundary=f_boundary[:, 0], size=size)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_predict, alpha=alpha, beta=beta, batchsize=batchsize, boundary_batchsize=boundary_batchsize,\n",
    "                                                         epochs=epochs, lr=lr, size=size, save=save, load_epoch=load_epoch, lr_decay=lr_decay,\n",
    "                                                         alpha_decay=alpha_decay, patience=patience, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the outputs to a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open('./figures/pickles/pinn_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/pickles/boundary_loss_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "     \n",
    "with open('./figures/pickles/predictions_' + filename + '.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "with open('./figures/pickles/f_boundary.pkl', 'wb') as file:\n",
    "    pkl.dump(f_boundary, file)\n",
    "    \n",
    "with open('./figures/pickles/p.pkl', 'wb') as file:\n",
    "    pkl.dump(p, file)\n",
    "    \n",
    "with open('./figures/pickles/T.pkl', 'wb') as file:\n",
    "    pkl.dump(T, file)\n",
    "    \n",
    "with open('./figures/pickles/r.pkl', 'wb') as file:\n",
    "    pkl.dump(r, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
