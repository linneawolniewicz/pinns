{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Physics Informed Neural Networks\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook adapts the code implementation of Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs, and the code by Michael Ito, to solve the force-field equation for solar modulation of cosmic rays. Michael Ito's code adaption use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure. \n",
    "\n",
    "In this application, our PINN is $h(r, p) = \\frac{\\partial f}{\\partial r} + \\frac{RV}{3k} \\frac{\\partial f}{\\partial p}$ where $k=\\beta(p)k_1(r)k_2(r)$ and $\\beta = \\frac{p}{\\sqrt{p^2 + M^2}}$. We will approximate $f(r, p)$ using a neural network.\n",
    "\n",
    "We have no initial data, but our boundary data will be given by $f(r_{HP}, p) = \\frac{J(r_{HP}, T)}{p^2} = \\frac{(T+M)^\\gamma}{p^2}$, where $r_{HP} = 120$ AU (i.e. the radius of Heliopause), $M=0.938$ GeV, $\\gamma$ is between $-2$ and $-3$, and $T = \\sqrt{p^2 + M^2} - M$. Or, vice versa, $p = \\sqrt{T^2 + 2TM}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sherpa\n",
    "import pickle as pkl\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants  \n",
    "m = 0.938 # GeV/c^2\n",
    "gamma = -3 # Between -2 and -3\n",
    "size = 512 # size of r, T, p, and f_boundary\n",
    "\n",
    "# Create intial data\n",
    "T = np.logspace(np.log10(0.001), np.log10(1000), size).flatten()[:,None] # GeV\n",
    "p = (np.sqrt((T+m)**2-m**2)).flatten()[:,None] # GeV/c\n",
    "r = np.logspace(np.log10(119*150e6), np.log10(120*150e6), size).flatten()[:,None] # km\n",
    "f_boundary = ((T + m)**gamma)/(p**2) # particles/(m^3 (GeV/c)^3)\n",
    "\n",
    "# Take the log of all input data\n",
    "r = np.log(r)\n",
    "T = np.log(T)\n",
    "p = np.log(p)\n",
    "f_boundary = np.log(f_boundary)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([p[0], r[0]]) # (p, r) in (GeV, AU)\n",
    "ub = np.array([p[-1], r[-1]]) # (p, r) in (GeV, AU)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "P, R = np.meshgrid(p, r)\n",
    "P_star = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))\n",
    "\n",
    "# Check inputs\n",
    "# print(f'r: {r.shape}, p: {p.shape}, T: {T.shape}, f_boundary: {f_boundary.shape}, P_star: {P_star.shape}, lb: {lb}, ub:{ub}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, r, f_boundary, size, n_samples=20000, n_boundary=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.r = r\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.n_boundary = n_boundary\n",
    "        self.size = size\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and boundary_f.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "        alpha = weight on pinn_loss\n",
    "        \n",
    "        beta = weight on boundary_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha=1, beta=1):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                # Forward pass P (PINN data)\n",
    "                P = tf.concat((p, r), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Forward pass P_boundary (boundary condition data)\n",
    "                P_boundary = tf.concat((p_boundary, r_boundary), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "\n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tf.math.reduce_mean(tf.math.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            f_p = t1.gradient(f, p)\n",
    "            f_r = t1.gradient(f, r)\n",
    "\n",
    "            # Calculate resulting loss = PINN loss + boundary loss\n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r)\n",
    "            total_loss = alpha*pinn_loss + beta*boundary_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Return losses\n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        alpha = weight on pinn_loss\n",
    "        \n",
    "        beta = weight on boundary_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_decay: If -1, learning rate will not be decayed. Otherwise, lr = lr_decay*lr if loss doesn't\n",
    "        decrease for 3 epochs\n",
    "        \n",
    "        weight_change: If -1, alpha will not be changed. Otherwise, alpha = weight_change*alpha if loss \n",
    "        doesn't decrease for 3 epochs\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, alpha=1, beta=1, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, size=256, \n",
    "            save=False, load_epoch=-1, lr_decay=-1, weight_change=-1):\n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            self.load_weights('./ckpts/pinn_epoch_' + str(load_epoch))\n",
    "        \n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            # Reset loss variables\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN and boundary variables and pass them to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN p and r variables via uniform distribution sampling between lower and upper bounds\n",
    "                p = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                r = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = self.p[p_idx]\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound[:] = self.upper_bound[1]\n",
    "                r_boundary = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                \n",
    "                # Pass variables through the model via train_step and get losses\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Calculate and print total losses for the epoch\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Training loss for epoch {epoch}: pinn: {total_pinn_loss[epoch]:.4f}, boundary: {total_boundary_loss[epoch]:.4f}, total: {(total_boundary_loss[epoch]+total_pinn_loss[epoch]):.4f}')\n",
    "            \n",
    "            # Predict\n",
    "            predictions[:, :, epoch] = self.predict(P_predict, size)\n",
    "            \n",
    "            # If the epoch is a multiple of 10, save to a checkpoint\n",
    "            if (epoch%10 == 0) & (save == True):\n",
    "                self.save_weights('./ckpts/pinn_epoch_' + str(epoch), overwrite=True, save_format=None, options=None)\n",
    "            \n",
    "            # Determine if loss has decreased for the past 2 or 5 epochs\n",
    "            if (epoch > 3):\n",
    "                isDecreasingFor2 = False\n",
    "                for i in range(2):\n",
    "                    if (total_pinn_loss[epoch-i] + total_boundary_loss[epoch-i]) < (total_pinn_loss[epoch-(i+1)] + total_boundary_loss[epoch-(i+1)]):\n",
    "                        isDecreasingFor2 = True\n",
    "                        \n",
    "                # If loss hasn't decreased for the past 2 epochs, decrease lr by lr_decay\n",
    "                if (lr_decay != -1) & (not isDecreasingFor2):\n",
    "                    lr = lr_decay*lr\n",
    "\n",
    "                # If pinn loss hasn't decreased for the past 2 epochs, increase alpha by weight_change\n",
    "                if (weight_change != -1) & (not isDecreasingFor2):\n",
    "                    alpha = weight_change*alpha\n",
    "            \n",
    "            if (epoch > 6):\n",
    "                isDecreasingFor5 = False\n",
    "                for i in range(5):\n",
    "                    if (total_pinn_loss[epoch-i] + total_boundary_loss[epoch-i]) < (total_pinn_loss[epoch-(i+1)] + total_boundary_loss[epoch-(i+1)]):\n",
    "                        isDecreasingFor5 = True\n",
    "                        \n",
    "                # If loss hasn't decreased for 5 epochs, break\n",
    "                if not isDecreasingFor5:\n",
    "                    break\n",
    "        \n",
    "        # Return epoch losses\n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, size, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(P.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((size**2, 1))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Pass prediction data through the model\n",
    "            preds[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        # Return f\n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r): \n",
    "        # Note: p and r are taken out of logspace for the PINN calculation\n",
    "        p = tf.math.exp(p) # GeV/c\n",
    "        r = tf.math.exp(r) # km\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tf.math.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tf.math.divide(p, tf.math.sqrt(tf.math.square(p) + tf.math.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tf.math.reduce_mean(tf.math.abs(f_r + (tf.math.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network regularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 09:52:12.121383: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 0: pinn: 2.1878, boundary: 134.7362, total: 136.9239\n",
      "Training loss for epoch 1: pinn: 3.0206, boundary: 35.6430, total: 38.6637\n",
      "Training loss for epoch 2: pinn: 2.0903, boundary: 31.5589, total: 33.6491\n",
      "Training loss for epoch 3: pinn: 1.8802, boundary: 27.1513, total: 29.0315\n",
      "Training loss for epoch 4: pinn: 1.9293, boundary: 29.4294, total: 31.3586\n",
      "Training loss for epoch 5: pinn: 1.8181, boundary: 24.8902, total: 26.7082\n",
      "Training loss for epoch 6: pinn: 2.2742, boundary: 31.3636, total: 33.6378\n",
      "Training loss for epoch 7: pinn: 2.1294, boundary: 22.0281, total: 24.1575\n",
      "Training loss for epoch 8: pinn: 2.1647, boundary: 23.9552, total: 26.1199\n",
      "Training loss for epoch 9: pinn: 1.6717, boundary: 17.8514, total: 19.5232\n",
      "Training loss for epoch 10: pinn: 1.7755, boundary: 20.7483, total: 22.5238\n",
      "Training loss for epoch 11: pinn: 1.7596, boundary: 23.9341, total: 25.6936\n",
      "Training loss for epoch 12: pinn: 1.4441, boundary: 16.5914, total: 18.0356\n",
      "Training loss for epoch 13: pinn: 1.4491, boundary: 20.4982, total: 21.9472\n",
      "Training loss for epoch 14: pinn: 1.2923, boundary: 14.4481, total: 15.7404\n",
      "Training loss for epoch 15: pinn: 1.2697, boundary: 14.2845, total: 15.5542\n",
      "Training loss for epoch 16: pinn: 1.2273, boundary: 12.5083, total: 13.7356\n",
      "Training loss for epoch 17: pinn: 1.1689, boundary: 14.4607, total: 15.6297\n",
      "Training loss for epoch 18: pinn: 1.1390, boundary: 14.0541, total: 15.1930\n",
      "Training loss for epoch 19: pinn: 1.3594, boundary: 20.4993, total: 21.8586\n",
      "Training loss for epoch 20: pinn: 1.1579, boundary: 11.6916, total: 12.8495\n",
      "Training loss for epoch 21: pinn: 1.1788, boundary: 12.2950, total: 13.4738\n",
      "Training loss for epoch 22: pinn: 1.1617, boundary: 11.8953, total: 13.0569\n",
      "Training loss for epoch 23: pinn: 1.1722, boundary: 12.2064, total: 13.3786\n",
      "Training loss for epoch 24: pinn: 1.1364, boundary: 11.4840, total: 12.6204\n",
      "Training loss for epoch 25: pinn: 1.0794, boundary: 11.1116, total: 12.1909\n",
      "Training loss for epoch 26: pinn: 1.1593, boundary: 13.7164, total: 14.8757\n",
      "Training loss for epoch 27: pinn: 1.0810, boundary: 10.6605, total: 11.7415\n",
      "Training loss for epoch 28: pinn: 1.0035, boundary: 9.8284, total: 10.8318\n",
      "Training loss for epoch 29: pinn: 1.0019, boundary: 12.0464, total: 13.0483\n",
      "Training loss for epoch 30: pinn: 1.0616, boundary: 10.3100, total: 11.3716\n",
      "Training loss for epoch 31: pinn: 1.0108, boundary: 9.7637, total: 10.7745\n",
      "Training loss for epoch 32: pinn: 1.0131, boundary: 10.0750, total: 11.0881\n",
      "Training loss for epoch 33: pinn: 1.0245, boundary: 9.9213, total: 10.9458\n",
      "Training loss for epoch 34: pinn: 0.9934, boundary: 9.4652, total: 10.4586\n",
      "Training loss for epoch 35: pinn: 1.0768, boundary: 13.8711, total: 14.9479\n",
      "Training loss for epoch 36: pinn: 0.9813, boundary: 8.9133, total: 9.8946\n",
      "Training loss for epoch 37: pinn: 0.9308, boundary: 9.4273, total: 10.3580\n",
      "Training loss for epoch 38: pinn: 0.9115, boundary: 9.8041, total: 10.7156\n",
      "Training loss for epoch 39: pinn: 0.8797, boundary: 8.4706, total: 9.3503\n",
      "Training loss for epoch 40: pinn: 0.8895, boundary: 7.2744, total: 8.1639\n",
      "Training loss for epoch 41: pinn: 0.8584, boundary: 7.5508, total: 8.4091\n",
      "Training loss for epoch 42: pinn: 0.7805, boundary: 6.0241, total: 6.8046\n",
      "Training loss for epoch 43: pinn: 0.7792, boundary: 7.3188, total: 8.0980\n",
      "Training loss for epoch 44: pinn: 0.7246, boundary: 7.0552, total: 7.7797\n",
      "Training loss for epoch 45: pinn: 0.7370, boundary: 7.8426, total: 8.5797\n",
      "Training loss for epoch 46: pinn: 0.6482, boundary: 5.2905, total: 5.9387\n",
      "Training loss for epoch 47: pinn: 0.7128, boundary: 8.8136, total: 9.5265\n",
      "Training loss for epoch 48: pinn: 0.7315, boundary: 8.1942, total: 8.9257\n",
      "Training loss for epoch 49: pinn: 0.7789, boundary: 8.0254, total: 8.8044\n",
      "Training loss for epoch 50: pinn: 0.8436, boundary: 7.7725, total: 8.6161\n",
      "Training loss for epoch 51: pinn: 0.8923, boundary: 7.4931, total: 8.3854\n",
      "Training loss for epoch 52: pinn: 0.9089, boundary: 7.3599, total: 8.2688\n",
      "Training loss for epoch 53: pinn: 0.8885, boundary: 7.1810, total: 8.0695\n",
      "Training loss for epoch 54: pinn: 0.9009, boundary: 9.9672, total: 10.8681\n",
      "Training loss for epoch 55: pinn: 0.8991, boundary: 7.4098, total: 8.3089\n",
      "Training loss for epoch 56: pinn: 0.8851, boundary: 7.3878, total: 8.2728\n",
      "Training loss for epoch 57: pinn: 0.9210, boundary: 7.3868, total: 8.3078\n",
      "Training loss for epoch 58: pinn: 0.9105, boundary: 7.3085, total: 8.2190\n",
      "Training loss for epoch 59: pinn: 0.8862, boundary: 7.3286, total: 8.2148\n",
      "Training loss for epoch 60: pinn: 0.8870, boundary: 7.2179, total: 8.1049\n",
      "Training loss for epoch 61: pinn: 0.8747, boundary: 5.6991, total: 6.5737\n",
      "Training loss for epoch 62: pinn: 0.9148, boundary: 7.1927, total: 8.1074\n",
      "Training loss for epoch 63: pinn: 0.9269, boundary: 7.0523, total: 7.9791\n",
      "Training loss for epoch 64: pinn: 0.9261, boundary: 6.6085, total: 7.5345\n",
      "Training loss for epoch 65: pinn: 0.9040, boundary: 6.1423, total: 7.0463\n",
      "Training loss for epoch 66: pinn: 0.8175, boundary: 6.8245, total: 7.6420\n",
      "Training loss for epoch 67: pinn: 0.8250, boundary: 6.9999, total: 7.8249\n",
      "Training loss for epoch 68: pinn: 0.8111, boundary: 7.0302, total: 7.8413\n",
      "Training loss for epoch 69: pinn: 0.8290, boundary: 5.4737, total: 6.3027\n",
      "Training loss for epoch 70: pinn: 0.8311, boundary: 5.0887, total: 5.9198\n",
      "Training loss for epoch 71: pinn: 0.7517, boundary: 5.2243, total: 5.9760\n",
      "Training loss for epoch 72: pinn: 0.7579, boundary: 5.0284, total: 5.7863\n",
      "Training loss for epoch 73: pinn: 0.7120, boundary: 5.5814, total: 6.2934\n",
      "Training loss for epoch 74: pinn: 0.6891, boundary: 5.4863, total: 6.1754\n",
      "Training loss for epoch 75: pinn: 0.6461, boundary: 4.9917, total: 5.6378\n",
      "Training loss for epoch 76: pinn: 0.6642, boundary: 5.4835, total: 6.1477\n",
      "Training loss for epoch 77: pinn: 0.6417, boundary: 5.3715, total: 6.0132\n",
      "Training loss for epoch 78: pinn: 0.6308, boundary: 5.6345, total: 6.2653\n",
      "Training loss for epoch 79: pinn: 0.6384, boundary: 5.4235, total: 6.0619\n",
      "Training loss for epoch 80: pinn: 0.6643, boundary: 6.0795, total: 6.7438\n",
      "Training loss for epoch 81: pinn: 0.6958, boundary: 5.4063, total: 6.1021\n",
      "Training loss for epoch 82: pinn: 0.7053, boundary: 6.0932, total: 6.7985\n",
      "Training loss for epoch 83: pinn: 0.6637, boundary: 5.5238, total: 6.1875\n",
      "Training loss for epoch 84: pinn: 0.6600, boundary: 5.4504, total: 6.1104\n",
      "Training loss for epoch 85: pinn: 0.6538, boundary: 5.3475, total: 6.0013\n",
      "Training loss for epoch 86: pinn: 0.6518, boundary: 5.5297, total: 6.1816\n",
      "Training loss for epoch 87: pinn: 0.6447, boundary: 5.1910, total: 5.8357\n",
      "Training loss for epoch 88: pinn: 0.6387, boundary: 4.0046, total: 4.6434\n",
      "Training loss for epoch 89: pinn: 0.6495, boundary: 3.8753, total: 4.5249\n",
      "Training loss for epoch 90: pinn: 0.7090, boundary: 6.2368, total: 6.9459\n",
      "Training loss for epoch 91: pinn: 0.7392, boundary: 4.7054, total: 5.4446\n",
      "Training loss for epoch 92: pinn: 0.6994, boundary: 7.3701, total: 8.0695\n",
      "Training loss for epoch 93: pinn: 0.6886, boundary: 5.4007, total: 6.0893\n",
      "Training loss for epoch 94: pinn: 0.7008, boundary: 5.3229, total: 6.0237\n",
      "Training loss for epoch 95: pinn: 0.6804, boundary: 5.4089, total: 6.0893\n",
      "Training loss for epoch 96: pinn: 0.7395, boundary: 5.4143, total: 6.1538\n",
      "Training loss for epoch 97: pinn: 0.7170, boundary: 4.6805, total: 5.3975\n",
      "Training loss for epoch 98: pinn: 0.6946, boundary: 4.7830, total: 5.4776\n",
      "Training loss for epoch 99: pinn: 0.7011, boundary: 4.4928, total: 5.1939\n",
      "Training loss for epoch 100: pinn: 0.6872, boundary: 4.3957, total: 5.0829\n",
      "Training loss for epoch 101: pinn: 0.7012, boundary: 5.6704, total: 6.3716\n",
      "Training loss for epoch 102: pinn: 0.6465, boundary: 4.6481, total: 5.2946\n",
      "Training loss for epoch 103: pinn: 0.6386, boundary: 4.6546, total: 5.2933\n",
      "Training loss for epoch 104: pinn: 0.6342, boundary: 4.8050, total: 5.4392\n",
      "Training loss for epoch 105: pinn: 0.6405, boundary: 4.8516, total: 5.4921\n",
      "Training loss for epoch 106: pinn: 0.6399, boundary: 4.1708, total: 4.8107\n",
      "Training loss for epoch 107: pinn: 0.6092, boundary: 3.0526, total: 3.6618\n",
      "Training loss for epoch 108: pinn: 0.6014, boundary: 4.1916, total: 4.7929\n",
      "Training loss for epoch 109: pinn: 0.6017, boundary: 4.6671, total: 5.2688\n",
      "Training loss for epoch 110: pinn: 0.6424, boundary: 3.5589, total: 4.2013\n",
      "Training loss for epoch 111: pinn: 0.6049, boundary: 3.5267, total: 4.1316\n",
      "Training loss for epoch 112: pinn: 0.6182, boundary: 5.2049, total: 5.8231\n",
      "Training loss for epoch 113: pinn: 0.6127, boundary: 3.3462, total: 3.9589\n",
      "Training loss for epoch 114: pinn: 0.5907, boundary: 3.7111, total: 4.3018\n",
      "Training loss for epoch 115: pinn: 0.5819, boundary: 3.7127, total: 4.2946\n",
      "Training loss for epoch 116: pinn: 0.5914, boundary: 3.7108, total: 4.3023\n",
      "Training loss for epoch 117: pinn: 0.5785, boundary: 3.6184, total: 4.1969\n",
      "Training loss for epoch 118: pinn: 0.5642, boundary: 3.6984, total: 4.2625\n",
      "Training loss for epoch 119: pinn: 0.5637, boundary: 3.7855, total: 4.3492\n",
      "Training loss for epoch 120: pinn: 0.5771, boundary: 3.3328, total: 3.9099\n",
      "Training loss for epoch 121: pinn: 0.5638, boundary: 3.0339, total: 3.5977\n",
      "Training loss for epoch 122: pinn: 0.5544, boundary: 3.6662, total: 4.2206\n",
      "Training loss for epoch 123: pinn: 0.5430, boundary: 3.1831, total: 3.7260\n",
      "Training loss for epoch 124: pinn: 0.5338, boundary: 3.2361, total: 3.7698\n",
      "Training loss for epoch 125: pinn: 0.5361, boundary: 3.1650, total: 3.7011\n",
      "Training loss for epoch 126: pinn: 0.5589, boundary: 2.9152, total: 3.4740\n",
      "Training loss for epoch 127: pinn: 0.5494, boundary: 3.1095, total: 3.6589\n",
      "Training loss for epoch 128: pinn: 0.5322, boundary: 3.3382, total: 3.8703\n",
      "Training loss for epoch 129: pinn: 0.5159, boundary: 2.9927, total: 3.5086\n",
      "Training loss for epoch 130: pinn: 0.5153, boundary: 3.0060, total: 3.5212\n",
      "Training loss for epoch 131: pinn: 0.4782, boundary: 2.9847, total: 3.4629\n",
      "Training loss for epoch 132: pinn: 0.4842, boundary: 2.5069, total: 2.9911\n",
      "Training loss for epoch 133: pinn: 0.4786, boundary: 3.2953, total: 3.7739\n",
      "Training loss for epoch 134: pinn: 0.4582, boundary: 3.0140, total: 3.4722\n",
      "Training loss for epoch 135: pinn: 0.4707, boundary: 3.0042, total: 3.4748\n",
      "Training loss for epoch 136: pinn: 0.4882, boundary: 2.9472, total: 3.4354\n",
      "Training loss for epoch 137: pinn: 0.4877, boundary: 2.9957, total: 3.4834\n",
      "Training loss for epoch 138: pinn: 0.5114, boundary: 2.9159, total: 3.4272\n",
      "Training loss for epoch 139: pinn: 0.4947, boundary: 2.9203, total: 3.4150\n",
      "Training loss for epoch 140: pinn: 0.5322, boundary: 2.9874, total: 3.5196\n",
      "Training loss for epoch 141: pinn: 0.5271, boundary: 2.9799, total: 3.5070\n",
      "Training loss for epoch 142: pinn: 0.5156, boundary: 2.7306, total: 3.2462\n",
      "Training loss for epoch 143: pinn: 0.5337, boundary: 3.3469, total: 3.8806\n",
      "Training loss for epoch 144: pinn: 0.5443, boundary: 3.4995, total: 4.0438\n",
      "Training loss for epoch 145: pinn: 0.4983, boundary: 2.3689, total: 2.8671\n",
      "Training loss for epoch 146: pinn: 0.5183, boundary: 4.3956, total: 4.9139\n",
      "Training loss for epoch 147: pinn: 0.5036, boundary: 2.6918, total: 3.1953\n",
      "Training loss for epoch 148: pinn: 0.4954, boundary: 2.6261, total: 3.1215\n",
      "Training loss for epoch 149: pinn: 0.4934, boundary: 2.6241, total: 3.1174\n",
      "Training loss for epoch 150: pinn: 0.4961, boundary: 2.6356, total: 3.1318\n",
      "Training loss for epoch 151: pinn: 0.4931, boundary: 2.6207, total: 3.1138\n",
      "Training loss for epoch 152: pinn: 0.4712, boundary: 1.9549, total: 2.4261\n",
      "Training loss for epoch 153: pinn: 0.5037, boundary: 4.1549, total: 4.6586\n",
      "Training loss for epoch 154: pinn: 0.4913, boundary: 2.1761, total: 2.6674\n",
      "Training loss for epoch 155: pinn: 0.4604, boundary: 2.6130, total: 3.0734\n",
      "Training loss for epoch 156: pinn: 0.4577, boundary: 2.6390, total: 3.0966\n",
      "Training loss for epoch 157: pinn: 0.4508, boundary: 2.4039, total: 2.8547\n",
      "Training loss for epoch 158: pinn: 0.4506, boundary: 1.9052, total: 2.3558\n",
      "Training loss for epoch 159: pinn: 0.4521, boundary: 3.0880, total: 3.5401\n",
      "Training loss for epoch 160: pinn: 0.4444, boundary: 2.6311, total: 3.0755\n",
      "Training loss for epoch 161: pinn: 0.4506, boundary: 2.2812, total: 2.7317\n",
      "Training loss for epoch 162: pinn: 0.4218, boundary: 1.8179, total: 2.2397\n",
      "Training loss for epoch 163: pinn: 0.3955, boundary: 2.3791, total: 2.7746\n",
      "Training loss for epoch 164: pinn: 0.3976, boundary: 1.8690, total: 2.2666\n",
      "Training loss for epoch 165: pinn: 0.3845, boundary: 2.0680, total: 2.4525\n",
      "Training loss for epoch 166: pinn: 0.3808, boundary: 1.7118, total: 2.0926\n",
      "Training loss for epoch 167: pinn: 0.3948, boundary: 2.6542, total: 3.0490\n",
      "Training loss for epoch 168: pinn: 0.3712, boundary: 1.8289, total: 2.2001\n",
      "Training loss for epoch 169: pinn: 0.3749, boundary: 2.3860, total: 2.7609\n",
      "Training loss for epoch 170: pinn: 0.3728, boundary: 2.4166, total: 2.7894\n",
      "Training loss for epoch 171: pinn: 0.3734, boundary: 2.0131, total: 2.3864\n",
      "Training loss for epoch 172: pinn: 0.3800, boundary: 2.1823, total: 2.5623\n",
      "Training loss for epoch 173: pinn: 0.3906, boundary: 1.4333, total: 1.8240\n",
      "Training loss for epoch 174: pinn: 0.3863, boundary: 2.1839, total: 2.5702\n",
      "Training loss for epoch 175: pinn: 0.3815, boundary: 2.0752, total: 2.4567\n",
      "Training loss for epoch 176: pinn: 0.3902, boundary: 2.1817, total: 2.5719\n",
      "Training loss for epoch 177: pinn: 0.3942, boundary: 2.1376, total: 2.5318\n",
      "Training loss for epoch 178: pinn: 0.3929, boundary: 1.9813, total: 2.3743\n",
      "Training loss for epoch 179: pinn: 0.3910, boundary: 2.0257, total: 2.4166\n",
      "Training loss for epoch 180: pinn: 0.3875, boundary: 2.1353, total: 2.5228\n",
      "Training loss for epoch 181: pinn: 0.3854, boundary: 1.9522, total: 2.3376\n",
      "Training loss for epoch 182: pinn: 0.3909, boundary: 1.9102, total: 2.3011\n",
      "Training loss for epoch 183: pinn: 0.3857, boundary: 1.8757, total: 2.2614\n",
      "Training loss for epoch 184: pinn: 0.3971, boundary: 2.1374, total: 2.5345\n",
      "Training loss for epoch 185: pinn: 0.3875, boundary: 1.8931, total: 2.2807\n",
      "Training loss for epoch 186: pinn: 0.3834, boundary: 1.8740, total: 2.2575\n",
      "Training loss for epoch 187: pinn: 0.3786, boundary: 1.8897, total: 2.2683\n",
      "Training loss for epoch 188: pinn: 0.3847, boundary: 1.9323, total: 2.3170\n",
      "Training loss for epoch 189: pinn: 0.3733, boundary: 1.5894, total: 1.9627\n",
      "Training loss for epoch 190: pinn: 0.3826, boundary: 1.6636, total: 2.0462\n",
      "Training loss for epoch 191: pinn: 0.3646, boundary: 1.6981, total: 2.0628\n",
      "Training loss for epoch 192: pinn: 0.3550, boundary: 1.5044, total: 1.8594\n",
      "Training loss for epoch 193: pinn: 0.3576, boundary: 1.5547, total: 1.9123\n",
      "Training loss for epoch 194: pinn: 0.3698, boundary: 1.4869, total: 1.8567\n",
      "Training loss for epoch 195: pinn: 0.3533, boundary: 1.5322, total: 1.8855\n",
      "Training loss for epoch 196: pinn: 0.3503, boundary: 1.3784, total: 1.7287\n",
      "Training loss for epoch 197: pinn: 0.3406, boundary: 1.6261, total: 1.9667\n",
      "Training loss for epoch 198: pinn: 0.3473, boundary: 1.5420, total: 1.8892\n",
      "Training loss for epoch 199: pinn: 0.3422, boundary: 1.4545, total: 1.7967\n",
      "Training loss for epoch 200: pinn: 0.3451, boundary: 1.4503, total: 1.7953\n",
      "Training loss for epoch 201: pinn: 0.3283, boundary: 1.3651, total: 1.6934\n",
      "Training loss for epoch 202: pinn: 0.3345, boundary: 1.4328, total: 1.7673\n",
      "Training loss for epoch 203: pinn: 0.3230, boundary: 1.7000, total: 2.0230\n",
      "Training loss for epoch 204: pinn: 0.3353, boundary: 1.3270, total: 1.6624\n",
      "Training loss for epoch 205: pinn: 0.3329, boundary: 1.2132, total: 1.5461\n",
      "Training loss for epoch 206: pinn: 0.3353, boundary: 0.9892, total: 1.3245\n",
      "Training loss for epoch 207: pinn: 0.3355, boundary: 1.4207, total: 1.7563\n",
      "Training loss for epoch 208: pinn: 0.3342, boundary: 1.2895, total: 1.6237\n",
      "Training loss for epoch 209: pinn: 0.3306, boundary: 1.3511, total: 1.6817\n",
      "Training loss for epoch 210: pinn: 0.3351, boundary: 1.4841, total: 1.8192\n",
      "Training loss for epoch 211: pinn: 0.3113, boundary: 0.9664, total: 1.2777\n",
      "Training loss for epoch 212: pinn: 0.3097, boundary: 0.8940, total: 1.2036\n",
      "Training loss for epoch 213: pinn: 0.3151, boundary: 1.6482, total: 1.9632\n",
      "Training loss for epoch 214: pinn: 0.3023, boundary: 1.1637, total: 1.4660\n",
      "Training loss for epoch 215: pinn: 0.3136, boundary: 1.6399, total: 1.9534\n",
      "Training loss for epoch 216: pinn: 0.3113, boundary: 1.3995, total: 1.7108\n",
      "Training loss for epoch 217: pinn: 0.3183, boundary: 1.2175, total: 1.5358\n",
      "Training loss for epoch 218: pinn: 0.3181, boundary: 1.0597, total: 1.3778\n",
      "Training loss for epoch 219: pinn: 0.3086, boundary: 1.0510, total: 1.3596\n",
      "Training loss for epoch 220: pinn: 0.3090, boundary: 0.9512, total: 1.2603\n",
      "Training loss for epoch 221: pinn: 0.2969, boundary: 0.9483, total: 1.2453\n",
      "Training loss for epoch 222: pinn: 0.3004, boundary: 1.3679, total: 1.6683\n",
      "Training loss for epoch 223: pinn: 0.2956, boundary: 1.1399, total: 1.4356\n",
      "Training loss for epoch 224: pinn: 0.3006, boundary: 0.8129, total: 1.1135\n",
      "Training loss for epoch 225: pinn: 0.3013, boundary: 1.1798, total: 1.4811\n",
      "Training loss for epoch 226: pinn: 0.2865, boundary: 0.8063, total: 1.0929\n",
      "Training loss for epoch 227: pinn: 0.2828, boundary: 0.9690, total: 1.2518\n",
      "Training loss for epoch 228: pinn: 0.2960, boundary: 1.4956, total: 1.7916\n",
      "Training loss for epoch 229: pinn: 0.2846, boundary: 1.2663, total: 1.5510\n",
      "Training loss for epoch 230: pinn: 0.2785, boundary: 0.7891, total: 1.0676\n",
      "Training loss for epoch 231: pinn: 0.2769, boundary: 1.0501, total: 1.3270\n",
      "Training loss for epoch 232: pinn: 0.2760, boundary: 0.9118, total: 1.1878\n",
      "Training loss for epoch 233: pinn: 0.2720, boundary: 0.8099, total: 1.0819\n",
      "Training loss for epoch 234: pinn: 0.2749, boundary: 1.1662, total: 1.4411\n",
      "Training loss for epoch 235: pinn: 0.2769, boundary: 1.1660, total: 1.4429\n",
      "Training loss for epoch 236: pinn: 0.2599, boundary: 0.9756, total: 1.2356\n",
      "Training loss for epoch 237: pinn: 0.2697, boundary: 1.0091, total: 1.2788\n",
      "Training loss for epoch 238: pinn: 0.2675, boundary: 0.8337, total: 1.1012\n",
      "Training loss for epoch 239: pinn: 0.2746, boundary: 0.7652, total: 1.0398\n",
      "Training loss for epoch 240: pinn: 0.2677, boundary: 0.8797, total: 1.1475\n",
      "Training loss for epoch 241: pinn: 0.2601, boundary: 0.9557, total: 1.2158\n",
      "Training loss for epoch 242: pinn: 0.2675, boundary: 0.7298, total: 0.9973\n",
      "Training loss for epoch 243: pinn: 0.2683, boundary: 0.8546, total: 1.1230\n",
      "Training loss for epoch 244: pinn: 0.2708, boundary: 0.6711, total: 0.9420\n",
      "Training loss for epoch 245: pinn: 0.2769, boundary: 0.8724, total: 1.1494\n",
      "Training loss for epoch 246: pinn: 0.2710, boundary: 0.8215, total: 1.0925\n",
      "Training loss for epoch 247: pinn: 0.2739, boundary: 1.0671, total: 1.3410\n",
      "Training loss for epoch 248: pinn: 0.2742, boundary: 0.9167, total: 1.1909\n",
      "Training loss for epoch 249: pinn: 0.2656, boundary: 0.9170, total: 1.1826\n",
      "Training loss for epoch 250: pinn: 0.2726, boundary: 0.9322, total: 1.2048\n",
      "Training loss for epoch 251: pinn: 0.2766, boundary: 0.9328, total: 1.2094\n",
      "Training loss for epoch 252: pinn: 0.2623, boundary: 0.8145, total: 1.0768\n",
      "Training loss for epoch 253: pinn: 0.2595, boundary: 0.8487, total: 1.1081\n",
      "Training loss for epoch 254: pinn: 0.2628, boundary: 0.7904, total: 1.0532\n",
      "Training loss for epoch 255: pinn: 0.2642, boundary: 0.8332, total: 1.0975\n",
      "Training loss for epoch 256: pinn: 0.2517, boundary: 0.8359, total: 1.0876\n",
      "Training loss for epoch 257: pinn: 0.2605, boundary: 0.7897, total: 1.0503\n",
      "Training loss for epoch 258: pinn: 0.2642, boundary: 0.8054, total: 1.0697\n",
      "Training loss for epoch 259: pinn: 0.2596, boundary: 0.8227, total: 1.0822\n",
      "Training loss for epoch 260: pinn: 0.2593, boundary: 0.7395, total: 0.9988\n",
      "Training loss for epoch 261: pinn: 0.2609, boundary: 0.7500, total: 1.0109\n",
      "Training loss for epoch 262: pinn: 0.2485, boundary: 0.7517, total: 1.0003\n",
      "Training loss for epoch 263: pinn: 0.2516, boundary: 0.7338, total: 0.9854\n",
      "Training loss for epoch 264: pinn: 0.2532, boundary: 0.7258, total: 0.9790\n",
      "Training loss for epoch 265: pinn: 0.2508, boundary: 0.7462, total: 0.9970\n",
      "Training loss for epoch 266: pinn: 0.2559, boundary: 0.6898, total: 0.9458\n",
      "Training loss for epoch 267: pinn: 0.2668, boundary: 0.8897, total: 1.1565\n",
      "Training loss for epoch 268: pinn: 0.2526, boundary: 0.7057, total: 0.9583\n",
      "Training loss for epoch 269: pinn: 0.2521, boundary: 0.7505, total: 1.0026\n",
      "Training loss for epoch 270: pinn: 0.2538, boundary: 0.7448, total: 0.9986\n",
      "Training loss for epoch 271: pinn: 0.2508, boundary: 0.6188, total: 0.8696\n",
      "Training loss for epoch 272: pinn: 0.2545, boundary: 0.7683, total: 1.0229\n",
      "Training loss for epoch 273: pinn: 0.2559, boundary: 0.7023, total: 0.9582\n",
      "Training loss for epoch 274: pinn: 0.2526, boundary: 0.7304, total: 0.9830\n",
      "Training loss for epoch 275: pinn: 0.2534, boundary: 0.7257, total: 0.9791\n",
      "Training loss for epoch 276: pinn: 0.2520, boundary: 0.6650, total: 0.9170\n",
      "Training loss for epoch 277: pinn: 0.2523, boundary: 0.7567, total: 1.0090\n",
      "Training loss for epoch 278: pinn: 0.2418, boundary: 0.7083, total: 0.9501\n",
      "Training loss for epoch 279: pinn: 0.2381, boundary: 0.5432, total: 0.7812\n",
      "Training loss for epoch 280: pinn: 0.2386, boundary: 0.8748, total: 1.1134\n",
      "Training loss for epoch 281: pinn: 0.2477, boundary: 0.6871, total: 0.9348\n",
      "Training loss for epoch 282: pinn: 0.2528, boundary: 0.5553, total: 0.8081\n",
      "Training loss for epoch 283: pinn: 0.2505, boundary: 0.7135, total: 0.9640\n",
      "Training loss for epoch 284: pinn: 0.2489, boundary: 0.9762, total: 1.2251\n",
      "Training loss for epoch 285: pinn: 0.2482, boundary: 0.6602, total: 0.9085\n",
      "Training loss for epoch 286: pinn: 0.2445, boundary: 0.6223, total: 0.8668\n",
      "Training loss for epoch 287: pinn: 0.2445, boundary: 0.6987, total: 0.9432\n",
      "Training loss for epoch 288: pinn: 0.2505, boundary: 0.6546, total: 0.9051\n",
      "Training loss for epoch 289: pinn: 0.2406, boundary: 0.6736, total: 0.9142\n",
      "Training loss for epoch 290: pinn: 0.2493, boundary: 0.7280, total: 0.9772\n",
      "Training loss for epoch 291: pinn: 0.2389, boundary: 0.6316, total: 0.8705\n",
      "Training loss for epoch 292: pinn: 0.2448, boundary: 0.5572, total: 0.8020\n",
      "Training loss for epoch 293: pinn: 0.2417, boundary: 0.6322, total: 0.8739\n",
      "Training loss for epoch 294: pinn: 0.2402, boundary: 0.6197, total: 0.8599\n",
      "Training loss for epoch 295: pinn: 0.2351, boundary: 0.5760, total: 0.8111\n",
      "Training loss for epoch 296: pinn: 0.2455, boundary: 0.8023, total: 1.0478\n",
      "Training loss for epoch 297: pinn: 0.2405, boundary: 0.5956, total: 0.8361\n",
      "Training loss for epoch 298: pinn: 0.2410, boundary: 0.5599, total: 0.8008\n",
      "Training loss for epoch 299: pinn: 0.2386, boundary: 0.6413, total: 0.8800\n",
      "Training loss for epoch 300: pinn: 0.2408, boundary: 0.6479, total: 0.8888\n",
      "Training loss for epoch 301: pinn: 0.2371, boundary: 0.5097, total: 0.7468\n",
      "Training loss for epoch 302: pinn: 0.2338, boundary: 0.4023, total: 0.6361\n",
      "Training loss for epoch 303: pinn: 0.2339, boundary: 0.5297, total: 0.7636\n",
      "Training loss for epoch 304: pinn: 0.2306, boundary: 0.5338, total: 0.7644\n",
      "Training loss for epoch 305: pinn: 0.2322, boundary: 0.4562, total: 0.6884\n",
      "Training loss for epoch 306: pinn: 0.2271, boundary: 0.4446, total: 0.6717\n",
      "Training loss for epoch 307: pinn: 0.2333, boundary: 0.4701, total: 0.7034\n",
      "Training loss for epoch 308: pinn: 0.2254, boundary: 0.4608, total: 0.6862\n",
      "Training loss for epoch 309: pinn: 0.2250, boundary: 0.3815, total: 0.6064\n",
      "Training loss for epoch 310: pinn: 0.2248, boundary: 0.4729, total: 0.6977\n",
      "Training loss for epoch 311: pinn: 0.2295, boundary: 0.4944, total: 0.7239\n",
      "Training loss for epoch 312: pinn: 0.2251, boundary: 0.4571, total: 0.6822\n",
      "Training loss for epoch 313: pinn: 0.2214, boundary: 0.4346, total: 0.6559\n",
      "Training loss for epoch 314: pinn: 0.2216, boundary: 0.4239, total: 0.6455\n",
      "Training loss for epoch 315: pinn: 0.2248, boundary: 0.4410, total: 0.6658\n",
      "Training loss for epoch 316: pinn: 0.2247, boundary: 0.4401, total: 0.6648\n",
      "Training loss for epoch 317: pinn: 0.2216, boundary: 0.4816, total: 0.7032\n",
      "Training loss for epoch 318: pinn: 0.2223, boundary: 0.4267, total: 0.6490\n",
      "Training loss for epoch 319: pinn: 0.2256, boundary: 0.4304, total: 0.6560\n",
      "Training loss for epoch 320: pinn: 0.2255, boundary: 0.4520, total: 0.6774\n",
      "Training loss for epoch 321: pinn: 0.2275, boundary: 0.3919, total: 0.6194\n",
      "Training loss for epoch 322: pinn: 0.2181, boundary: 0.4314, total: 0.6495\n",
      "Training loss for epoch 323: pinn: 0.2215, boundary: 0.5190, total: 0.7405\n",
      "Training loss for epoch 324: pinn: 0.2221, boundary: 0.4111, total: 0.6332\n",
      "Training loss for epoch 325: pinn: 0.2208, boundary: 0.3654, total: 0.5861\n",
      "Training loss for epoch 326: pinn: 0.2170, boundary: 0.3595, total: 0.5765\n",
      "Training loss for epoch 327: pinn: 0.2127, boundary: 0.3746, total: 0.5872\n",
      "Training loss for epoch 328: pinn: 0.2202, boundary: 0.4700, total: 0.6902\n",
      "Training loss for epoch 329: pinn: 0.2188, boundary: 0.3206, total: 0.5394\n",
      "Training loss for epoch 330: pinn: 0.2193, boundary: 0.3151, total: 0.5343\n",
      "Training loss for epoch 331: pinn: 0.2185, boundary: 0.3351, total: 0.5536\n",
      "Training loss for epoch 332: pinn: 0.2200, boundary: 0.3367, total: 0.5567\n",
      "Training loss for epoch 333: pinn: 0.2212, boundary: 0.3413, total: 0.5625\n",
      "Training loss for epoch 334: pinn: 0.2212, boundary: 0.3228, total: 0.5439\n",
      "Training loss for epoch 335: pinn: 0.2169, boundary: 0.2625, total: 0.4794\n",
      "Training loss for epoch 336: pinn: 0.2145, boundary: 0.2766, total: 0.4911\n",
      "Training loss for epoch 337: pinn: 0.2142, boundary: 0.3174, total: 0.5316\n",
      "Training loss for epoch 338: pinn: 0.2152, boundary: 0.2745, total: 0.4897\n",
      "Training loss for epoch 339: pinn: 0.2086, boundary: 0.1870, total: 0.3956\n",
      "Training loss for epoch 340: pinn: 0.2121, boundary: 0.2562, total: 0.4684\n",
      "Training loss for epoch 341: pinn: 0.2132, boundary: 0.2497, total: 0.4629\n",
      "Training loss for epoch 342: pinn: 0.2142, boundary: 0.2656, total: 0.4797\n",
      "Training loss for epoch 343: pinn: 0.2096, boundary: 0.2232, total: 0.4328\n",
      "Training loss for epoch 344: pinn: 0.2119, boundary: 0.3264, total: 0.5382\n",
      "Training loss for epoch 345: pinn: 0.2070, boundary: 0.2409, total: 0.4479\n",
      "Training loss for epoch 346: pinn: 0.2055, boundary: 0.2525, total: 0.4580\n",
      "Training loss for epoch 347: pinn: 0.2118, boundary: 0.2522, total: 0.4640\n",
      "Training loss for epoch 348: pinn: 0.2064, boundary: 0.2323, total: 0.4387\n",
      "Training loss for epoch 349: pinn: 0.2091, boundary: 0.2339, total: 0.4430\n",
      "Training loss for epoch 350: pinn: 0.2083, boundary: 0.2270, total: 0.4353\n",
      "Training loss for epoch 351: pinn: 0.2052, boundary: 0.1948, total: 0.4000\n",
      "Training loss for epoch 352: pinn: 0.2091, boundary: 0.2534, total: 0.4625\n",
      "Training loss for epoch 353: pinn: 0.2072, boundary: 0.2297, total: 0.4369\n",
      "Training loss for epoch 354: pinn: 0.2051, boundary: 0.2422, total: 0.4473\n",
      "Training loss for epoch 355: pinn: 0.2066, boundary: 0.2614, total: 0.4680\n",
      "Training loss for epoch 356: pinn: 0.2040, boundary: 0.1975, total: 0.4015\n",
      "Training loss for epoch 357: pinn: 0.2066, boundary: 0.2291, total: 0.4357\n",
      "Training loss for epoch 358: pinn: 0.2067, boundary: 0.2085, total: 0.4152\n",
      "Training loss for epoch 359: pinn: 0.2052, boundary: 0.2181, total: 0.4233\n",
      "Training loss for epoch 360: pinn: 0.2063, boundary: 0.2201, total: 0.4264\n",
      "Training loss for epoch 361: pinn: 0.2101, boundary: 0.1828, total: 0.3929\n",
      "Training loss for epoch 362: pinn: 0.2026, boundary: 0.1683, total: 0.3710\n",
      "Training loss for epoch 363: pinn: 0.2033, boundary: 0.2060, total: 0.4092\n",
      "Training loss for epoch 364: pinn: 0.2051, boundary: 0.1997, total: 0.4048\n",
      "Training loss for epoch 365: pinn: 0.1993, boundary: 0.1736, total: 0.3730\n",
      "Training loss for epoch 366: pinn: 0.1966, boundary: 0.1881, total: 0.3846\n",
      "Training loss for epoch 367: pinn: 0.2064, boundary: 0.1859, total: 0.3923\n",
      "Training loss for epoch 368: pinn: 0.1999, boundary: 0.1700, total: 0.3699\n",
      "Training loss for epoch 369: pinn: 0.2020, boundary: 0.1348, total: 0.3368\n",
      "Training loss for epoch 370: pinn: 0.2049, boundary: 0.1596, total: 0.3645\n",
      "Training loss for epoch 371: pinn: 0.2020, boundary: 0.1476, total: 0.3495\n",
      "Training loss for epoch 372: pinn: 0.2009, boundary: 0.1689, total: 0.3698\n",
      "Training loss for epoch 373: pinn: 0.2018, boundary: 0.1623, total: 0.3641\n",
      "Training loss for epoch 374: pinn: 0.1989, boundary: 0.1904, total: 0.3893\n",
      "Training loss for epoch 375: pinn: 0.2021, boundary: 0.1712, total: 0.3733\n",
      "Training loss for epoch 376: pinn: 0.1984, boundary: 0.1762, total: 0.3747\n",
      "Training loss for epoch 377: pinn: 0.1995, boundary: 0.1990, total: 0.3985\n",
      "Training loss for epoch 378: pinn: 0.1974, boundary: 0.1550, total: 0.3523\n",
      "Training loss for epoch 379: pinn: 0.1980, boundary: 0.1390, total: 0.3370\n",
      "Training loss for epoch 380: pinn: 0.2013, boundary: 0.1961, total: 0.3974\n",
      "Training loss for epoch 381: pinn: 0.1971, boundary: 0.1607, total: 0.3578\n",
      "Training loss for epoch 382: pinn: 0.1936, boundary: 0.1541, total: 0.3477\n",
      "Training loss for epoch 383: pinn: 0.2038, boundary: 0.1565, total: 0.3603\n",
      "Training loss for epoch 384: pinn: 0.1957, boundary: 0.1622, total: 0.3578\n",
      "Training loss for epoch 385: pinn: 0.1985, boundary: 0.1162, total: 0.3147\n",
      "Training loss for epoch 386: pinn: 0.1952, boundary: 0.1705, total: 0.3657\n",
      "Training loss for epoch 387: pinn: 0.1976, boundary: 0.1591, total: 0.3567\n",
      "Training loss for epoch 388: pinn: 0.1999, boundary: 0.1333, total: 0.3332\n",
      "Training loss for epoch 389: pinn: 0.1973, boundary: 0.1626, total: 0.3600\n",
      "Training loss for epoch 390: pinn: 0.1944, boundary: 0.1553, total: 0.3497\n",
      "Training loss for epoch 391: pinn: 0.1947, boundary: 0.1383, total: 0.3331\n",
      "Training loss for epoch 392: pinn: 0.1944, boundary: 0.1671, total: 0.3616\n",
      "Training loss for epoch 393: pinn: 0.1933, boundary: 0.1625, total: 0.3559\n",
      "Training loss for epoch 394: pinn: 0.1955, boundary: 0.1546, total: 0.3501\n",
      "Training loss for epoch 395: pinn: 0.1943, boundary: 0.1606, total: 0.3548\n",
      "Training loss for epoch 396: pinn: 0.1963, boundary: 0.1229, total: 0.3192\n",
      "Training loss for epoch 397: pinn: 0.1994, boundary: 0.1563, total: 0.3558\n",
      "Training loss for epoch 398: pinn: 0.1918, boundary: 0.1361, total: 0.3279\n",
      "Training loss for epoch 399: pinn: 0.1907, boundary: 0.1752, total: 0.3659\n",
      "Training loss for epoch 400: pinn: 0.1993, boundary: 0.1992, total: 0.3985\n",
      "Training loss for epoch 401: pinn: 0.1936, boundary: 0.1297, total: 0.3233\n",
      "Training loss for epoch 402: pinn: 0.1923, boundary: 0.1341, total: 0.3264\n",
      "Training loss for epoch 403: pinn: 0.1907, boundary: 0.1326, total: 0.3232\n",
      "Training loss for epoch 404: pinn: 0.1959, boundary: 0.1419, total: 0.3378\n",
      "Training loss for epoch 405: pinn: 0.1932, boundary: 0.1384, total: 0.3316\n",
      "Training loss for epoch 406: pinn: 0.1926, boundary: 0.1355, total: 0.3281\n",
      "Training loss for epoch 407: pinn: 0.1943, boundary: 0.1425, total: 0.3368\n",
      "Training loss for epoch 408: pinn: 0.1954, boundary: 0.1260, total: 0.3214\n",
      "Training loss for epoch 409: pinn: 0.1928, boundary: 0.1094, total: 0.3022\n",
      "Training loss for epoch 410: pinn: 0.1930, boundary: 0.1432, total: 0.3362\n",
      "Training loss for epoch 411: pinn: 0.1915, boundary: 0.1398, total: 0.3313\n",
      "Training loss for epoch 412: pinn: 0.1905, boundary: 0.1400, total: 0.3305\n",
      "Training loss for epoch 413: pinn: 0.1918, boundary: 0.1419, total: 0.3337\n",
      "Training loss for epoch 414: pinn: 0.1903, boundary: 0.1103, total: 0.3006\n",
      "Training loss for epoch 415: pinn: 0.1913, boundary: 0.1434, total: 0.3348\n",
      "Training loss for epoch 416: pinn: 0.1924, boundary: 0.1372, total: 0.3296\n",
      "Training loss for epoch 417: pinn: 0.1919, boundary: 0.1330, total: 0.3249\n",
      "Training loss for epoch 418: pinn: 0.1913, boundary: 0.1571, total: 0.3484\n",
      "Training loss for epoch 419: pinn: 0.1924, boundary: 0.1290, total: 0.3214\n",
      "Training loss for epoch 420: pinn: 0.1909, boundary: 0.1426, total: 0.3335\n",
      "Training loss for epoch 421: pinn: 0.1942, boundary: 0.1508, total: 0.3449\n",
      "Training loss for epoch 422: pinn: 0.1934, boundary: 0.1347, total: 0.3281\n",
      "Training loss for epoch 423: pinn: 0.1886, boundary: 0.1349, total: 0.3235\n",
      "Training loss for epoch 424: pinn: 0.1912, boundary: 0.1301, total: 0.3213\n",
      "Training loss for epoch 425: pinn: 0.1924, boundary: 0.0909, total: 0.2833\n",
      "Training loss for epoch 426: pinn: 0.1900, boundary: 0.1283, total: 0.3182\n",
      "Training loss for epoch 427: pinn: 0.1903, boundary: 0.1238, total: 0.3141\n",
      "Training loss for epoch 428: pinn: 0.1894, boundary: 0.1114, total: 0.3008\n",
      "Training loss for epoch 429: pinn: 0.1912, boundary: 0.1461, total: 0.3373\n",
      "Training loss for epoch 430: pinn: 0.1899, boundary: 0.1263, total: 0.3162\n",
      "Training loss for epoch 431: pinn: 0.1898, boundary: 0.1325, total: 0.3224\n",
      "Training loss for epoch 432: pinn: 0.1913, boundary: 0.1160, total: 0.3073\n",
      "Training loss for epoch 433: pinn: 0.1903, boundary: 0.1397, total: 0.3300\n",
      "Training loss for epoch 434: pinn: 0.1894, boundary: 0.1344, total: 0.3238\n",
      "Training loss for epoch 435: pinn: 0.1857, boundary: 0.1253, total: 0.3110\n",
      "Training loss for epoch 436: pinn: 0.1905, boundary: 0.1320, total: 0.3224\n",
      "Training loss for epoch 437: pinn: 0.1914, boundary: 0.0985, total: 0.2900\n",
      "Training loss for epoch 438: pinn: 0.1897, boundary: 0.1007, total: 0.2904\n",
      "Training loss for epoch 439: pinn: 0.1896, boundary: 0.1310, total: 0.3207\n",
      "Training loss for epoch 440: pinn: 0.1887, boundary: 0.1221, total: 0.3108\n",
      "Training loss for epoch 441: pinn: 0.1864, boundary: 0.1090, total: 0.2954\n",
      "Training loss for epoch 442: pinn: 0.1893, boundary: 0.1173, total: 0.3066\n",
      "Training loss for epoch 443: pinn: 0.1878, boundary: 0.1173, total: 0.3052\n",
      "Training loss for epoch 444: pinn: 0.1871, boundary: 0.1084, total: 0.2955\n",
      "Training loss for epoch 445: pinn: 0.1896, boundary: 0.1222, total: 0.3118\n",
      "Training loss for epoch 446: pinn: 0.1850, boundary: 0.1067, total: 0.2917\n",
      "Training loss for epoch 447: pinn: 0.1896, boundary: 0.1187, total: 0.3083\n",
      "Training loss for epoch 448: pinn: 0.1893, boundary: 0.1251, total: 0.3144\n",
      "Training loss for epoch 449: pinn: 0.1862, boundary: 0.0986, total: 0.2848\n",
      "Training loss for epoch 450: pinn: 0.1847, boundary: 0.1018, total: 0.2865\n",
      "Training loss for epoch 451: pinn: 0.1852, boundary: 0.1034, total: 0.2886\n",
      "Training loss for epoch 452: pinn: 0.1857, boundary: 0.0976, total: 0.2833\n",
      "Training loss for epoch 453: pinn: 0.1857, boundary: 0.1062, total: 0.2919\n",
      "Training loss for epoch 454: pinn: 0.1847, boundary: 0.0892, total: 0.2739\n",
      "Training loss for epoch 455: pinn: 0.1852, boundary: 0.1000, total: 0.2853\n",
      "Training loss for epoch 456: pinn: 0.1859, boundary: 0.1046, total: 0.2905\n",
      "Training loss for epoch 457: pinn: 0.1852, boundary: 0.0945, total: 0.2797\n",
      "Training loss for epoch 458: pinn: 0.1841, boundary: 0.0778, total: 0.2618\n",
      "Training loss for epoch 459: pinn: 0.1850, boundary: 0.0956, total: 0.2806\n",
      "Training loss for epoch 460: pinn: 0.1840, boundary: 0.0766, total: 0.2606\n",
      "Training loss for epoch 461: pinn: 0.1849, boundary: 0.0882, total: 0.2731\n",
      "Training loss for epoch 462: pinn: 0.1854, boundary: 0.1005, total: 0.2858\n",
      "Training loss for epoch 463: pinn: 0.1860, boundary: 0.0870, total: 0.2730\n",
      "Training loss for epoch 464: pinn: 0.1861, boundary: 0.0691, total: 0.2553\n",
      "Training loss for epoch 465: pinn: 0.1866, boundary: 0.0730, total: 0.2596\n",
      "Training loss for epoch 466: pinn: 0.1872, boundary: 0.0937, total: 0.2809\n",
      "Training loss for epoch 467: pinn: 0.1832, boundary: 0.0717, total: 0.2549\n",
      "Training loss for epoch 468: pinn: 0.1855, boundary: 0.0736, total: 0.2591\n",
      "Training loss for epoch 469: pinn: 0.1851, boundary: 0.0848, total: 0.2699\n",
      "Training loss for epoch 470: pinn: 0.1875, boundary: 0.0712, total: 0.2586\n",
      "Training loss for epoch 471: pinn: 0.1851, boundary: 0.0754, total: 0.2605\n",
      "Training loss for epoch 472: pinn: 0.1862, boundary: 0.0789, total: 0.2651\n",
      "Training loss for epoch 473: pinn: 0.1817, boundary: 0.0720, total: 0.2537\n",
      "Training loss for epoch 474: pinn: 0.1821, boundary: 0.0727, total: 0.2548\n",
      "Training loss for epoch 475: pinn: 0.1842, boundary: 0.0590, total: 0.2433\n",
      "Training loss for epoch 476: pinn: 0.1848, boundary: 0.0755, total: 0.2604\n",
      "Training loss for epoch 477: pinn: 0.1868, boundary: 0.0717, total: 0.2585\n",
      "Training loss for epoch 478: pinn: 0.1833, boundary: 0.0768, total: 0.2601\n",
      "Training loss for epoch 479: pinn: 0.1847, boundary: 0.0602, total: 0.2449\n",
      "Training loss for epoch 480: pinn: 0.1849, boundary: 0.0716, total: 0.2564\n",
      "Training loss for epoch 481: pinn: 0.1801, boundary: 0.0736, total: 0.2538\n",
      "Training loss for epoch 482: pinn: 0.1824, boundary: 0.0773, total: 0.2597\n",
      "Training loss for epoch 483: pinn: 0.1837, boundary: 0.0601, total: 0.2438\n",
      "Training loss for epoch 484: pinn: 0.1815, boundary: 0.0639, total: 0.2454\n",
      "Training loss for epoch 485: pinn: 0.1788, boundary: 0.0672, total: 0.2460\n",
      "Training loss for epoch 486: pinn: 0.1818, boundary: 0.0733, total: 0.2550\n",
      "Training loss for epoch 487: pinn: 0.1830, boundary: 0.0642, total: 0.2472\n",
      "Training loss for epoch 488: pinn: 0.1824, boundary: 0.0604, total: 0.2428\n",
      "Training loss for epoch 489: pinn: 0.1826, boundary: 0.0576, total: 0.2402\n",
      "Training loss for epoch 490: pinn: 0.1852, boundary: 0.0621, total: 0.2472\n",
      "Training loss for epoch 491: pinn: 0.1832, boundary: 0.0524, total: 0.2356\n",
      "Training loss for epoch 492: pinn: 0.1863, boundary: 0.0677, total: 0.2540\n",
      "Training loss for epoch 493: pinn: 0.1841, boundary: 0.0579, total: 0.2419\n",
      "Training loss for epoch 494: pinn: 0.1807, boundary: 0.0580, total: 0.2387\n",
      "Training loss for epoch 495: pinn: 0.1856, boundary: 0.0608, total: 0.2464\n",
      "Training loss for epoch 496: pinn: 0.1814, boundary: 0.0624, total: 0.2438\n",
      "Training loss for epoch 497: pinn: 0.1816, boundary: 0.0635, total: 0.2451\n",
      "Training loss for epoch 498: pinn: 0.1825, boundary: 0.0620, total: 0.2445\n",
      "Training loss for epoch 499: pinn: 0.1827, boundary: 0.0626, total: 0.2454\n",
      "Training loss for epoch 500: pinn: 0.1808, boundary: 0.0614, total: 0.2422\n",
      "Training loss for epoch 501: pinn: 0.1803, boundary: 0.0589, total: 0.2391\n",
      "Training loss for epoch 502: pinn: 0.1836, boundary: 0.0563, total: 0.2399\n",
      "Training loss for epoch 503: pinn: 0.1814, boundary: 0.0675, total: 0.2489\n",
      "Training loss for epoch 504: pinn: 0.1822, boundary: 0.0535, total: 0.2358\n",
      "Training loss for epoch 505: pinn: 0.1813, boundary: 0.0673, total: 0.2486\n",
      "Training loss for epoch 506: pinn: 0.1799, boundary: 0.0618, total: 0.2417\n",
      "Training loss for epoch 507: pinn: 0.1808, boundary: 0.0601, total: 0.2410\n",
      "Training loss for epoch 508: pinn: 0.1823, boundary: 0.0587, total: 0.2410\n",
      "Training loss for epoch 509: pinn: 0.1796, boundary: 0.0628, total: 0.2424\n",
      "Training loss for epoch 510: pinn: 0.1828, boundary: 0.0542, total: 0.2370\n",
      "Training loss for epoch 511: pinn: 0.1799, boundary: 0.0622, total: 0.2421\n",
      "Training loss for epoch 512: pinn: 0.1808, boundary: 0.0526, total: 0.2334\n",
      "Training loss for epoch 513: pinn: 0.1809, boundary: 0.0562, total: 0.2371\n",
      "Training loss for epoch 514: pinn: 0.1813, boundary: 0.0648, total: 0.2460\n",
      "Training loss for epoch 515: pinn: 0.1810, boundary: 0.0567, total: 0.2376\n",
      "Training loss for epoch 516: pinn: 0.1823, boundary: 0.0619, total: 0.2442\n",
      "Training loss for epoch 517: pinn: 0.1794, boundary: 0.0563, total: 0.2357\n",
      "Training loss for epoch 518: pinn: 0.1815, boundary: 0.0542, total: 0.2357\n",
      "Training loss for epoch 519: pinn: 0.1793, boundary: 0.0519, total: 0.2312\n",
      "Training loss for epoch 520: pinn: 0.1830, boundary: 0.0614, total: 0.2444\n",
      "Training loss for epoch 521: pinn: 0.1793, boundary: 0.0601, total: 0.2394\n",
      "Training loss for epoch 522: pinn: 0.1803, boundary: 0.0594, total: 0.2397\n",
      "Training loss for epoch 523: pinn: 0.1783, boundary: 0.0572, total: 0.2355\n",
      "Training loss for epoch 524: pinn: 0.1811, boundary: 0.0606, total: 0.2417\n",
      "Training loss for epoch 525: pinn: 0.1826, boundary: 0.0572, total: 0.2398\n",
      "Training loss for epoch 526: pinn: 0.1800, boundary: 0.0578, total: 0.2379\n",
      "Training loss for epoch 527: pinn: 0.1808, boundary: 0.0566, total: 0.2374\n",
      "Training loss for epoch 528: pinn: 0.1806, boundary: 0.0550, total: 0.2356\n",
      "Training loss for epoch 529: pinn: 0.1823, boundary: 0.0530, total: 0.2354\n",
      "Training loss for epoch 530: pinn: 0.1807, boundary: 0.0564, total: 0.2371\n",
      "Training loss for epoch 531: pinn: 0.1808, boundary: 0.0568, total: 0.2376\n",
      "Training loss for epoch 532: pinn: 0.1800, boundary: 0.0479, total: 0.2279\n",
      "Training loss for epoch 533: pinn: 0.1796, boundary: 0.0562, total: 0.2357\n",
      "Training loss for epoch 534: pinn: 0.1811, boundary: 0.0533, total: 0.2344\n",
      "Training loss for epoch 535: pinn: 0.1781, boundary: 0.0524, total: 0.2305\n",
      "Training loss for epoch 536: pinn: 0.1800, boundary: 0.0536, total: 0.2337\n",
      "Training loss for epoch 537: pinn: 0.1789, boundary: 0.0498, total: 0.2287\n",
      "Training loss for epoch 538: pinn: 0.1784, boundary: 0.0582, total: 0.2367\n",
      "Training loss for epoch 539: pinn: 0.1800, boundary: 0.0528, total: 0.2328\n",
      "Training loss for epoch 540: pinn: 0.1814, boundary: 0.0497, total: 0.2311\n",
      "Training loss for epoch 541: pinn: 0.1802, boundary: 0.0514, total: 0.2316\n",
      "Training loss for epoch 542: pinn: 0.1780, boundary: 0.0532, total: 0.2312\n",
      "Training loss for epoch 543: pinn: 0.1822, boundary: 0.0520, total: 0.2342\n",
      "Training loss for epoch 544: pinn: 0.1786, boundary: 0.0519, total: 0.2305\n",
      "Training loss for epoch 545: pinn: 0.1810, boundary: 0.0574, total: 0.2384\n",
      "Training loss for epoch 546: pinn: 0.1789, boundary: 0.0530, total: 0.2319\n",
      "Training loss for epoch 547: pinn: 0.1837, boundary: 0.0540, total: 0.2376\n",
      "Training loss for epoch 548: pinn: 0.1794, boundary: 0.0512, total: 0.2306\n",
      "Training loss for epoch 549: pinn: 0.1803, boundary: 0.0485, total: 0.2287\n",
      "Training loss for epoch 550: pinn: 0.1821, boundary: 0.0597, total: 0.2418\n",
      "Training loss for epoch 551: pinn: 0.1781, boundary: 0.0535, total: 0.2315\n",
      "Training loss for epoch 552: pinn: 0.1811, boundary: 0.0528, total: 0.2339\n",
      "Training loss for epoch 553: pinn: 0.1797, boundary: 0.0512, total: 0.2309\n",
      "Training loss for epoch 554: pinn: 0.1813, boundary: 0.0575, total: 0.2388\n",
      "Training loss for epoch 555: pinn: 0.1810, boundary: 0.0522, total: 0.2332\n",
      "Training loss for epoch 556: pinn: 0.1788, boundary: 0.0537, total: 0.2326\n",
      "Training loss for epoch 557: pinn: 0.1838, boundary: 0.0564, total: 0.2403\n",
      "Training loss for epoch 558: pinn: 0.1823, boundary: 0.0535, total: 0.2357\n",
      "Training loss for epoch 559: pinn: 0.1796, boundary: 0.0512, total: 0.2308\n",
      "Training loss for epoch 560: pinn: 0.1782, boundary: 0.0537, total: 0.2320\n",
      "Training loss for epoch 561: pinn: 0.1824, boundary: 0.0483, total: 0.2307\n",
      "Training loss for epoch 562: pinn: 0.1812, boundary: 0.0474, total: 0.2287\n",
      "Training loss for epoch 563: pinn: 0.1826, boundary: 0.0554, total: 0.2380\n",
      "Training loss for epoch 564: pinn: 0.1802, boundary: 0.0500, total: 0.2302\n",
      "Training loss for epoch 565: pinn: 0.1854, boundary: 0.0583, total: 0.2437\n",
      "Training loss for epoch 566: pinn: 0.1813, boundary: 0.0516, total: 0.2329\n",
      "Training loss for epoch 567: pinn: 0.1794, boundary: 0.0531, total: 0.2325\n",
      "Training loss for epoch 568: pinn: 0.1792, boundary: 0.0540, total: 0.2332\n",
      "Training loss for epoch 569: pinn: 0.1796, boundary: 0.0587, total: 0.2382\n",
      "Training loss for epoch 570: pinn: 0.1787, boundary: 0.0513, total: 0.2300\n",
      "Training loss for epoch 571: pinn: 0.1786, boundary: 0.0470, total: 0.2256\n",
      "Training loss for epoch 572: pinn: 0.1805, boundary: 0.0468, total: 0.2273\n",
      "Training loss for epoch 573: pinn: 0.1791, boundary: 0.0519, total: 0.2310\n",
      "Training loss for epoch 574: pinn: 0.1819, boundary: 0.0489, total: 0.2309\n",
      "Training loss for epoch 575: pinn: 0.1793, boundary: 0.0457, total: 0.2250\n",
      "Training loss for epoch 576: pinn: 0.1772, boundary: 0.0519, total: 0.2291\n",
      "Training loss for epoch 577: pinn: 0.1813, boundary: 0.0496, total: 0.2310\n",
      "Training loss for epoch 578: pinn: 0.1779, boundary: 0.0436, total: 0.2215\n",
      "Training loss for epoch 579: pinn: 0.1770, boundary: 0.0499, total: 0.2268\n",
      "Training loss for epoch 580: pinn: 0.1763, boundary: 0.0469, total: 0.2232\n",
      "Training loss for epoch 581: pinn: 0.1789, boundary: 0.0458, total: 0.2247\n",
      "Training loss for epoch 582: pinn: 0.1840, boundary: 0.0468, total: 0.2309\n"
     ]
    }
   ],
   "source": [
    "# Neural network. Note: 2 inputs- (p, r), 1 output- f(r, p)\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(100, activation='relu')(inputs)\n",
    "x_ = tf.keras.layers.Dense(100, activation='relu')(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 1 # pinn_loss weight\n",
    "beta = 5 # boundary_loss weight\n",
    "lr = 3e-3\n",
    "lr_decay = 0.9\n",
    "batchsize = 1032\n",
    "boundary_batchsize = 256 \n",
    "epochs = 1000\n",
    "save = True\n",
    "load_epoch = -1\n",
    "weight_change = 1.01\n",
    "    \n",
    "# Initialize and fit the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "            f_boundary=f_boundary[:, 0], size=size)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, beta=beta, batchsize=batchsize, \n",
    "                                                 boundary_batchsize=boundary_batchsize, epochs=epochs, lr=lr, size=size, \n",
    "                                                 save=save, load_epoch=load_epoch, lr_decay=lr_decay, weight_change=weight_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a for loop\n",
    "Train NN to match radii 115-120 AU, then pass result at 115 AU as boundary condition to the next PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.4  30.3  60.2  90.1 120. ]\n",
      "Index 0 with r ranging from 120.0 to 90.1\n",
      "Boundary condition for pinn: [  6.46689992   4.91829299   3.32097146   1.50962314  -1.09672979\n",
      "  -5.54827872 -11.95383354 -19.28785223 -26.88519576 -34.54346332]\n",
      "Training loss for epoch 0: pinn: 2.8481, boundary: 152.8901, total: 155.7382\n",
      "Index 1 with r ranging from 90.1 to 60.199999999999996\n",
      "Boundary condition for pinn: [  6.44985104   4.08876514   1.47078407  -1.33484173  -4.41559362\n",
      "  -8.30787468 -13.2056427  -18.56442833 -23.32230759   0.        ]\n",
      "Training loss for epoch 0: pinn: 3.7587, boundary: 81.7249, total: 85.4836\n",
      "Index 2 with r ranging from 60.199999999999996 to 30.299999999999997\n",
      "Boundary condition for pinn: [  6.96520567   4.78456354   2.31335592  -0.28900659  -3.16939282\n",
      "  -6.78622198 -11.33738232 -16.28641319 -20.76246643   0.        ]\n",
      "Training loss for epoch 0: pinn: 3.4722, boundary: 74.6249, total: 78.0971\n",
      "Index 3 with r ranging from 30.299999999999997 to 0.4\n",
      "Boundary condition for pinn: [  5.86325788   3.94287443   1.55487514  -1.00471723  -3.87175584\n",
      "  -7.36674547 -11.70519543 -15.7842865  -19.13517952   0.        ]\n",
      "Training loss for epoch 0: pinn: 4.1331, boundary: 69.0087, total: 73.1418\n"
     ]
    }
   ],
   "source": [
    "# Constants  \n",
    "m = 0.938 # GeV/c^2\n",
    "gamma = -3 # Between -2 and -3\n",
    "size = 512 # size of r, T, p, and f_boundary\n",
    "num_r = 120\n",
    "\n",
    "# Create intial r, p, and T predict data\n",
    "T = np.logspace(np.log10(0.001), np.log10(1000), size).flatten()[:,None] # GeV\n",
    "p = (np.sqrt((T+m)**2-m**2)).flatten()[:,None] # GeV/c\n",
    "r_mins = np.linspace(0.4, 120, num_r)\n",
    "print(r_mins)\n",
    "\n",
    "# Create boundary f data (f at r_HP) for boundary loss\n",
    "f_boundary = ((T + m)**gamma)/(p**2) # particles/(m^3 (GeV/c)^3)\n",
    "\n",
    "T = np.log(T)\n",
    "p = np.log(p)\n",
    "f_boundary = np.log(f_boundary)\n",
    "\n",
    "# Neural network. Note: 2 inputs- (p, r), 1 output- f(r, p)\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(100, activation='relu')(inputs)\n",
    "x_ = tf.keras.layers.Dense(100, activation='relu')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='relu')(x_)\n",
    "x_ = tf.keras.layers.Dense(100, activation='relu')(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 1 # pinn_loss weight\n",
    "beta = 5 # boundary_loss weight\n",
    "lr = 3e-3\n",
    "lr_decay = 0.9\n",
    "batchsize = 1032\n",
    "boundary_batchsize = 256 \n",
    "epochs = 500\n",
    "save = True\n",
    "load_epoch = -1\n",
    "weight_change = 1.01\n",
    "\n",
    "boundaries = np.zeros((size, num_r+1)) \n",
    "boundaries[:, 0] = f_boundary[:, 0]\n",
    "\n",
    "for i in range(num_r):\n",
    "    print(f'Index {i} with r ranging from {r_mins[-(i+1)]} to {r_mins[-(i+2)]}')\n",
    "    print(f'Boundary condition for pinn: {boundaries[:, i]}')\n",
    "    r = (np.logspace(np.log10(r_mins[-(i+1)]*150e6), np.log10(r_mins[-i]*150e6), size)).flatten()[:,None] # km\n",
    "    r = np.log(r)\n",
    "\n",
    "    # Domain bounds\n",
    "    lb = np.array([p[0], r[0]]) # (p, r) in (GeV, AU)\n",
    "    ub = np.array([p[-1], r[-1]]) # (p, r) in (GeV, AU)\n",
    "\n",
    "    # Flatten and transpose data for ML\n",
    "    P, R = np.meshgrid(p, r)\n",
    "    P_star = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))\n",
    "    \n",
    "    pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "                    f_boundary=boundaries[:, i], size=size)        \n",
    "    \n",
    "    pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, beta=beta, batchsize=batchsize, \n",
    "                                                     boundary_batchsize=boundary_batchsize, epochs=epochs, lr=lr, size=size, \n",
    "                                                     save=save, load_epoch=load_epoch, lr_decay=lr_decay, weight_change=weight_change)\n",
    "    \n",
    "    boundaries[:, i+1] = predictions[:, :, -1].reshape((size, size))[-1, :]\n",
    "    \n",
    "    # Break if reached inner boundary of r\n",
    "    if(num_r - (i+2))==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the outputs to a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open('./figures/pinn_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/boundary_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/predictions.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "with open('./figures/f_boundary.pkl', 'wb') as file:\n",
    "    pkl.dump(f_boundary, file)\n",
    "    \n",
    "with open('./figures/p.pkl', 'wb') as file:\n",
    "    pkl.dump(p, file)\n",
    "    \n",
    "with open('./figures/T.pkl', 'wb') as file:\n",
    "    pkl.dump(T, file)\n",
    "    \n",
    "with open('./figures/r.pkl', 'wb') as file:\n",
    "    pkl.dump(r, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network with Sherpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set-up sherpa parameters, algorithm, and study\n",
    "# parameters = [sherpa.Ordinal(name='beta', range=[5, 10, 15, 20, 30]),\n",
    "#               sherpa.Continuous(name='lr', range=[3e-5, 3e-1], scale='log'),\n",
    "#               sherpa.Ordinal(name='batchsize', range=[256, 512, 1032, 2048]),\n",
    "#               sherpa.Ordinal(name='boundary_batchsize', range=[64, 128, 256]),\n",
    "#               sherpa.Ordinal(name='num_hidden_units', range=[100, 500, 1000]),\n",
    "#               sherpa.Choice(name='activation', range=['relu', 'tanh'])]\n",
    "\n",
    "# algorithm = sherpa.algorithms.RandomSearch(max_num_trials=2)\n",
    "\n",
    "# study = sherpa.Study(parameters=parameters,\n",
    "#                  algorithm=algorithm,\n",
    "#                  lower_is_better=True)\n",
    "\n",
    "# num_iterations = 1\n",
    "\n",
    "# # For each trial in the study, fit the model on the parameters and add the observation to the study\n",
    "# for trial in study:\n",
    "#     # Define neural network\n",
    "#     inputs = tf.keras.Input((2))\n",
    "#     x_ = tf.keras.layers.Dense(trial.parameters, activation=trial.parameters[5])(inputs)\n",
    "#     x_ = tf.keras.layers.Dense(trial.parameters[4], activation=trial.parameters[5])(x_)\n",
    "#     outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "#     # Initialize PINN and compile\n",
    "#     pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "#             f_boundary=f_boundary[:, 0], size=size)\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=trial.parameters[1])\n",
    "#     pinn.compile(optimizer=optimizer)\n",
    "    \n",
    "#     # For each iteration, fit the PINN and add the observation to the study\n",
    "#     for iteration in range(num_iterations):\n",
    "#         total_loss, pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, beta=trial.parameters[0], batchsize=trial.parameters[2], \n",
    "#                                                              boundary_batchsize=trial.parameters[3], epochs=100, size=size)\n",
    "#         study.add_observation(trial=trial,\n",
    "#                               iteration=iteration,\n",
    "#                               objective=boundary_loss,\n",
    "#                               context={'boundary_loss': boundary_loss})\n",
    "#     study.finalize(trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
