{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Physics Informed Neural Networks\n",
    "\n",
    "This notebook implements PINNs from Raissi et al. 2017. Specifically, the notebook adapts the code implementation of Data-Driven Solutions of Nonlinear Partial Differential Equations from https://github.com/maziarraissi/PINNs, and the code by Michael Ito, to solve the force-field equation for solar modulation of cosmic rays. Michael Ito's code adaption use the TF2 API where the main mechanisms of the PINN arise in the train_step function efficiently computing higher order derivatives of custom loss functions through the use of the GradientTape data structure. \n",
    "\n",
    "In this application, our PINN is $h(r, p) = \\frac{\\partial f}{\\partial r} + \\frac{RV}{3k} \\frac{\\partial f}{\\partial p}$ where $k=\\beta(p)k_1(r)k_2(r)$ and $\\beta = \\frac{p}{\\sqrt{p^2 + M^2}}$. We will approximate $f(r, p)$ using a neural network.\n",
    "\n",
    "We have no initial data, but our boundary data will be given by $f(r_{HP}, p) = \\frac{J(r_{HP}, T)}{p^2} = \\frac{(T+M)^\\gamma}{p^2}$, where $r_{HP} = 120$ AU (i.e. the radius of Heliopause), $M=0.938$ GeV, $\\gamma$ is between $-2$ and $-3$, and $T = \\sqrt{p^2 + M^2} - M$. Or, vice versa, $p = \\sqrt{T^2 + 2TM}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 16:41:32.659290: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/system/CUDA/11.0.2/nvvm/lib64:/opt/apps/software/system/CUDA/11.0.2/extras/CUPTI/lib64:/opt/apps/software/system/CUDA/11.0.2/lib:/opt/apps/software/lib/slurm-drmaa/1.1.3/lib:/opt/apps/software/lib/libevent/2.1.8/lib:/opt/apps/software/devel/PCRE/8.41-GCCcore-7.3.0/lib:/opt/apps/software/lang/Perl/5.28.0-GCCcore-7.3.0/lib:/opt/apps/software/tools/expat/2.2.5-GCCcore-7.3.0/lib:/opt/apps/software/lang/Python/2.7.15-foss-2018b/lib/python2.7/site-packages/numpy-1.14.5-py2.7-linux-x86_64.egg/numpy/core/lib:/opt/apps/software/lang/Python/2.7.15-foss-2018b/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-7.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-7.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-7.3.0/lib:/opt/apps/software/devel/SQLite/3.24.0-GCCcore-7.3.0/lib:/opt/apps/software/lang/Tcl/8.6.8-GCCcore-7.3.0/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-7.3.0/lib:/opt/apps/software/devel/ncurses/6.1-GCCcore-7.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6-GCCcore-7.3.0/lib:/opt/apps/software/numlib/ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1/lib:/opt/apps/software/numlib/FFTW/3.3.8-gompi-2018b/lib:/opt/apps/software/numlib/OpenBLAS/0.3.1-GCC-7.3.0-2.30/lib:/opt/apps/software/system/hwloc/1.11.10-GCCcore-7.3.0/lib:/opt/apps/software/system/libpciaccess/0.14-GCCcore-7.3.0/lib:/opt/apps/software/lib/libxml2/2.9.8-GCCcore-7.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-7.3.0/lib:/opt/apps/software/tools/numactl/2.0.11-GCCcore-7.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-7.3.0/lib:/opt/apps/software/tools/binutils/2.30-GCCcore-7.3.0/lib:/opt/apps/software/compiler/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0:/opt/apps/software/compiler/GCCcore/7.3.0/lib64:/opt/apps/software/compiler/GCCcore/7.3.0/lib:/opt/apps/software/tools/zsh/5.7.1/lib\n",
      "2022-09-13 16:41:32.663946: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/system/CUDA/11.0.2/nvvm/lib64:/opt/apps/software/system/CUDA/11.0.2/extras/CUPTI/lib64:/opt/apps/software/system/CUDA/11.0.2/lib:/opt/apps/software/lib/slurm-drmaa/1.1.3/lib:/opt/apps/software/lib/libevent/2.1.8/lib:/opt/apps/software/devel/PCRE/8.41-GCCcore-7.3.0/lib:/opt/apps/software/lang/Perl/5.28.0-GCCcore-7.3.0/lib:/opt/apps/software/tools/expat/2.2.5-GCCcore-7.3.0/lib:/opt/apps/software/lang/Python/2.7.15-foss-2018b/lib/python2.7/site-packages/numpy-1.14.5-py2.7-linux-x86_64.egg/numpy/core/lib:/opt/apps/software/lang/Python/2.7.15-foss-2018b/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-7.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-7.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-7.3.0/lib:/opt/apps/software/devel/SQLite/3.24.0-GCCcore-7.3.0/lib:/opt/apps/software/lang/Tcl/8.6.8-GCCcore-7.3.0/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-7.3.0/lib:/opt/apps/software/devel/ncurses/6.1-GCCcore-7.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6-GCCcore-7.3.0/lib:/opt/apps/software/numlib/ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1/lib:/opt/apps/software/numlib/FFTW/3.3.8-gompi-2018b/lib:/opt/apps/software/numlib/OpenBLAS/0.3.1-GCC-7.3.0-2.30/lib:/opt/apps/software/system/hwloc/1.11.10-GCCcore-7.3.0/lib:/opt/apps/software/system/libpciaccess/0.14-GCCcore-7.3.0/lib:/opt/apps/software/lib/libxml2/2.9.8-GCCcore-7.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-7.3.0/lib:/opt/apps/software/tools/numactl/2.0.11-GCCcore-7.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-7.3.0/lib:/opt/apps/software/tools/binutils/2.30-GCCcore-7.3.0/lib:/opt/apps/software/compiler/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0:/opt/apps/software/compiler/GCCcore/7.3.0/lib64:/opt/apps/software/compiler/GCCcore/7.3.0/lib:/opt/apps/software/tools/zsh/5.7.1/lib\n",
      "2022-09-13 16:41:32.663973: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sherpa\n",
    "import pickle as pkl\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: (10, 1), p: (10, 1), T: (10, 1), f_boundary: (10, 1), P_star: (100, 2), lb: [[-3.13904026]\n",
      " [17.90985512]], ub:[[ 6.9086924 ]\n",
      " [23.61363759]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAG1CAYAAAAV2Js8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJhklEQVR4nO3deViU5cIG8PudGXZZZUkEcQUXBATMtFDJ0tQ0NM38Ki3LjnY6pmia2TH1pJ4ytTymaVa2WaiFlVpGpYFomigo4oIKIQIisgz7MvN+f6gkoTLoDM8s9++65op55p2Ze56IuXtXSZZlGUREREQWSCE6ABEREZEoLEJERERksViEiIiIyGKxCBEREZHFYhEiIiIii8UiRERERBaLRYiIiIgsFosQERERWSyV6ADGTqvVIicnB46OjpAkSXQcIiIi0oEsyygtLYW3tzcUipuv92ERakJOTg58fX1FxyAiIqLbcP78efj4+Nz0cRahJjg6OgK4MpFOTk6C0xAREZEu1Go1fH1967/Hb4ZFqAnXNoc5OTmxCBEREZmYpnZr4c7SREREZLFYhIiIiMhisQgRERGRxWIRIiIiIovFIkREREQWi0WIiIiILBaLEBEREVksFiEiIiKyWCxCREREZLFYhIiIiMhisQgRERGRxbKIIrR9+3YEBASgS5cu2LBhg+g4REREZCTM/qKrdXV1iI6Oxu7du+Hk5ITQ0FCMHj0abm5uQnOtiDuNI1lFkCQJEgBJwtV//nUfkK4bB6Rr96/+jCuLNHre9fdx/fNu8Do3fY+rOa9drO7vj115vlT/XiqFBKVCgpVSglKhgEohQaW8Nq6o/1mlVNx42frlFVBe9/Nfy14dv/6xq6/Z1AX1iIiIbsbsi9DBgwfRo0cPtG3bFgAwbNgw7Nq1C+PHjxea6/iFEiSkFwjNYC4UEv5WsG5Qmq4+drOCZXV13NnOCi4OVnC1t4abvTVc7K3g5mANF3truNpbwcXeGkoFixcRkbkw+iIUHx+PZcuWISkpCbm5uYiNjUVUVFSDZdasWYNly5YhNzcXPXr0wDvvvIOIiAgAQE5OTn0JAgAfHx9cuHChJT/CDT0b0QHDg9pAlgEZgCzLkAFABmTI1403vI+ry8nyldeRr7t/7XVwg+ddf7/+eU28x61eB9e9n1YGNFoZdVrtlX9qZNRpr940WtRpZWi0Mmo1Vx+/On5lTK5/bt3V5/79/rXXrdVcS9WQVgZq6rSo0fe/pBuQJMDJ1gqu9lZwdbCG67WyZG8NV4e/fnaxt4arw18/W6ssYis0EZHJMfoiVF5ejuDgYDzzzDN49NFHGz0eExOD6dOnY82aNbj33nuxbt06DB06FGlpaWjXrl19MbjerTalVFdXo7q6uv6+Wq3Wzwf5m36d3A3yuubsWumq02r/KlsNylXD0qTRyqhtUM6uljLN34uWDI1Wi5o6LUoqa1FYXoviihoUVdSgsOLqz+U1UFfVQZaBkspalFTWIvNyhc7ZHayVDYvT3352uW4NlKvDlZ/trJUGnE0iIgJMoAgNHToUQ4cOvenjK1aswLPPPovnnnsOAPDOO+9g165dWLt2LZYuXYq2bds2WAOUnZ2NPn363PT1li5dioULF+rvA5DeSJIEpQQoFUrYCPjNrdNoUVx5pRgVlteiqKKm/ucr/6xB0dXiVFhRg+KrP2tloLxGg/KaSmQXVer8fjYqBVyvrmlytbe6+rPV1QJlDTcHq6ub7K6WKAcrONqouM8UEVEzSPKNVpkYKUmSGmwaq6mpgb29PbZs2YJRo0bVL/fSSy8hOTkZv/32G+rq6tCtWzfs2bOnfmfp33//Ha1bt77he9xojZCvry9KSkrg5ORk0M9H5kerlaGuqkVRxZXiVHR9Wbr685WxK8Wp8Gq5utlmwKaoFBJcHawR4uuCyABPDAzwgLeLnZ4/FRGR8VOr1XB2dm7y+9vo1wjdSkFBATQaDby8vBqMe3l5IS8vDwCgUqmwfPlyREZGQqvVYvbs2TctQQBgY2MDGxsbg+Ymy6FQSHC5uganAxx0eo4syyiv0dQXpOvLUqPiVF5TvwaqqvbK5r5LpdWIS7uIuLSLAICudzliYIAnIgM8EOrnCisl91ciIrrGpIvQNX/fFCDLcoOxkSNHYuTIkS0di+i2SJKEVjYqtLJRwdfNXufnVdVqUFRRg9ySKiSmF2D3qXwcOV+Mk3mlOJlXivd/OwtHWxX6+3sgMsATA/w94OHI0k9Els2ki5C7uzuUSmX92p9r8vPzG60lIjJ3tlZKtHG2QxtnO4S2c8W/BnVBYXkNEtIvYffJfPx2+hKKKmqx42gudhzNBQAE+TjXry0K8nHhqQGIyOKYdBGytrZGWFgY4uLiGuwjFBcXh0ceeURgMiLj4OZgjUdC2uKRkLbQaGWkZBdjz8l87D51CcculOBo9pXbql/S4eZgjQH+HhgY4IH+XTzg6mAtOj4RkcEZfREqKyvDmTNn6u9nZGQgOTkZbm5uaNeuHaKjo/HUU08hPDwcffv2xfr165GVlYUpU6YITE1kfJQKCaHtXBHazhXRgwOQX1qF305dwp5TlxB/+hIKy2sQe+QCYo9cgEICerVzRWSABwYGeKKHtxOPRiMis2T0R43t2bMHkZGRjcYnTpyIjRs3ArhyQsW33noLubm5CAwMxMqVK9G/f3+9vL+ue50TmbJajRaH/yzC7lOXsOdUPk7mlTZ43NPRBgMDruxbdG8XdzjZWglKSkSkG12/v42+CInGIkSWKKe4ErtP5WP3yUtIPFOAylpN/WMqhYTw9q6IDPDE/V090dmzFdcWEZHRYRHSExYhsnTVdRoczCjE7pNX1hadKyhv8HhbFztEdr2ytqhvp9awtzb6Le5EZAFYhPSERYioocyCcuw5dWWH6/3nLqOmTlv/mLVKgXs6tkbk1c1o7d11O3cSEZG+sQjpCYsQ0c1V1miw/1wBdp+8hF9P5uNCccNLiHRwd6jft+juDm6wteL104ioZbAI6QmLEJFuZFnG2Utl+PXklX2L/sgsRJ32rz8vdlZK3Nu59ZXzFnX1RFte+oOIDIhFSE9YhIhuT2lVLRLPXFlbtPtUPvJLqxs87u/V6ur10DwR3p6X/iAi/WIR0hMWIaI7J8sy0nLV2HPqylmuD2cV4bqVRXC0USHC3x0DAzwx0N8Dnk624sISkVlgEdITFiEi/SuuqEF8egH2nMzHnqsnc7xeYFsnRAZ44rFw32Zdb42I6BoWIT1hESIyLI1WxrELJdh9Mh97TuUjJbuk/rFWNiq8+WgQhge1EZiQiEwRi5CesAgRtaxLpdWIP30JXxz4E4ezigEAT/drj1eHdYO1ivsREZFudP3+5l8VIjIqHo42eDTMB5v/0RdTB3YCAGzcl4mx6/Yju6hCcDoiMjcsQkRklFRKBeY81BUfTgyHs50VUs4XY/iqvfj15EXR0YjIjLAIEZFRG9TNC9v/dR+CfZxRUlmLSRsP4c0fT6JOo236yURETWARIiKj5+tmj81T+uLpfu0BAGv3nMX/bTiAfHWV2GBEZPJYhIjIJNiolFgwsgdW/18vOFgrcTCjEMNWJWDfmQLR0YjIhLEIEZFJeTjIG9//6z50vcsRBWU1ePLDA/jfL+nQankALBE1H4sQEZmcjh6tEPvCvRgb5gOtDCyPO41nNv7R6MSMRERNYREiIpNkZ63EsrHBeGtMEGytFPjt9CUMX5WApD+LREcjIhPCIkREJu2xcF9s++e96OjugNySKoxbtx8bEs6B54olIl2wCBGRyet6lxO+ffFeDA9qgzqtjDd2nMDUzw9DXVUrOhoRGTkWISIyC462Vlg9vhcWPdIDVkoJPx7Pw8Or9iL1QknTTyYii8UiRERmQ5IkTOjbHlum9ENbFztkFVZg9Np92HQgi5vKiOiGWISIyOyE+Lpgx7T7MKirJ2rqtHg19hiiN6egoqZOdDQiMjIsQkRkllzsrfHBhHDMeagrlAoJsUcu4JHViUi/WCo6GhEZERYhIjJbCoWEqQM7YdNzfeDpaIP0/DKMXJ2IbUcuiI5GREaCRYiIzF6fjq2xY1oE+nVqjcpaDabHJOPV2GOoqtWIjkZEgrEIEZFF8HC0wWfP9sG0+ztDkoBNB7Iw5v19yLpcIToaEQnEIkREFkOpkBA9OAAbn7kbrvZWSL2gxvD/JWDX8TzR0YhIEBYhIrI4A/w9sGNaBML8XFFaVYd/fJaEN7anoVajFR2NiFoYixARWSRvFzt89fw9eO6+DgCADXsz8Pj635FbUik4GRG1JBYhIrJYVkoFXnu4O95/MgyONiok/VmE4av2Iv70JdHRiKiFsAgRkcV7KPAubJ92H3p4O6GwvAYTPz6IFT+dgkbLs1ETmTsWISIiAH6tHfD11H74vz7tIMvAql/PYMJHB3CptFp0NCIyIBYhIqKrbK2UWDKqJ1aOC4adlRKJZy5j+KoEHMwoFB2NiAyERYiI6G9G9fLBdy/ei86erZBfWo3xH/yO9387Cy03lRGZHRYhIqIb6OLliG//eS+iQryh0cr47w8nMfnTQyiuqBEdjYj0iEWIiOgmHGxUWDkuBEtG9YS1SoFfTuZj+Kq9SDlfLDoaEekJixAR0S1IkoT/69MO30zth3Zu9rhQXImx7+/Hp/szIcvcVEZk6liEiIh0ENjWGd//6z4M6eGFGo0W8789jn99eQRl1XWioxHRHWARIiLSkbOdFd5/MgyvDe8GlULC9qO5GPm/vTiZpxYdjYhuE4sQEVEzSJKE5yI6IuYf96CNsy3OFZQj6r1EbDl0XnQ0IroNLEJERLchzM8NO6ZFoL+/B6pqtXh561HM3pqCyhqN6GhE1AwsQkREt8nNwRobn+6NmQ/6QyEBmw9lY9SaRJy7VCY6GhHpiEWIiOgOKBQS/jWoCz5/tg/cW1njZF4pRq5OxI6juaKjEZEOWISIiPSgX2d37JgWgbs7uKGsug7/3HQYC747jpo6rehoRHQLLEJERHri5WSLTc/1wZQBnQAAG/dlYuy6/cguqhCcjIhuhkWIiEiPVEoFXhnaFR9ODIeznRVSzhdj7Pv7kV9aJToaEd0AixARkQEM6uaF7f+6Dx09HJBbUoUpnyWhuo5HlBEZGxYhIiID8XWzx4cTe8PJVoXDWcWYF5vKy3IQGRkWISIiA+rg7oD3ngiFQgK2JmXjw70ZoiMR0XVYhIiIDCyiiwdeG94dALBk5wnsPpUvOBERXcMiRETUAp65tz3GhftCKwPTNh3BmXyedJHIGLAIERG1AEmS8J+oQPRu74rS6jpM/vQQSipqRccisngsQkRELcRapcDaJ8PQ1sUOGQXlePHLw6jT8ISLRCKxCBERtSD3VjZYPyEMdlZKJKQXYMnOk6IjEVk0FiEiohbWw9sZKx4LBgB8lJiBzX+cF5yIyHKxCBERCTC0ZxtMf6ALAGDetmP4I7NQcCIiy8QiREQkyLT7u2BYz7tQq5Ex5bMkXCiuFB2JyOKwCBERCaJQSHh7bDC6t3HC5fIaPPfJIVTU1ImORWRRWISIiASyt1bhg4nhcG9ljRO5aszcnAKtlpfhIGopLEJERIK1dbHD+0+GwUop4YfUPKz6NV10JCKLYfZF6Pz58xg4cCC6d++OoKAgbNmyRXQkIqJGwtu7YXFUTwDAOz+n44djuYITEVkGsy9CKpUK77zzDtLS0vDzzz9jxowZKC8vFx2LiKiRx3r7YtK9HQAA0ZtTcDynRHAiIvNn9kWoTZs2CAkJAQB4enrCzc0NhYU8TJWIjNOrw7oioos7Kms1mPzJIVwqrRYdicisCS9C8fHxGDFiBLy9vSFJErZt29ZomTVr1qBDhw6wtbVFWFgYEhISbuu9Dh06BK1WC19f3ztMTURkGCqlAqvHh6KjuwNySqow9fMkVNdpRMciMlvCi1B5eTmCg4OxevXqGz4eExOD6dOnY968eThy5AgiIiIwdOhQZGVl1S8TFhaGwMDARrecnJz6ZS5fvowJEyZg/fr1t8xTXV0NtVrd4EZE1JKc7a3wwcRwONqqcOjPIvx7WypkmUeSERmCJBvRf12SJCE2NhZRUVH1Y3369EFoaCjWrl1bP9atWzdERUVh6dKlOr1udXU1HnzwQUyePBlPPfXULZddsGABFi5c2Gi8pKQETk5Oun0QIiI9+O30JTzz8UFoZWD+w90x6b4OoiMRmQy1Wg1nZ+cmv7+FrxG6lZqaGiQlJWHw4MENxgcPHox9+/bp9BqyLOPpp5/G/fff32QJAoC5c+eipKSk/nb+PK8BRERiDPD3wKvDugEA3tiRhvjTlwQnIjI/Rl2ECgoKoNFo4OXl1WDcy8sLeXl5Or1GYmIiYmJisG3bNoSEhCAkJATHjh276fI2NjZwcnJqcCMiEuXZ+zpgTJgPtDLw4qbDOHepTHQkIrOiEh1AF5IkNbgvy3KjsZu57777oNVqDRGLiMjgJEnC4lGBOHepDIezivHcp4cQ+8K9cLazEh2NyCwY9Rohd3d3KJXKRmt/8vPzG60lIiIyVzYqJd5/KgxtnG1x7lI5pn15BBpehoNIL4y6CFlbWyMsLAxxcXENxuPi4tCvXz9BqYiIWp6noy0+mBAOWysFfjt9CUt3nhAdicgsCC9CZWVlSE5ORnJyMgAgIyMDycnJ9YfHR0dHY8OGDfjoo49w4sQJzJgxA1lZWZgyZYrA1ERELS+wrTOWjw0BAGzYm4Eth3gwB9GdEr6P0KFDhxAZGVl/Pzo6GgAwceJEbNy4EePGjcPly5exaNEi5ObmIjAwEDt37oSfn5+oyEREwgwPaoNTeZ2x6tczmBebio4eDgjzcxMdi8hkGdV5hIyRruchICJqKVqtjKlfJGHX8Ytwb2WD7168F94udqJjERkVsziPEBERNaZQSFjxWAi63uWIgrJqTP70ECpreBkOotvBIkREZIIcbFT4YEI43ByscTxHjVlbU3gZDqLbwCJERGSifN3s8f6TYbBSSthxNBf/+/WM6EhEJodFiIjIhN3dwQ3/eSQQALAi7jR+TNXtrPtEdAWLEBGRiXv87nZ4ul97AED05mScyFWLDURkQliEiIjMwGvDu+G+zu6oqNHguU8OoaCsWnQkIpPAIkREZAZUSgVW/18vtG9tjwvFlXjh88OoqeN1FomawiJERGQmXOytsWFiOBxtVDiYWYjXv0vlkWRETWARIiIyI509HbFqfC9IEvDlwfP4ZF+m6EhERo1FiIjIzER29cTcoV0BAP/ZcQJ70wsEJyIyXixCRERmaHJER4zu1RYarYx/bjqMjIJy0ZGIjBKLEBGRGZIkCUtG90Svdi4oqazFc5/8AXVVrehYREaHRYiIyEzZWimx7skw3OVki7OXyvHSl0eg0XLnaaLrsQgREZkxTydbrJ8QBhuVArtPXcJbP54UHYnIqLAIERGZuSAfF7w9NhgAsC7+HL5OyhaciMh4sAgREVmAEcHeeDGyMwBg7jfHcDirSHAiIuPAIkREZCGiH/THg929UKPR4h+fJSG3pFJ0JCLhWISIiCyEQiFh5bgQBHg54lJpNZ7/NAmVNRrRsYiE0rkIpaSk4I033sCaNWtQUNDw5FxqtRqTJk3SezgiItKvVjYqbJgYDld7Kxy7UILZXx/lZTjIoulUhH766Sfcfffd+Oqrr/Dmm2+iW7du2L17d/3jlZWV+OSTTwwWkoiI9MfXzR5rnwyDSiHh+5QcrNlzVnQkImF0KkILFizArFmzkJqaiszMTMyePRsjR47Ejz/+aOh8RERkAPd0bI2Fj/QAACzbdQo/Hc8TnIhIDJ2K0PHjx+s3fUmShJdffhnr16/HmDFj8P333xs0IBERGcYTffwwoa8fAGBGTDJO5qkFJyJqeToVIRsbGxQXFzcYGz9+PD788EM8/vjjiI2NNUQ2IiIysH8/3B19O7ZGeY0Gz31yCIXlNaIjEbUonYpQSEhIg32Crhk3bhw2bNiAadOm6T0YEREZnpVSgTVPhKKdmz2yiyox9fMk1NRpRcciajE6FaGpU6fiwoULN3xs/Pjx+OSTT9C/f3+9BiMiopbh6mCNDRPD0cpGhQMZhVjw/XEeSUYWQ5L5235LarUazs7OKCkpgZOTk+g4REQG88uJi3ju00OQZeA/j/TAU33bi45EdNt0/f7mCRWJiAgAMKibF2YP6QoAWPB9GvadKWjiGUSmr1lFaPPmzYbKQURERmDKgI6ICvGGRivjhU2H8eflctGRiAxK5yL0/vvvY/r06QaMQkREokmShP8+GoRgH2cUV9TiuU8OobSqVnQsIoPRqQi98cYbeO211/DDDz8YOg8REQlma6XE+gnh8HS0QXp+GaZ/lQyNlruTknlqsghNnz4dy5Ytw44dOxAcHNwSmYiISDAvJ1usnxAOa5UCv5zMx7Jdp0RHIjKIJovQqlWrsHz5cvTp06cl8hARkZEI8XXBsjFBAID3fzuLbUdufBoVIlPWZBF69NFH8frrr+PcuXMtkYeIiIzIIyFtMXVgJwDAnK+P4kx+qeBERPrVZBHavHkzHn74YQwaNOimJ1UkIiLzNWtwACK6uKO6TotpXyajuk4jOhKR3jRZhCRJwrp16/D444/j/vvvb4lMRERkRJQKCW+PDYarvRXSctVY8dNp0ZGI9Ebnw+eXLl2KqVOnGjILEREZKS8nW/z30Sv7C61POMeTLZLZaNYJFXkeISIiyzWkx10Yf7cvZBmI3pyC4gpeqZ5MHy+xQUREOvv3w93R0d0BeeoqvBp7jBdnJZPX7CJ06tQpvPjiixg0aBAeeOABvPjiizh1iueXICKyBPbWKrzzeAhUCgk7j+Vha1K26EhEd6RZRWjr1q0IDAxEUlISgoODERQUhMOHDyMwMBBbtmwxVEYiIjIiQT4umPGgPwBgwXfHkVnA65GR6ZLkZqzX7NixI5588kksWrSowfjrr7+Ozz77zCzPNaRWq+Hs7IySkhI4OTmJjkNEZBQ0WhnjP/gdBzMKEeLrgi1T+sJKyb0tyHjo+v3drN/avLw8TJgwodH4k08+iby8vOanJCIik6RUSFg5LgSOtiokny/G/349IzoS0W1pVhEaOHAgEhISGo3v3bsXERERegtFRETGr62LHRaP6gkAWP1rOg5lFgpORNR8quYsPHLkSMyZMwdJSUm45557AAC///47tmzZgoULF+K7775rsCwREZm3kcHe2H0yH7FHLmB6TDJ+eCkCjrZWomMR6axZ+wgpFLqtQJIkCRqNeZyCnfsIERHdmrqqFsPeTUB2USVG92qLFeNCREciMsw+QlqtVqebuZQgIiJqmpOtFd4ZFwKFBHxz5AK+S8kRHYlIZzoXoeLiYlRXVwMAampqUFxcbKhMRERkYsLbu+HFyM4AgHmxx3ChuFJwIiLd6FyENm3ahCVLlgAAlixZgi+//NJgoYiIyPT8a1AXhPi6oLSqDjNikqHR8qzTZPx0LkIvvPACDh48iF27duHAgQO8ACsRETVgpVTgnXEhsLdW4mBGIdbFnxUdiahJOhWhZ555BpMmTUJdXR1GjBgBjUaDSZMmYdKkSYbOR0REJqS9uwMWjOwBAFjx02kczS4WG4ioCTodPr9gwQIAwPvvv4/a2lqEh4fjH//4hyFzERGRiRob5oM9p/Kx81gepn+VjO3T7oO9dbPO1kLUYnRaI+Tn5weNRoPExET8/PPPSExMhFarhZ+fn6HzERGRiZEkCUtG9cRdTrY4V1CO/2w/IToS0U3pvI/Q77//jjfffBMqlQpvv/029u/fb8hcRERkwlzsrbHisWBIEvDlwSzsOs7LMJFxatYJFS0RT6hIRHT7luw8gfXx5+Bqb4Vd0/vD08lWdCSyELp+fzdro21mZiYSEhKQmZmJiooKeHh4oFevXujbty9sbfnLTUREDc0c7I+96QVIy1Vj5pYUfPLM3VAoJNGxiOrpVIQ2bdqEVatW4eDBg/D09ETbtm1hZ2eHwsJCnD17Fra2tnjiiScwZ84c7jdERET1bFRKrBofguGr9iIhvQAf78vEs/d1EB2LqF6T+wiFhoZixYoVePLJJ5GZmYm8vDwkJSVh7969SEtLg1qtxrfffgutVovw8HBs2bKlJXITEZGJ6OzpiNeGdwMAvPnDSZzIVQtORPSXJvcR2rFjB4YPH67TixUUFCAjIwO9e/fWSzhjwH2EiIjunCzLePaTQ/j1ZD4CvBzx7Yv3wtZKKToWmTG9XXRV1xIEAO7u7mZVgoiISD8kScJbY4Lg3soapy6W4s0fT4qORASgmTtLA4BGo0FsbCxOnDgBSZLQtWtXREVFQaXiybKIiOjm3FvZYNmYYDyz8Q98nJiJAf4eGBjgKToWWTidzyMEAKmpqfD398fEiRMRGxuLb775Bk8//TS6dOmCY8eOGSqjXlRUVMDPzw+zZs0SHYWIyGJFdvXExL5XDqqZteUoLpdVC05Elq5ZRei5555Djx49kJ2djcOHD+Pw4cM4f/48goKC8Pzzzxsqo14sXrwYffr0ER2DiMjizR3WDV08W6GgrBpzvj4Gns6ORGpWEUpJScHSpUvh6upaP+bq6orFixcjOTlZ39n0Jj09HSdPnsSwYcNERyEisni2Vkq883gIrJUK/HziIjYdzBIdiSxYs4pQQEAALl682Gg8Pz8fnTt3vq0A8fHxGDFiBLy9vSFJErZt29ZomTVr1qBDhw6wtbVFWFgYEhISmvUes2bNwtKlS28rHxER6V8Pb2fMfigAAPCf7Wk4k18mOBFZqmYVoSVLlmDatGnYunUrsrOzkZ2dja1bt2L69Ol48803oVar62+6Ki8vR3BwMFavXn3Dx2NiYjB9+nTMmzcPR44cQUREBIYOHYqsrL/+DyIsLAyBgYGNbjk5Ofj222/h7+8Pf39/nfJUV1c3+BzN+SxERKS7Sfd2wH2d3VFVq8X0mCOoqdOKjkQWqFnXGlMo/upNknTlFOnXnn79fUmSoNFomh9GkhAbG4uoqKj6sT59+iA0NBRr166tH+vWrRuioqJ0Wsszd+5cfP7551AqlSgrK0NtbS1mzpyJ+fPn33D5BQsWYOHChY3GeR4hIiL9yyupwkPvxqO4ohZTBnTCK0O7io5EZkLX8wg1qwj99ttvOgcYMGCAzsvWh/lbEaqpqYG9vT22bNmCUaNG1S/30ksvITk5uVl5AGDjxo1ITU3F22+/fdNlqqurUV3911EMarUavr6+LEJERAbyY2oupnx+GJIEbHruHvTt1Fp0JDIDBrno6u2UmztRUFAAjUYDLy+vBuNeXl7Iy8szyHva2NjAxsbGIK9NRESNPRTYBuPCfRFz6DyiNyfjx5f6w9neSnQsshAmcRbEa5vdrrm2+a25nn76aT0lIiIifZo/ojsOZFxG5uUKvLrtGFaP73Vbf+eJmqtZO0u3NHd3dyiVykZrf/Lz8xutJSIiItPlYKPCO4/3glIhYcfRXHxz+ILoSGQhjLoIWVtbIywsDHFxcQ3G4+Li0K9fP0GpiIjIEEJ8XTDjgS4AgPnfpiLrcoXgRGQJhBehsrIyJCcn15+QMSMjA8nJyfWHx0dHR2PDhg346KOPcOLECcyYMQNZWVmYMmWKwNRERGQIUwd2Ru/2riiv0WB6zBHUaXhIPRlWs44aM4Q9e/YgMjKy0fjEiROxceNGAFdOqPjWW28hNzcXgYGBWLlyJfr3798i+XTd65yIiPTjfGEFhr2bgNLqOkx/oAumP6DbeeCIrmeQw+dv5f7770dkZCRmzpwJe3t7fbykUWARIiJqed8mX8BLXyVDqZCw+R99Eebn2vSTiK6j6/e33jaN+fn54ddff0W3bt309ZJERGShHglpi0dCvKHRypgecwSlVbWiI5GZ0vumsbKyMrRq1UqfLykU1wgREYlRUlmLYe8m4EJxJR4N9cHyx4JFRyITovc1QrW1tYiMjMTp06dvuZw5lSAiIhLH2c4KK8eFQCEBXx/OxvajOaIjkRnSuQhZWVkhNTWVJ7giIqIWc3cHN7wwsDMA4NVvjiGnuFJwIjI3zdpHaMKECfjwww8NlYWIiKiRlx7ogmAfZ6ir6hC9ORkardCDncnMNOsSGzU1NdiwYQPi4uIQHh4OBweHBo+vWLFCr+GIiIislAq883gvDF+VgN/PFeKDhHOYMqCT6FhkJppVhFJTUxEaGgoAjfYV4iYzIiIylA7uDnh9RHfM+foYlv90Cvd1dkdgW2fRscgMCD+horHjUWNERMZBlmVM/fwwfjyeh44eDtjxrwjYWStFxyIj1eLnESIiIjIkSZKwdHRPeDnZ4NylcryxI010JDIDTRahKVOm4Pz58zq9WExMDL744os7DkVERHQjrg7WWD42BADwxYEsxKVdFBuITF6T+wh5eHggMDAQ/fr1w8iRIxEeHg5vb2/Y2tqiqKgIaWlp2Lt3L7766iu0bdsW69evb4ncRERkoe7r4o7n7uuADXszMOfrowj2jYCno63oWGSidNpHKD8/Hx9++CG++uorpKamNnjM0dERDzzwAJ5//nkMHjzYYEFF4T5CRETGp7pOg0dWJ+JkXikG+Htg4zO9edAONWCwi64WFxfjzz//RGVlJdzd3dGpUyez/uVjESIiMk6nL5ZixP/2orpOiwUjuuPpezuIjkRGRG87S48ePRpqtRoA8Omnn8LOzg7BwcG455570LlzZ7MuQUREZLz8vRzx6rArF/pe8sNJnMorFZyITFGTRWj79u0oLy8HADzzzDMoKSkxeCgiIiJdTOjrh8gAD9TUafHSV0dQVasRHYlMTJM7S3ft2hVz585FZGQkZFnG5s2bb7qKacKECXoPSEREdDOSJOGtMcF46J14nMwrxbJdp/Dvh7uLjkUmpMl9hPbt24fo6GicPXsWhYWFcHR0vOHmMEmSUFhYaLCgonAfISIi4/fLiYt49pNDAIBPJ92N/v4eghORaAbZWVqhUCAvLw+enp56CWkKWISIiEzDv7el4rPf/4Snow1+nN4fbg7WoiORQAY5s3RGRgY8PNiyiYjI+Lw6rBs6eTggv7Qac74+Cl5BinTRrCLk5+fHo8SIiMgo2Vkr8e7jvWCllBCXdhFf/aHbVRHIsvFaY0REZDYC2zpj1uAAAMCi79Nw7lKZ4ERk7FiEiIjIrEyO6Ih+nVqjslaD6THJqNVoRUciI8YiREREZkWhkLD8sWA421nhaHYJ3vn5tOhIZMRYhIiIyOy0cbbD0tE9AQBr9pzFgXOXBSciY9XsIvTCCy+goKCg0c9ERETGZFjPNhgb5gNZBmbEJKOkslZ0JDJCzS5Cn3/+ef21x67/mYiIyNi8PrIH/FrbI6ekCq9tS+Uh9dRIs4vQ9b9E/IUiIiJj1spGhZXjQqBUSPg+JQfbki+IjkRGhvsIERGRWQtt54qXBnUBAMzfdhznCysEJyJjwiJERERm74WBnRDm54rS6jrMiEmGRsstGnQFixAREZk9lVKBd8aFoJWNCof+LMK6+LOiI5GRYBEiIiKL4Otmj/kjugMAVsadxolcHuxDd1iEeN0xIiIyJWPDfPBAN0/UamREb05BTR3POm3p7qgI8agxIiIyJZIkYenoILg5WONErhrv/sKzTlu6ZhehtLQ0tG/fvv5nPz8/fWciIiIyGA9HGyyOCgQArN1zFoezigQnIpGaXYR8fX2hUCjqf1YqlXoPRUREZEhDe7ZBVIg3tDIwa3MKKms0oiORINxZmoiILNLCkYG4y8kW5wrK8eaPJ0XHIUFYhIiIyCI521vhzTFBAICN+zKReIbXzrRELEJERGSxBvh74Ik+7QAAL29JgbqKF2a1NCxCRERk0V4d1q3+wqwLv0sTHYdaGIsQERFZNAcbFZaPDYYkAV8fzsZPx/NER6IWpNJ1wZKSEsTGxiIhIQGZmZmoqKiAh4cHevXqhSFDhqBfv36GzElERGQw4e3d8Hz/jlj32zm8GnsMYX6uaN3KRnQsagFNrhHKzc3F5MmT0aZNGyxatAjl5eUICQnBoEGD4OPjg927d+PBBx9E9+7dERMT0xKZiYiI9C76QX8EeDmioKwG82JTedJgC9HkGqHg4GA89dRTOHjwIAIDA2+4TGVlJbZt24YVK1bg/PnzmDVrlt6DEhERGZKNSonljwUj6r1E/Hg8D9uSL2BULx/RscjAJLmJynvp0iV4eHjo/ILNXd7YqdVqODs7o6SkBE5OTqLjEBGRgf3vl3QsjzsNR1sVdk3vD28XO9GR6Dbo+v3d5KYxDw8PTJo0CaWlpTq9sTmVICIisjxTB3ZCsK8LSqvqMOfro9xEZuZ0Omrsk08+QWVlpaGzEBERCadSKrDisWDYqBRISC/A57//KToSGZBORYhtmIiILEknj1Z4ZWhXAMCSnSeRWVAuOBEZis7nEZIkyZA5iIiIjMrEvu3Rt2NrVNZqMHNLCjRarhQwRzoXIX9/f7i5ud3yRkREZC4UCgnLxgahlY0KSX8WYX38OdGRyAB0PqHiwoUL4ezsbMgsRERERsXH1R7zR3TH7K1HsTLuNAYGeKBbGx5BbE6aPHweABQKBfLy8uDp6dkSmYwKD58nIrJssixj8qeH8POJfHRr44Rv/3kvrFW8QpWx09vh8wD3DyIiIsslSRKWjg6Cm4M1TuSq8e4vp0VHIj3iUWNERERN8HC0weKoK1dXWLvnLA5nFQlORPqiUxHSarUWuVmMiIjomqE92yAqxBtaGZi1OQWVNRrRkUgPuJGTiIhIRwtHBuIuJ1ucKyjHmz+eFB2H9IBFiIiISEfO9lZ4c0wQAGDjvkwknikQnIjuFIsQERFRMwzw98ATfdoBAF7ekgJ1Va3gRHQnWISIiIia6dVh3eDX2h45JVVY+F2a6Dh0B/RWhOLj41FSUqKvlyMiIjJaDjYqLB8bDEkCvj6cjZ+O54mORLdJb0Vo4MCB6NixI5YvX66vlyQiIjJa4e3d8Hz/jgCAV2OP4XJZteBEdDv0VoQyMjLw9ddfo6DA+HYcy8jIQGRkJLp3746ePXuivJxXESYiojsX/aA/ArwcUVBWg3mxqTzvngnS6RIbAKDRaLB3714EBQXB1dXV0Ln0asCAAXjjjTcQERGBwsJCODk5QaXS7TJrvMQGERHdSuqFEkS9l4g6rYyV44IxqpeP6EgEPV9iAwCUSiWGDBmC4uJifeRrMcePH4eVlRUiIiIAAG5ubjqXICIioqYEtnXGS4O6AADmf3scuSWVghNRczRr01jPnj1x7tw5vQaIj4/HiBEj4O3tDUmSsG3btkbLrFmzBh06dICtrS3CwsKQkJCg8+unp6ejVatWGDlyJEJDQ7FkyRI9piciIgKmDuyEYF8XlFbVYfbWo9xEZkKaVYQWL16MWbNmYfv27cjNzYVarW5wux3l5eUIDg7G6tWrb/h4TEwMpk+fjnnz5uHIkSOIiIjA0KFDkZWVVb9MWFgYAgMDG91ycnJQW1uLhIQEvPfee9i/fz/i4uIQFxd30zzV1dV6+VxERGQ5VEoFVjwWDFsrBRLSC/D573+KjkQ60nkfIQBQKP7qTddfkV6WZUiSBI3mzq67IkkSYmNjERUVVT/Wp08fhIaGYu3atfVj3bp1Q1RUFJYuXdrka+7fvx8LFy7Ejz/+CABYtmwZAODll1++4fILFizAwoULG41zHyEiImrKx4kZWPh9GuyslPjhpQi0d3cQHcli6bqPULN2ltm9e/cdB2uOmpoaJCUl4ZVXXmkwPnjwYOzbt0+n1+jduzcuXryIoqIiODs7Iz4+Hv/4xz9uuvzcuXMRHR1df1+tVsPX1/f2PgAREVmUiX3b46fjF7H/3GXM3JKCzf/oC6VCavqJJEyzitCAAQMMleOGCgoKoNFo4OXl1WDcy8sLeXm6nbxKpVJhyZIl6N+/P2RZxuDBg/Hwww/fdHkbGxvY2NjcUW4iIrJMCoWEZWOD8NA7CUj6swjr489h6sBOomPRLTS5j9D1++Lo4sKFC7cd5mau3wwH/LUpTldDhw7FsWPHkJqaihUrVug7HhERUT0fV3vMH9EdALAy7jRO5nFfU2PWZBHq3bs3Jk+ejIMHD950mZKSEnzwwQcIDAzEN998o7dw7u7uUCqVjdb+5OfnN1pLREREZCzGhvnggW6eqNFoMSMmBTV1WtGR6Caa3DR24sQJLFmyBA899BCsrKwQHh4Ob29v2NraoqioCGlpaTh+/DjCw8OxbNkyDB06VG/hrK2tERYWhri4OIwaNap+PC4uDo888oje3oeIiEifJEnC0tFBOPxOPE7kqrHql3TMGhIgOhbdQJNrhNzc3PD2228jJycHa9euhb+/PwoKCpCeng4AeOKJJ5CUlITExMTbKkFlZWVITk5GcnIygCuXw0hOTq7fJBcdHY0NGzbgo48+wokTJzBjxgxkZWVhypQpzX4vIiKiluLhaIPFUYEAgDV7zuBwVpHgRHQjzTp83hD27NmDyMjIRuMTJ07Exo0bAVw5oeJbb72F3NxcBAYGYuXKlejfv3+L5OMlNoiI6E5M/+oItiXnoKO7A3ZMi4CdtVJ0JIug6/e3TkVo9OjRTb6hSqXCXXfdhQcffBAjRoxoXlojxiJERER3oqSiFkPeiUeeugpP92uPBSN7iI5kEfR6rTFnZ+cmb3Z2dkhPT8e4ceMwf/58vX0QIiIiU+Zsb4U3xwQBADbuy0TimQLBieh6et80tmPHDkydOrXZh90bK64RIiIifZgXewxfHMiCt7MtfpzRH062VqIjmTW9X31eV/feey/Cw8P1/bJEREQm7dVh3eDX2h45JVVY9H2a6Dh0ld6LkIuLi17PJURERGQOHGxUWD42GJIEbE3KRlzaRdGRCAYoQkRERHRj4e3d8Hz/jgCAud8cxeWyasGJiEWIiIioBUU/6I8AL0cUlNVgXmwqBJ/FxuKxCBEREbUgG5USyx8Lhkoh4cfjediWrP9rdJLuWISIiIhaWGBbZ7w0qAsAYP63x5FbUik4keViESIiIhJg6sBOCPZ1QWlVHWZvPcpNZIKwCBEREQmgUiqw4rFg2FopkJBegM8PmMf590wNixAREZEgnTxaYc5DXQEAS3acQGZBueBElodFiIiISKCJfdujb8fWqKzVYOaWFGi03ETWkliEiIiIBFIoJCwbG4RWNiok/VmE9fHnREeyKCxCREREgvm42mP+iO4AgJVxp3EyTy04keVgESIiIjICY8N88EA3L9RotJgRk4KaOq3oSBaBRYiIiMgISJKEpaN7ws3BGidy1Vj1S7roSBaBRYiIiMhIeDjaYHFUIABgzZ4zOJJVJDiR+WMRIiIiMiJDe7ZBVIg3tDIwc3MKKms0oiOZNRYhIiIiI7NwZCDucrLFuYJyvPnjSdFxzBqLEBERkZFxtrfCm2OCAAAb92Ui8UyB4ETmi0WIiIjICA3w98ATfdoBAF7ekgJ1Va3gROaJRYiIiMhIvTqsG/xa2yOnpAqLvk8THccssQgREREZKQcbFZaPDYYkAVuTshGXdlF0JLPDIkRERGTEwtu74fn+HQEAc785istl1YITmRcWISIiIiMX/aA/ArwcUVBWg9e2pUKWeWFWfWERIiIiMnI2KiWWPxYMlULCD6l5+DY5R3Qks8EiREREZAIC2zrjpUFdAAD//jYVuSWVghOZBxYhIiIiEzF1YCcE+7qgtKoOs7ce5SYyPWARIiIiMhEqpQIrHguGrZUCCekF+OJAluhIJo9FiIiIyIR08miFOQ91BQAs2XkCWZcrBCcybSxCREREJmZi3/a4p6MbKmo0mLUlBVotN5HdLhYhIiIiE6NQSFg2JhgO1koczCzER4kZoiOZLBYhIiIiE+TrZo/XHu4OAHhr1ymcyS8VnMg0sQgRERGZqMd7+2KAvwdq6rSYuTkFdRqt6Egmh0WIiIjIREmShDcfDYKTrQop2SVYF39OdCSTwyJERERkwu5ytsXCR3oAAN75+TTSctSCE5kWFiEiIiITFxXSFkN6eKFWIyN6czJq6riJTFcsQkRERCZOkiQsHtUTbg7WOJlXilW/pIuOZDJYhIiIiMyAeysbLI4KBACs/e0sks8Xiw1kIliEiIiIzMTQnm3wSIg3NFoZMzcno6pWIzqS0WMRIiIiMiMLR/aAp6MNzl4qx9u7TomOY/RYhIiIiMyIi7013nw0CADwYWIGDmYUCk5k3FiEiIiIzExkV0+MC/eFLAOztqSgvLpOdCSjxSJERERkhl57uBvautghq7ACS384ITqO0WIRIiIiMkOOtlZYNubKJrLPf89C/OlLghMZJxYhIiIiM9Wvszue7tceADDn66MoqawVG8gIsQgRERGZsdkPBaB9a3vkllThP9vTRMcxOixCREREZszeWoXljwVDIQFbk7IRl3ZRdCSjwiJERERk5sL83DC5f0cAwNxvjqGwvEZwIuPBIkRERGQBZjzgjy6erVBQVo1/f5sqOo7RYBEiIiKyALZWSqx4LARKhYQdR3PxfUqO6EhGgUWIiIjIQvT0ccaLkZ0BAP/+NhX5pVWCE4nHIkRERGRBXry/M3p4O6G4ohZzvz4GWZZFRxKKRYiIiMiCWCkVWPFYCKyVCvxyMh9bk7JFRxKKRYiIiMjCBNzliBkP+gMAFn2fhpziSsGJxGERIiIiskDP9++IXu1cUFpdh9lbj1rsJjIWISIiIgukVEhYPjYYtlYK7D1TgM8PZImOJASLEBERkYXq6NEKcx7qCgBYsuME/rxcLjhRy2MRIiIismAT+7bHPR3dUFmrwctbjkKjtaxNZCxCREREFkyhkLBsTDAcrJU4mFmIjxMzREdqURZRhFauXIkePXqge/fumDZtmsXuEEZERHQjvm72+PfD3QEAb+06hTP5pYITtRyzL0KXLl3C6tWrkZSUhGPHjiEpKQm///676FhERERGZVxvXwwM8EBNnRYzN6egTqMVHalFmH0RAoC6ujpUVVWhtrYWtbW18PT0FB2JiIjIqEiShP+ODoKTrQop2SV4/7ezoiO1COFFKD4+HiNGjIC3tzckScK2bdsaLbNmzRp06NABtra2CAsLQ0JCgs6v7+HhgVmzZqFdu3bw9vbGAw88gE6dOunxExAREZmHu5xtseiRQADAu7+k43hOieBEhie8CJWXlyM4OBirV6++4eMxMTGYPn065s2bhyNHjiAiIgJDhw5FVtZf5zsICwtDYGBgo1tOTg6Kioqwfft2ZGZm4sKFC9i3bx/i4+Nvmqe6uhpqtbrBjYiIyFI8EuKNh3rchVqNjJmbU1BdpxEdyaAk2Yj2HJYkCbGxsYiKiqof69OnD0JDQ7F27dr6sW7duiEqKgpLly5t8jW3bNmCPXv24L333gMALFu2DLIsY/bs2TdcfsGCBVi4cGGj8ZKSEjg5OTXzExEREZmegrJqDF4Zj8LyGvwzshNeHtJVdKRmU6vVcHZ2bvL7W/gaoVupqalBUlISBg8e3GB88ODB2Ldvn06v4evri3379qGqqgoajQZ79uxBQEDATZefO3cuSkpK6m/nz5+/o89ARERkatxb2WDJqCubyNbuOYsjWUWCExmOURehgoICaDQaeHl5NRj38vJCXl6eTq9xzz33YNiwYejVqxeCgoLQqVMnjBw58qbL29jYwMnJqcGNiIjI0jwU2AZRId7QysDMLSmoqjXPTWRGXYSukSSpwX1ZlhuN3crixYtx4sQJHD9+HKtWrWrWc4mIiCzVwpGB8HKywblL5Vi265ToOAZh1EXI3d0dSqWy0dqf/Pz8RmuJiIiISL+c7a3w30eDAAAfJWbgwLnLghPpn1EXIWtra4SFhSEuLq7BeFxcHPr16ycoFRERkeWIDPDE4719IcvArK0pKK+uEx1Jr4QXobKyMiQnJyM5ORkAkJGRgeTk5PrD46Ojo7FhwwZ89NFHOHHiBGbMmIGsrCxMmTJFYGoiIiLLMW94N7R1scP5wkos2XlCdBy9UokOcOjQIURGRtbfj46OBgBMnDgRGzduxLhx43D58mUsWrQIubm5CAwMxM6dO+Hn5ycqMhERkUVxtLXCsrFB+L8PDuCLA1kY3OMuDPD3EB1LL4zqPELGSNfzEBAREZm7Bd8dx8Z9mbjLyRa7ZvSHs52V6Eg3ZRbnESIiIiLjMeehrujg7oA8dRUWfn9cdBy9YBEiIiIindhZK/H22CAoJOCbwxfw03HdzulnzFiEiIiISGdhfm54vv+Vi5e/GnsMheU1ghPdGRYhIiIiapYZD3aBv1crFJTV4LVtx2DKuxuzCBEREVGz2KiUWD42BCqFhJ3H8vD90VzRkW4bixARERE1W08fZ7x4f2cAwPxvU5GvrhKc6PawCBEREdFt+WdkZwS2dUJxRS3mfmOam8hYhIiIiOi2WCkVWD42BNZKBX45mY8tSdmiIzUbixARERHdtoC7HBE92B8AsOj7NFworhScqHlYhIiIiOiOTI7oiNB2LiirrsOcrUeh1ZrOJjIWISIiIrojSoWE5Y+FwNZKgb1nCvDFgT9FR9IZixARERHdsQ7uDnjloa4AgCU7TyKzoFxwIt2wCBEREZFeTOjbHn07tkZlrQaztqRAYwKbyFiEiIiISC8UCglvjQlCKxsVDv1ZhI/2ZoiO1CQWISIiItIbXzd7vDa8GwBg2U+nkH6xVHCiW2MRIiIiIr0a19sXAwM8UFOnxcwtKajVaEVHuikWISIiItIrSZLw5qNBcLazwtHsEqzdc1Z0pJtiESIiIiK983KyxaJHegAAVv2SjuM5JYIT3RiLEBERERnEyGBvPNTjLtRpZczcnILqOo3oSI2wCBEREZFBSJKEN0YForWDNU7mleLdn9NFR2qERYiIiIgMxr2VDRaPCgQAvP/bWRzOKhKcqCEWISIiIjKohwLbYFSvttDKwKzNKaisMZ5NZCxCREREZHALRvSAl5MNzhWUY9muU6Lj1GMRIiIiIoNztrfCfx8NAgB8lJiB/WcvC050BYsQERERtYjIAE+Mv9sXAPDy1hSUVdcJTsQiRERERC1o3vDu8HG1Q3ZRJZbsPCE6DosQERERtZxWNiq8NebKJrJNB7Lw2+lLQvOwCBEREVGL6tfJHU/3aw8AmLP1KEoqaoVlYREiIiKiFjfnoa7o4O6APHUVvvwjS1gOlbB3JiIiIotlZ63E22ODkZarxpN92gnLwSJEREREQoT5uSLMz1VoBm4aIyIiIovFIkREREQWi0WIiIiILBaLEBEREVksFiEiIiKyWCxCREREZLFYhIiIiMhisQgRERGRxWIRIiIiIovFIkREREQWi0WIiIiILBaLEBEREVksFiEiIiKyWLz6fBNkWQYAqNVqwUmIiIhIV9e+t699j98Mi1ATSktLAQC+vr6CkxAREVFzlZaWwtnZ+aaPS3JTVcnCabVa5OTkwNHREZIkNXisd+/e+OOPP245drP7arUavr6+OH/+PJycnPSe+0bZ9PWcWy13s8c4V7o/didzBcCg83U7c6Xr8ww1V38f41w1b8zU/zvkXOn+nKaWMbW/77Iso7S0FN7e3lAobr4nENcINUGhUMDHx+eGjymVykb/4v4+1tR9Jycng/yHcqNs+nrOrZa72WOcK90f08dcAYaZr9uZK12fZ6i5+vsY56p5Y6b+3yHnSvfnNLWMKf59v9WaoGu4s/Qd+Oc//9nkWFP3DeV23kfX59xquZs9xrnS/TFzmytdn2eoufr7GOeqeWPGPF+cK/0+p6llTPFvli64aUwQtVoNZ2dnlJSUGKQFmxPOVfNwvnTHudId50p3nCvdGcNccY2QIDY2Nnj99ddhY2MjOorR41w1D+dLd5wr3XGudMe50p0xzBXXCBEREZHF4hohIiIislgsQkRERGSxWISIiIjIYrEIERERkcViESIiIiKLxSJkAkpLS9G7d2+EhISgZ8+e+OCDD0RHMlrnz5/HwIED0b17dwQFBWHLli2iIxm1UaNGwdXVFWPGjBEdxehs374dAQEB6NKlCzZs2CA6jlHj75Hu+DdKdy313cfD502ARqNBdXU17O3tUVFRgcDAQPzxxx9o3bq16GhGJzc3FxcvXkRISAjy8/MRGhqKU6dOwcHBQXQ0o7R7926UlZXhk08+wdatW0XHMRp1dXXo3r07du/eDScnJ4SGhuLAgQNwc3MTHc0o8fdId/wbpbuW+u7jGiEToFQqYW9vDwCoqqqCRqMB++uNtWnTBiEhIQAAT09PuLm5obCwUGwoIxYZGQlHR0fRMYzOwYMH0aNHD7Rt2xaOjo4YNmwYdu3aJTqW0eLvke74N0p3LfXdxyKkB/Hx8RgxYgS8vb0hSRK2bdvWaJk1a9agQ4cOsLW1RVhYGBISEpr1HsXFxQgODoaPjw9mz54Nd3d3PaVvWS0xV9ccOnQIWq0Wvr6+d5hajJacK3Nzp3OXk5ODtm3b1t/38fHBhQsXWiJ6i+PvWfPoc75M/W9UU/QxVy3x3ccipAfl5eUIDg7G6tWrb/h4TEwMpk+fjnnz5uHIkSOIiIjA0KFDkZWVVb9MWFgYAgMDG91ycnIAAC4uLkhJSUFGRgY2bdqEixcvtshn07eWmCsAuHz5MiZMmID169cb/DMZSkvNlTm607m70f91SpJk0Myi6OP3zJLoa77M4W9UU/QxVy3y3SeTXgGQY2NjG4zdfffd8pQpUxqMde3aVX7llVdu6z2mTJkib968+XYjGg1DzVVVVZUcEREhf/rpp/qIaRQM+Xu1e/du+dFHH73TiEbrduYuMTFRjoqKqn9s2rRp8hdffGHwrKLdye+Zuf8e3cjtzpc5/o1qij7+hhnqu49rhAyspqYGSUlJGDx4cIPxwYMHY9++fTq9xsWLF6FWqwFcuVJvfHw8AgIC9J5VNH3MlSzLePrpp3H//ffjqaeeMkRMo6CPubJUuszd3XffjdTUVFy4cAGlpaXYuXMnhgwZIiKuUPw9ax5d5stS/kY1RZe5aqnvPpXeX5EaKCgogEajgZeXV4NxLy8v5OXl6fQa2dnZePbZZyHLMmRZxosvvoigoCBDxBVKH3OVmJiImJgYBAUF1W+P/uyzz9CzZ099xxVKH3MFAEOGDMHhw4dRXl4OHx8fxMbGonfv3vqOa1R0mTuVSoXly5cjMjISWq0Ws2fPtsijNHX9PbPE36Mb0WW+LOVvVFN0mauW+u5jEWohf9+/QJZlnfc5CAsLQ3JysgFSGac7mav77rsPWq3WELGM0p3MFQCLPhKqqbkbOXIkRo4c2dKxjFJTc2XJv0c3cqv5srS/UU251Vy11HcfN40ZmLu7O5RKZaP/S8/Pz2/UhC0d50p3nKvbx7nTHeeqeThfujOmuWIRMjBra2uEhYUhLi6uwXhcXBz69esnKJVx4lzpjnN1+zh3uuNcNQ/nS3fGNFfcNKYHZWVlOHPmTP39jIwMJCcnw83NDe3atUN0dDSeeuophIeHo2/fvli/fj2ysrIwZcoUganF4FzpjnN1+zh3uuNcNQ/nS3cmM1d6Pw7NAu3evVsG0Og2ceLE+mXee+892c/PT7a2tpZDQ0Pl3377TVxggThXuuNc3T7One44V83D+dKdqcwVrzVGREREFov7CBEREZHFYhEiIiIii8UiRERERBaLRYiIiIgsFosQERERWSwWISIiIrJYLEJERERksViEiIiIyGKxCBEREZHFYhEiIrNy+fJleHp6IjMzU1iG1atXY+TIkcLen4h0xyJEREZHkqRb3p5++umbPnfp0qUYMWIE2rdv32D866+/xv333w9XV1fY29sjICAAkyZNwpEjR3TKVFNTA3d3d7zxxhs3fV93d3fU1NRg8uTJ+OOPP7B3715dPzIRCcJrjRGR0cnLy6v/OSYmBvPnz8epU6fqx+zs7ODs7NzoeZWVlfD29sbOnTvRt2/f+vE5c+Zg+fLlmDZtGkaNGgUfHx9kZWVh79692Lt3L3744Qedck2fPh3bt29Heno6JElq8Ji/vz+GDx+OlStXAgBmzpyJ7OxsxMTENOuzE1ELa/HLvBIRNcPHH38sOzs767Ts119/Lbu7uzcY279/vwxAfvfdd2/4HK1W2+D+d999J4eGhso2NjZyhw4d5AULFsi1tbWyLMvy0aNHZQDynj17GjwnPj5eBiAfO3asfmzPnj2ytbW1XFFRoVN2IhKDm8aIyGzEx8cjPDy8wdiXX36JVq1a4YUXXrjhc65fs7Nr1y48+eSTmDZtGtLS0rBu3Tps3LgRixcvBgD07NkTvXv3xscff9zgNT766CPcfffdCAwMrB8LDw9HbW0tDh48qK+PR0QGwCJERGYjMzMT3t7eDcZOnz6Njh07QqVS1Y+tWLECrVq1qr+VlJQAABYvXoxXXnkFEydORMeOHfHggw/iP//5D9atW1f/3EmTJmHr1q0oKysDAJSVlWHLli149tlnG7yvg4MDXFxchO60TURNYxEiIrNRWVkJW1vbRuN/359n0qRJSE5Oxrp161BeXg756q6SSUlJWLRoUYOSNHnyZOTm5qKiogIAMH78eGi12vp9f2JiYiDLMh5//PFG72tnZ1f/PCIyTqqmFyEiMg3u7u4oKipqMNalSxfs3bsXtbW1sLKyAgC4uLjAxcUF2dnZDZbVarVYuHAhRo8e3ei1rxUsZ2dnjBkzBh9//DGeffZZfPzxxxgzZgycnJwaPaewsBAeHh76+nhEZABcI0REZqNXr15IS0trMDZ+/HiUlZVhzZo1TT4/NDQUp06dQufOnRvdFIq//lw+++yzSExMxPbt25GYmNhosxgAnD17FlVVVejVq9edfzAiMhiuESIiszFkyBDMnTsXRUVFcHV1BQD07dsXM2fOxMyZM/Hnn39i9OjR8PX1RW5uLj788ENIklRfcubPn4+HH34Yvr6+GDt2LBQKBY4ePYpjx441OH/QgAED0LlzZ0yYMAGdO3dG//79G2VJSEhAx44d0alTp5b58ER0W7hGiIjMRs+ePREeHo7Nmzc3GH/77bexadMmHDlyBA8//DC6dOmCsWPHQqvVYv/+/fWbtYYMGYLt27cjLi4OvXv3xj333IMVK1bAz8+v0XtNmjQJRUVFmDRp0g2zfPnll5g8ebL+PyQR6RVPqEhEZmXnzp2YNWsWUlNTG2zOakmpqakYNGgQTp8+fcMTPxKR8eCmMSIyK8OGDUN6ejouXLgAX19fIRlycnLw6aefsgQRmQCuESIiIiKLxX2EiIiIyGKxCBEREZHFYhEiIiIii8UiRERERBaLRYiIiIgsFosQERERWSwWISIiIrJYLEJERERksViEiIiIyGL9P4SVvXIRuCRaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constants  \n",
    "m = 0.938 # GeV/c^2\n",
    "gamma = -3 # Between -2 and -3\n",
    "size = 512 # size of r, T, p, and f_boundary\n",
    "\n",
    "# Create intial r, p, and T predict data\n",
    "T = np.logspace(np.log10(0.001), np.log10(1000), size).flatten()[:,None] # GeV\n",
    "p = (np.sqrt((T+m)**2-m**2)).flatten()[:,None] # GeV/c\n",
    "r = (np.logspace(np.log10(0.4), np.log10(120), size)*150e6).flatten()[:,None] # km\n",
    "\n",
    "# Create boundary f data (f at r_HP) for boundary loss\n",
    "f_boundary = ((T + m)**gamma)/(p**2) # particles/(m^3 (GeV/c)^3)\n",
    "\n",
    "# Plot\n",
    "plt.loglog(T, f_boundary*(p**2))\n",
    "plt.xlabel(\"T (GeV)\")\n",
    "plt.ylabel(\"J(r, T) = f(r, p)*p^2\")\n",
    "\n",
    "# Take the log of all input data\n",
    "r = np.log(r)\n",
    "T = np.log(T)\n",
    "p = np.log(p)\n",
    "f_boundary = np.log(f_boundary)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([p[0], r[0]]) # (p, r) in (GeV, AU)\n",
    "ub = np.array([p[-1], r[-1]]) # (p, r) in (GeV, AU)\n",
    "\n",
    "# Flatten and transpose data for ML\n",
    "P, R = np.meshgrid(p, r)\n",
    "P_star = np.hstack((P.flatten()[:,None], R.flatten()[:,None]))\n",
    "\n",
    "# Check inputs\n",
    "print(f'r: {r.shape}, p: {p.shape}, T: {T.shape}, f_boundary: {f_boundary.shape}, P_star: {P_star.shape}, lb: {lb}, ub:{ub}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Class\n",
    "\n",
    "The PINN class subclasses the Keras Model so that we can implement our custom fit and train_step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Description: Defines the class for a PINN model implementing train_step, fit, and predict functions. Note, it is necessary \n",
    "to design each PINN seperately for each system of PDEs since the train_step is customized for a specific system. \n",
    "This PINN in particular solves the force-field equation for solar modulation of cosmic rays. Once trained, the PINN can predict the solution space given \n",
    "domain bounds and the input space. \n",
    "'''\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, lower_bound, upper_bound, p, r, f_boundary, size, n_samples=20000, n_boundary=50):\n",
    "        super(PINN, self).__init__(inputs=inputs, outputs=outputs)\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.p = p\n",
    "        self.r = r\n",
    "        self.f_boundary = f_boundary\n",
    "        self.n_samples = n_samples\n",
    "        self.n_boundary = n_boundary\n",
    "        self.size = size\n",
    "        \n",
    "    '''\n",
    "    Description: A system of PDEs are determined by 2 types of equations: the main partial differential equations \n",
    "    and the boundary value equations. These two equations will serve as loss functions which \n",
    "    we train the PINN to satisfy. If a PINN can satisfy BOTH equations, the system is solved. Since there are 2 types of \n",
    "    equations (PDE, Boundary Value), we will need 2 types of inputs. Each input is composed of a spatial \n",
    "    variable 'r' and a momentum variable 'p'. The different types of (p, r) pairs are described below.\n",
    "    \n",
    "    Inputs: \n",
    "        p, r: (batchsize, 1) shaped arrays : These inputs are used to derive the main partial differential equation loss.\n",
    "        Train step first feeds (p, r) through the PINN for the forward propagation. This expression is PINN(p, r) = f. \n",
    "        Next, the partials f_p and f_r are obtained. We utilize TF2s GradientTape data structure to obtain all partials. \n",
    "        Once we obtain these partials, we can compute the main PDE loss and optimize weights w.r.t. to the loss. \n",
    "        \n",
    "        p_boundary, r_boundary : (boundary_batchsize, 1) shaped arrays : These inputs are used to derive the boundary value\n",
    "        equations. The boundary value loss relies on target data (**not an equation**), so we can just measure the MAE of \n",
    "        PINN(p_boundary, r_boundary) = f_pred_boundary and boundary_f.\n",
    "        \n",
    "        f_boundary: (boundary_batchsize, 1) shaped arrays : This is the target data for the boundary value inputs\n",
    "        \n",
    "        alpha = weight on pinn_loss\n",
    "        \n",
    "        beta = weight on boundary_loss\n",
    "        \n",
    "    Outputs: sum_loss, pinn_loss, boundary_loss\n",
    "    '''\n",
    "    def train_step(self, p, r, p_boundary, r_boundary, f_boundary, alpha=1, beta=1):\n",
    "        with tf.GradientTape(persistent=True) as t2: \n",
    "            with tf.GradientTape(persistent=True) as t1: \n",
    "                # Forward pass P (PINN data)\n",
    "                P = tf.concat((p, r), axis=1)\n",
    "                f = self.tf_call(P)\n",
    "\n",
    "                # Forward pass P_boundary (boundary condition data)\n",
    "                P_boundary = tf.concat((p_boundary, r_boundary), axis=1)\n",
    "                f_pred_boundary = self.tf_call(P_boundary)\n",
    "\n",
    "                # Calculate boundary loss\n",
    "                boundary_loss = tf.math.reduce_mean(tf.math.abs(f_pred_boundary - f_boundary))\n",
    "\n",
    "            # Calculate first-order PINN gradients\n",
    "            f_p = t1.gradient(f, p)\n",
    "            f_r = t1.gradient(f, r)\n",
    "\n",
    "            # Calculate resulting loss = PINN loss + boundary loss\n",
    "            pinn_loss = self.pinn_loss(p, r, f_p, f_r)\n",
    "            total_loss = alpha*pinn_loss + beta*boundary_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = t2.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Return losses\n",
    "        return pinn_loss.numpy(), boundary_loss.numpy()\n",
    "    \n",
    "    '''\n",
    "    Description: The fit function used to iterate through epoch * steps_per_epoch steps of train_step. \n",
    "    \n",
    "    Inputs: \n",
    "        P_predict: (N, 2) array: Input data for entire spatial and temporal domain. Used for vizualization for\n",
    "        predictions at the end of each epoch. Michael created a very pretty video file with it. \n",
    "        \n",
    "        alpha = weight on pinn_loss\n",
    "        \n",
    "        beta = weight on boundary_loss\n",
    "        \n",
    "        batchsize: batchsize for (p, r) in train step\n",
    "        \n",
    "        boundary_batchsize: batchsize for (x_lower, t_boundary) and (x_upper, t_boundary) in train step\n",
    "        \n",
    "        epochs: epochs\n",
    "        \n",
    "        lr: learning rate\n",
    "        \n",
    "        size: size of the prediction data (i.e. len(p) and len(r))\n",
    "        \n",
    "        save: Whether or not to save the model to a checkpoint every 10 epochs\n",
    "        \n",
    "        load_epoch: If -1, a saved model will not be loaded. Otherwise, the model will be \n",
    "        loaded from the provided epoch\n",
    "        \n",
    "        lr_decay: If -1, learning rate will not be decayed. Otherwise, lr = lr_decay*lr if loss doesn't\n",
    "        decrease for 3 epochs\n",
    "        \n",
    "        weight_change: If -1, alpha will not be changed. Otherwise, alpha = weight_change*alpha if loss \n",
    "        doesn't decrease for 3 epochs\n",
    "    \n",
    "    Outputs: Losses for each equation (Total, PDE, Boundary Value), and predictions for each epoch.\n",
    "    '''\n",
    "    def fit(self, P_predict, alpha=1, beta=1, batchsize=64, boundary_batchsize=16, epochs=20, lr=3e-3, size=256, \n",
    "            save=False, load_epoch=-1, lr_decay=-1, weight_change=-1):\n",
    "        # If load == True, load the weights\n",
    "        if load_epoch != -1:\n",
    "            self.load_weights('./ckpts/pinn_epoch_' + str(load_epoch))\n",
    "        \n",
    "        # Initialize losses as zeros\n",
    "        steps_per_epoch = np.ceil(self.n_samples / batchsize).astype(int)\n",
    "        total_pinn_loss = np.zeros((epochs, ))\n",
    "        total_boundary_loss = np.zeros((epochs, ))\n",
    "        predictions = np.zeros((size**2, 1, epochs))\n",
    "        \n",
    "        # For each epoch, sample new values in the PINN and boundary areas and pass them to train_step\n",
    "        for epoch in range(epochs):\n",
    "            # Compile\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            self.compile(optimizer=opt)\n",
    "\n",
    "            # Reset loss variables\n",
    "            sum_loss = np.zeros((steps_per_epoch,))\n",
    "            pinn_loss = np.zeros((steps_per_epoch,))\n",
    "            boundary_loss = np.zeros((steps_per_epoch,))\n",
    "            \n",
    "            # For each step, get PINN and boundary variables and pass them to train_step\n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get PINN p and r variables via uniform distribution sampling between lower and upper bounds\n",
    "                p = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[0], maxval=self.upper_bound[0]))\n",
    "                r = tf.Variable(tf.random.uniform((batchsize, 1), minval=self.lower_bound[1], maxval=self.upper_bound[1]))\n",
    "                \n",
    "                # Randomly sample boundary_batchsize from p_boundary and f_boundary\n",
    "                p_idx = np.expand_dims(np.random.choice(self.f_boundary.shape[0], boundary_batchsize, replace=False), axis=1)\n",
    "                p_boundary = self.p[p_idx]\n",
    "                f_boundary = self.f_boundary[p_idx]\n",
    "                \n",
    "                # Create r_boundary array = r_HP\n",
    "                upper_bound = np.zeros((boundary_batchsize, 1))\n",
    "                upper_bound[:] = self.upper_bound[1]\n",
    "                r_boundary = tf.Variable(upper_bound, dtype=tf.float32)\n",
    "                \n",
    "                # Pass variables through the model via train_step and get losses\n",
    "                losses = self.train_step(p, r, p_boundary, r_boundary, f_boundary, alpha, beta)\n",
    "                pinn_loss[step] = losses[0]\n",
    "                boundary_loss[step] = losses[1]\n",
    "            \n",
    "            # Calculate and print total losses for the epoch\n",
    "            total_pinn_loss[epoch] = np.sum(pinn_loss)\n",
    "            total_boundary_loss[epoch] = np.sum(boundary_loss)\n",
    "            print(f'Training loss for epoch {epoch}: pinn: {total_pinn_loss[epoch]:.4f}, boundary: {total_boundary_loss[epoch]:.4f}, total: {(total_boundary_loss[epoch]+total_pinn_loss[epoch]):.4f}')\n",
    "            \n",
    "            # Predict\n",
    "            predictions[:, :, epoch] = self.predict(P_predict, size)\n",
    "            \n",
    "            # If the epoch is a multiple of 10, save to a checkpoint\n",
    "            if (epoch%10 == 0) & (save == True):\n",
    "                self.save_weights('./ckpts/pinn_epoch_' + str(epoch), overwrite=True, save_format=None, options=None)\n",
    "            \n",
    "            # Determine if loss has decreased for the past 2 or 5 epochs\n",
    "            if (epoch > 3):\n",
    "                isDecreasingFor2 = False\n",
    "                for i in range(2):\n",
    "                    if (total_pinn_loss[epoch-i] + total_boundary_loss[epoch-i]) < (total_pinn_loss[epoch-(i+1)] + total_boundary_loss[epoch-(i+1)]):\n",
    "                        isDecreasingFor2 = True\n",
    "                        \n",
    "                # If loss hasn't decreased for the past 2 epochs, decrease lr by lr_decay\n",
    "                if (lr_decay != -1) & (not isDecreasingFor2):\n",
    "                    lr = lr_decay*lr\n",
    "\n",
    "                # If pinn loss hasn't decreased for the past 2 epochs, increase alpha by weight_change\n",
    "                if (weight_change != -1) & (not isDecreasingFor2):\n",
    "                    alpha = weight_change*alpha\n",
    "                    print(\"Increased fo 2! new alpha: \", alpha)\n",
    "            \n",
    "            if (epoch > 6):\n",
    "                isDecreasingFor5 = False\n",
    "                for i in range(5):\n",
    "                    if (total_pinn_loss[epoch-i] + total_boundary_loss[epoch-i]) < (total_pinn_loss[epoch-(i+1)] + total_boundary_loss[epoch-(i+1)]):\n",
    "                        isDecreasingFor5 = True\n",
    "                        \n",
    "                # If loss hasn't decreased for 5 epochs, break\n",
    "                if not isDecreasingFor5:\n",
    "                    break\n",
    "        \n",
    "        # Return epoch losses\n",
    "        return total_pinn_loss, total_boundary_loss, predictions\n",
    "    \n",
    "    # Predict for some P's the value of the neural network f(r, p)\n",
    "    def predict(self, P, size, batchsize=2048):\n",
    "        steps_per_epoch = np.ceil(P.shape[0] / batchsize).astype(int)\n",
    "        preds = np.zeros((size**2, 1))\n",
    "        \n",
    "        # For each step calculate start and end index values for prediction data\n",
    "        for step in range(steps_per_epoch):\n",
    "            start_idx = step * 64\n",
    "            \n",
    "            # If last step of the epoch, end_idx is shape-1. Else, end_idx is start_idx + 64 \n",
    "            if step == steps_per_epoch - 1:\n",
    "                end_idx = P.shape[0] - 1\n",
    "            else:\n",
    "                end_idx = start_idx + 64\n",
    "                \n",
    "            # Pass prediction data through the model\n",
    "            preds[start_idx:end_idx, :] = self.tf_call(P[start_idx:end_idx, :]).numpy()\n",
    "        \n",
    "        # Return f\n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, ): \n",
    "        pass\n",
    "    \n",
    "    # pinn_loss calculates the PINN loss by calculating the MAE of the pinn function\n",
    "    @tf.function\n",
    "    def pinn_loss(self, p, r, f_p, f_r): \n",
    "        # Note: p and r are taken out of logspace for the PINN calculation\n",
    "        p = tf.math.exp(p) # GeV/c\n",
    "        r = tf.math.exp(r) # km\n",
    "        V = 400 # 400 km/s\n",
    "        m = 0.938 # GeV/c^2\n",
    "        k_0 = 1e11 # km^2/s\n",
    "        k_1 = k_0 * tf.math.divide(r, 150e6) # km^2/s\n",
    "        k_2 = p # unitless, k_2 = p/p0 and p0 = 1 GeV/c\n",
    "        R = p # GV\n",
    "        beta = tf.math.divide(p, tf.math.sqrt(tf.math.square(p) + tf.math.square(m))) \n",
    "        k = beta*k_1*k_2\n",
    "        \n",
    "        # Calculate physics loss\n",
    "        l_f = tf.math.reduce_mean(tf.math.abs(f_r + (tf.math.divide(R*V, 3*k) * f_p)))\n",
    "        \n",
    "        return l_f\n",
    "    \n",
    "    # tf_call passes inputs through the neural network\n",
    "    @tf.function\n",
    "    def tf_call(self, inputs): \n",
    "        return self.call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network regularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 0: pinn: 2.1531, boundary: 145.1537, total: 147.3067\n",
      "Training loss for epoch 1: pinn: 3.9974, boundary: 45.5742, total: 49.5716\n",
      "Training loss for epoch 2: pinn: 3.8040, boundary: 39.9033, total: 43.7073\n",
      "Training loss for epoch 3: pinn: 3.7477, boundary: 42.7817, total: 46.5295\n",
      "Training loss for epoch 4: pinn: 3.7616, boundary: 45.2689, total: 49.0305\n",
      "Increased fo 2! new alpha:  1.1\n",
      "Training loss for epoch 5: pinn: 3.2479, boundary: 32.5888, total: 35.8366\n",
      "Training loss for epoch 6: pinn: 2.6857, boundary: 26.5467, total: 29.2324\n",
      "Training loss for epoch 7: pinn: 2.7154, boundary: 25.4784, total: 28.1938\n",
      "Training loss for epoch 8: pinn: 2.9382, boundary: 31.0240, total: 33.9622\n",
      "Training loss for epoch 9: pinn: 2.3647, boundary: 20.7304, total: 23.0951\n",
      "Training loss for epoch 10: pinn: 2.6411, boundary: 28.2394, total: 30.8805\n",
      "Training loss for epoch 11: pinn: 2.5738, boundary: 25.6194, total: 28.1932\n",
      "Training loss for epoch 12: pinn: 2.2597, boundary: 21.4406, total: 23.7002\n",
      "Training loss for epoch 13: pinn: 2.1765, boundary: 18.1232, total: 20.2998\n",
      "Training loss for epoch 14: pinn: 2.1614, boundary: 17.3660, total: 19.5275\n",
      "Training loss for epoch 15: pinn: 1.8855, boundary: 14.3269, total: 16.2124\n",
      "Training loss for epoch 16: pinn: 2.1573, boundary: 23.5398, total: 25.6971\n",
      "Training loss for epoch 17: pinn: 1.9908, boundary: 15.5433, total: 17.5342\n",
      "Training loss for epoch 18: pinn: 1.8702, boundary: 15.9026, total: 17.7728\n",
      "Training loss for epoch 19: pinn: 1.9107, boundary: 18.4527, total: 20.3634\n",
      "Increased fo 2! new alpha:  1.2100000000000002\n",
      "Training loss for epoch 20: pinn: 1.7654, boundary: 15.0658, total: 16.8312\n",
      "Training loss for epoch 21: pinn: 1.6003, boundary: 12.0359, total: 13.6362\n",
      "Training loss for epoch 22: pinn: 1.5926, boundary: 12.4339, total: 14.0264\n",
      "Training loss for epoch 23: pinn: 1.6078, boundary: 14.0657, total: 15.6735\n",
      "Increased fo 2! new alpha:  1.3310000000000004\n",
      "Training loss for epoch 24: pinn: 1.5676, boundary: 11.0170, total: 12.5846\n",
      "Training loss for epoch 25: pinn: 1.2951, boundary: 5.4913, total: 6.7863\n",
      "Training loss for epoch 26: pinn: 1.2476, boundary: 7.2841, total: 8.5317\n",
      "Training loss for epoch 27: pinn: 1.2668, boundary: 10.1576, total: 11.4244\n",
      "Increased fo 2! new alpha:  1.4641000000000006\n",
      "Training loss for epoch 28: pinn: 1.2694, boundary: 9.5174, total: 10.7869\n",
      "Training loss for epoch 29: pinn: 1.2163, boundary: 6.8849, total: 8.1012\n",
      "Training loss for epoch 30: pinn: 1.2894, boundary: 9.6874, total: 10.9769\n",
      "Training loss for epoch 31: pinn: 1.1927, boundary: 4.4038, total: 5.5965\n",
      "Training loss for epoch 32: pinn: 1.2693, boundary: 9.6763, total: 10.9455\n",
      "Training loss for epoch 33: pinn: 1.2736, boundary: 7.0624, total: 8.3361\n",
      "Training loss for epoch 34: pinn: 1.2832, boundary: 6.1543, total: 7.4375\n",
      "Training loss for epoch 35: pinn: 1.3305, boundary: 10.4530, total: 11.7836\n",
      "Training loss for epoch 36: pinn: 1.3099, boundary: 10.0533, total: 11.3633\n",
      "Training loss for epoch 37: pinn: 1.3134, boundary: 7.8387, total: 9.1521\n",
      "Training loss for epoch 38: pinn: 1.3375, boundary: 7.9903, total: 9.3278\n",
      "Training loss for epoch 39: pinn: 1.3099, boundary: 7.8531, total: 9.1630\n",
      "Training loss for epoch 40: pinn: 1.3077, boundary: 7.7945, total: 9.1022\n",
      "Training loss for epoch 41: pinn: 1.2637, boundary: 8.0528, total: 9.3166\n",
      "Training loss for epoch 42: pinn: 1.2580, boundary: 7.7671, total: 9.0251\n",
      "Training loss for epoch 43: pinn: 1.2449, boundary: 8.0229, total: 9.2678\n",
      "Training loss for epoch 44: pinn: 1.2333, boundary: 7.4162, total: 8.6496\n",
      "Training loss for epoch 45: pinn: 1.2058, boundary: 7.5315, total: 8.7373\n",
      "Training loss for epoch 46: pinn: 1.1704, boundary: 7.6180, total: 8.7884\n",
      "Increased fo 2! new alpha:  1.6105100000000008\n",
      "Training loss for epoch 47: pinn: 1.1195, boundary: 7.4892, total: 8.6087\n",
      "Training loss for epoch 48: pinn: 1.0869, boundary: 6.6468, total: 7.7336\n",
      "Training loss for epoch 49: pinn: 1.0598, boundary: 6.5523, total: 7.6122\n",
      "Training loss for epoch 50: pinn: 1.0279, boundary: 6.4097, total: 7.4376\n",
      "Training loss for epoch 51: pinn: 0.9805, boundary: 6.7493, total: 7.7298\n",
      "Training loss for epoch 52: pinn: 0.9404, boundary: 6.5694, total: 7.5098\n",
      "Training loss for epoch 53: pinn: 0.9269, boundary: 6.9914, total: 7.9183\n",
      "Training loss for epoch 54: pinn: 0.8761, boundary: 6.1906, total: 7.0667\n",
      "Training loss for epoch 55: pinn: 0.8562, boundary: 6.4206, total: 7.2767\n",
      "Training loss for epoch 56: pinn: 0.8073, boundary: 6.2241, total: 7.0314\n",
      "Training loss for epoch 57: pinn: 0.8038, boundary: 6.3326, total: 7.1364\n",
      "Training loss for epoch 58: pinn: 0.7977, boundary: 5.4255, total: 6.2232\n",
      "Training loss for epoch 59: pinn: 0.7565, boundary: 5.5651, total: 6.3216\n",
      "Training loss for epoch 60: pinn: 0.7410, boundary: 6.0640, total: 6.8050\n",
      "Increased fo 2! new alpha:  1.771561000000001\n",
      "Training loss for epoch 61: pinn: 0.7219, boundary: 4.6324, total: 5.3543\n",
      "Training loss for epoch 62: pinn: 0.6850, boundary: 5.1534, total: 5.8384\n",
      "Training loss for epoch 63: pinn: 0.6609, boundary: 5.2462, total: 5.9071\n",
      "Increased fo 2! new alpha:  1.9487171000000014\n",
      "Training loss for epoch 64: pinn: 0.6288, boundary: 4.1447, total: 4.7735\n",
      "Training loss for epoch 65: pinn: 0.5937, boundary: 4.9364, total: 5.5302\n",
      "Training loss for epoch 66: pinn: 0.5511, boundary: 5.3401, total: 5.8912\n",
      "Increased fo 2! new alpha:  2.1435888100000016\n",
      "Training loss for epoch 67: pinn: 0.5209, boundary: 4.9610, total: 5.4820\n",
      "Training loss for epoch 68: pinn: 0.4876, boundary: 4.1185, total: 4.6061\n",
      "Training loss for epoch 69: pinn: 0.5039, boundary: 5.2650, total: 5.7688\n",
      "Training loss for epoch 70: pinn: 0.4549, boundary: 3.2730, total: 3.7278\n",
      "Training loss for epoch 71: pinn: 0.4216, boundary: 4.8300, total: 5.2516\n",
      "Training loss for epoch 72: pinn: 0.4182, boundary: 4.4738, total: 4.8920\n",
      "Training loss for epoch 73: pinn: 0.4141, boundary: 4.3347, total: 4.7489\n",
      "Training loss for epoch 74: pinn: 0.3883, boundary: 4.7118, total: 5.1001\n",
      "Training loss for epoch 75: pinn: 0.4036, boundary: 3.2484, total: 3.6519\n",
      "Training loss for epoch 76: pinn: 0.4301, boundary: 4.5669, total: 4.9970\n",
      "Training loss for epoch 77: pinn: 0.3650, boundary: 3.9186, total: 4.2835\n",
      "Training loss for epoch 78: pinn: 0.3747, boundary: 3.7974, total: 4.1721\n",
      "Training loss for epoch 79: pinn: 0.3626, boundary: 3.3448, total: 3.7074\n",
      "Training loss for epoch 80: pinn: 0.3514, boundary: 3.6718, total: 4.0232\n",
      "Training loss for epoch 81: pinn: 0.3657, boundary: 4.0744, total: 4.4401\n",
      "Increased fo 2! new alpha:  2.357947691000002\n",
      "Training loss for epoch 82: pinn: 0.3409, boundary: 3.8518, total: 4.1927\n",
      "Training loss for epoch 83: pinn: 0.3361, boundary: 4.0626, total: 4.3987\n",
      "Training loss for epoch 84: pinn: 0.3302, boundary: 3.1162, total: 3.4464\n",
      "Training loss for epoch 85: pinn: 0.3807, boundary: 5.3465, total: 5.7271\n",
      "Training loss for epoch 86: pinn: 0.3400, boundary: 3.9374, total: 4.2774\n",
      "Training loss for epoch 87: pinn: 0.3307, boundary: 3.5638, total: 3.8945\n",
      "Training loss for epoch 88: pinn: 0.3562, boundary: 5.2607, total: 5.6169\n",
      "Training loss for epoch 89: pinn: 0.3093, boundary: 3.3427, total: 3.6520\n",
      "Training loss for epoch 90: pinn: 0.3299, boundary: 3.8561, total: 4.1860\n",
      "Training loss for epoch 91: pinn: 0.3385, boundary: 3.9853, total: 4.3238\n",
      "Increased fo 2! new alpha:  2.5937424601000023\n",
      "Training loss for epoch 92: pinn: 0.3061, boundary: 3.0530, total: 3.3591\n",
      "Training loss for epoch 93: pinn: 0.3497, boundary: 4.8519, total: 5.2017\n",
      "Training loss for epoch 94: pinn: 0.3243, boundary: 4.6713, total: 4.9957\n",
      "Training loss for epoch 95: pinn: 0.3043, boundary: 3.9166, total: 4.2209\n",
      "Training loss for epoch 96: pinn: 0.2936, boundary: 3.3589, total: 3.6525\n",
      "Training loss for epoch 97: pinn: 0.2925, boundary: 3.1097, total: 3.4022\n",
      "Training loss for epoch 98: pinn: 0.3079, boundary: 3.8379, total: 4.1458\n",
      "Training loss for epoch 99: pinn: 0.2935, boundary: 3.6066, total: 3.9001\n",
      "Training loss for epoch 100: pinn: 0.3238, boundary: 3.3308, total: 3.6546\n",
      "Training loss for epoch 101: pinn: 0.3300, boundary: 3.9123, total: 4.2423\n",
      "Training loss for epoch 102: pinn: 0.3147, boundary: 3.5398, total: 3.8545\n",
      "Training loss for epoch 103: pinn: 0.3247, boundary: 4.0719, total: 4.3966\n",
      "Training loss for epoch 104: pinn: 0.3080, boundary: 3.7060, total: 4.0140\n",
      "Training loss for epoch 105: pinn: 0.3025, boundary: 3.7408, total: 4.0433\n",
      "Training loss for epoch 106: pinn: 0.3135, boundary: 3.5256, total: 3.8391\n",
      "Training loss for epoch 107: pinn: 0.2961, boundary: 3.0306, total: 3.3267\n",
      "Training loss for epoch 108: pinn: 0.3058, boundary: 3.7654, total: 4.0712\n",
      "Training loss for epoch 109: pinn: 0.3245, boundary: 3.4063, total: 3.7308\n",
      "Training loss for epoch 110: pinn: 0.2965, boundary: 3.8646, total: 4.1611\n",
      "Training loss for epoch 111: pinn: 0.3004, boundary: 3.7737, total: 4.0741\n",
      "Training loss for epoch 112: pinn: 0.2916, boundary: 2.3133, total: 2.6049\n",
      "Training loss for epoch 113: pinn: 0.3104, boundary: 3.2788, total: 3.5893\n",
      "Training loss for epoch 114: pinn: 0.2703, boundary: 3.5350, total: 3.8053\n",
      "Increased fo 2! new alpha:  2.853116706110003\n",
      "Training loss for epoch 115: pinn: 0.3126, boundary: 3.0030, total: 3.3156\n",
      "Training loss for epoch 116: pinn: 0.2669, boundary: 3.0248, total: 3.2917\n",
      "Training loss for epoch 117: pinn: 0.2580, boundary: 2.9516, total: 3.2095\n",
      "Training loss for epoch 118: pinn: 0.2496, boundary: 3.0161, total: 3.2657\n",
      "Training loss for epoch 119: pinn: 0.2506, boundary: 3.0417, total: 3.2923\n",
      "Increased fo 2! new alpha:  3.1384283767210035\n",
      "Training loss for epoch 120: pinn: 0.2466, boundary: 3.0367, total: 3.2832\n",
      "Training loss for epoch 121: pinn: 0.2576, boundary: 1.8913, total: 2.1489\n",
      "Training loss for epoch 122: pinn: 0.2505, boundary: 2.8741, total: 3.1246\n",
      "Training loss for epoch 123: pinn: 0.2356, boundary: 2.3476, total: 2.5832\n",
      "Training loss for epoch 124: pinn: 0.2494, boundary: 2.1852, total: 2.4346\n",
      "Training loss for epoch 125: pinn: 0.2294, boundary: 2.6029, total: 2.8323\n",
      "Training loss for epoch 126: pinn: 0.2331, boundary: 2.6911, total: 2.9242\n",
      "Increased fo 2! new alpha:  3.4522712143931042\n",
      "Training loss for epoch 127: pinn: 0.2219, boundary: 2.4007, total: 2.6226\n",
      "Training loss for epoch 128: pinn: 0.2249, boundary: 2.7509, total: 2.9759\n",
      "Training loss for epoch 129: pinn: 0.2301, boundary: 2.7420, total: 2.9721\n",
      "Training loss for epoch 130: pinn: 0.2279, boundary: 1.9620, total: 2.1899\n",
      "Training loss for epoch 131: pinn: 0.2185, boundary: 2.3659, total: 2.5845\n",
      "Training loss for epoch 132: pinn: 0.2199, boundary: 2.5498, total: 2.7697\n",
      "Increased fo 2! new alpha:  3.797498335832415\n",
      "Training loss for epoch 133: pinn: 0.2244, boundary: 2.1492, total: 2.3736\n",
      "Training loss for epoch 134: pinn: 0.2100, boundary: 2.3187, total: 2.5287\n",
      "Training loss for epoch 135: pinn: 0.2305, boundary: 3.1401, total: 3.3705\n",
      "Increased fo 2! new alpha:  4.177248169415656\n",
      "Training loss for epoch 136: pinn: 0.2040, boundary: 1.9796, total: 2.1836\n",
      "Training loss for epoch 137: pinn: 0.2119, boundary: 2.0225, total: 2.2343\n",
      "Training loss for epoch 138: pinn: 0.2192, boundary: 2.3209, total: 2.5401\n",
      "Increased fo 2! new alpha:  4.594972986357222\n",
      "Training loss for epoch 139: pinn: 0.2160, boundary: 2.3366, total: 2.5526\n",
      "Increased fo 2! new alpha:  5.054470284992944\n",
      "Training loss for epoch 140: pinn: 0.1952, boundary: 1.6198, total: 1.8150\n",
      "Training loss for epoch 141: pinn: 0.2044, boundary: 1.8319, total: 2.0363\n",
      "Training loss for epoch 142: pinn: 0.1906, boundary: 1.5550, total: 1.7456\n",
      "Training loss for epoch 143: pinn: 0.1894, boundary: 1.5575, total: 1.7470\n",
      "Training loss for epoch 144: pinn: 0.1961, boundary: 1.4227, total: 1.6187\n",
      "Training loss for epoch 145: pinn: 0.1838, boundary: 1.2694, total: 1.4532\n",
      "Training loss for epoch 146: pinn: 0.1901, boundary: 1.7154, total: 1.9055\n",
      "Training loss for epoch 147: pinn: 0.2050, boundary: 2.1944, total: 2.3994\n",
      "Increased fo 2! new alpha:  5.559917313492239\n",
      "Training loss for epoch 148: pinn: 0.1808, boundary: 1.4592, total: 1.6400\n",
      "Training loss for epoch 149: pinn: 0.1866, boundary: 1.5956, total: 1.7822\n",
      "Training loss for epoch 150: pinn: 0.1916, boundary: 1.4957, total: 1.6873\n",
      "Training loss for epoch 151: pinn: 0.1852, boundary: 1.6394, total: 1.8246\n",
      "Training loss for epoch 152: pinn: 0.1917, boundary: 1.5292, total: 1.7209\n",
      "Training loss for epoch 153: pinn: 0.1871, boundary: 1.2297, total: 1.4168\n",
      "Training loss for epoch 154: pinn: 0.1802, boundary: 1.5607, total: 1.7409\n",
      "Training loss for epoch 155: pinn: 0.1725, boundary: 1.2873, total: 1.4598\n",
      "Training loss for epoch 156: pinn: 0.1850, boundary: 1.5913, total: 1.7762\n",
      "Training loss for epoch 157: pinn: 0.1833, boundary: 1.5481, total: 1.7313\n",
      "Training loss for epoch 158: pinn: 0.1817, boundary: 1.5758, total: 1.7575\n",
      "Training loss for epoch 159: pinn: 0.1805, boundary: 1.1801, total: 1.3606\n",
      "Training loss for epoch 160: pinn: 0.1725, boundary: 1.2694, total: 1.4419\n",
      "Training loss for epoch 161: pinn: 0.1691, boundary: 1.0414, total: 1.2105\n",
      "Training loss for epoch 162: pinn: 0.1789, boundary: 1.5386, total: 1.7175\n",
      "Training loss for epoch 163: pinn: 0.1792, boundary: 1.6027, total: 1.7819\n",
      "Increased fo 2! new alpha:  6.115909044841463\n",
      "Training loss for epoch 164: pinn: 0.1708, boundary: 1.2848, total: 1.4556\n",
      "Training loss for epoch 165: pinn: 0.1705, boundary: 1.2019, total: 1.3724\n",
      "Training loss for epoch 166: pinn: 0.1627, boundary: 0.7704, total: 0.9331\n",
      "Training loss for epoch 167: pinn: 0.1687, boundary: 1.0277, total: 1.1964\n",
      "Training loss for epoch 168: pinn: 0.1687, boundary: 1.1637, total: 1.3325\n",
      "Increased fo 2! new alpha:  6.72749994932561\n",
      "Training loss for epoch 169: pinn: 0.1686, boundary: 1.2695, total: 1.4381\n",
      "Increased fo 2! new alpha:  7.400249944258172\n",
      "Training loss for epoch 170: pinn: 0.1644, boundary: 1.2804, total: 1.4448\n",
      "Increased fo 2! new alpha:  8.140274938683989\n",
      "Training loss for epoch 171: pinn: 0.1556, boundary: 0.9531, total: 1.1087\n",
      "Training loss for epoch 172: pinn: 0.1594, boundary: 1.1227, total: 1.2822\n",
      "Training loss for epoch 173: pinn: 0.1534, boundary: 0.8262, total: 0.9795\n",
      "Training loss for epoch 174: pinn: 0.1619, boundary: 0.9583, total: 1.1202\n",
      "Training loss for epoch 175: pinn: 0.1536, boundary: 1.0213, total: 1.1749\n",
      "Increased fo 2! new alpha:  8.954302432552389\n",
      "Training loss for epoch 176: pinn: 0.1515, boundary: 0.7710, total: 0.9225\n",
      "Training loss for epoch 177: pinn: 0.1567, boundary: 0.9212, total: 1.0779\n",
      "Training loss for epoch 178: pinn: 0.1539, boundary: 0.9577, total: 1.1116\n",
      "Increased fo 2! new alpha:  9.849732675807628\n",
      "Training loss for epoch 179: pinn: 0.1530, boundary: 0.7798, total: 0.9328\n",
      "Training loss for epoch 180: pinn: 0.1433, boundary: 0.6225, total: 0.7658\n",
      "Training loss for epoch 181: pinn: 0.1509, boundary: 0.6821, total: 0.8331\n",
      "Training loss for epoch 182: pinn: 0.1511, boundary: 0.8548, total: 1.0058\n",
      "Increased fo 2! new alpha:  10.834705943388391\n",
      "Training loss for epoch 183: pinn: 0.1476, boundary: 0.7799, total: 0.9276\n",
      "Training loss for epoch 184: pinn: 0.1408, boundary: 0.4830, total: 0.6238\n",
      "Training loss for epoch 185: pinn: 0.1431, boundary: 0.6153, total: 0.7584\n",
      "Training loss for epoch 186: pinn: 0.1450, boundary: 0.6555, total: 0.8005\n",
      "Increased fo 2! new alpha:  11.91817653772723\n",
      "Training loss for epoch 187: pinn: 0.1448, boundary: 0.6145, total: 0.7593\n",
      "Training loss for epoch 188: pinn: 0.1450, boundary: 0.6397, total: 0.7847\n",
      "Training loss for epoch 189: pinn: 0.1445, boundary: 0.6891, total: 0.8336\n",
      "Increased fo 2! new alpha:  13.109994191499954\n",
      "Training loss for epoch 190: pinn: 0.1399, boundary: 0.6194, total: 0.7593\n",
      "Training loss for epoch 191: pinn: 0.1417, boundary: 0.6036, total: 0.7452\n",
      "Training loss for epoch 192: pinn: 0.1427, boundary: 0.6101, total: 0.7528\n",
      "Training loss for epoch 193: pinn: 0.1430, boundary: 0.6056, total: 0.7486\n",
      "Training loss for epoch 194: pinn: 0.1432, boundary: 0.6191, total: 0.7623\n",
      "Training loss for epoch 195: pinn: 0.1446, boundary: 0.6104, total: 0.7550\n",
      "Training loss for epoch 196: pinn: 0.1412, boundary: 0.6084, total: 0.7497\n",
      "Training loss for epoch 197: pinn: 0.1428, boundary: 0.6216, total: 0.7643\n",
      "Training loss for epoch 198: pinn: 0.1422, boundary: 0.6024, total: 0.7446\n",
      "Training loss for epoch 199: pinn: 0.1389, boundary: 0.5636, total: 0.7025\n",
      "Training loss for epoch 200: pinn: 0.1469, boundary: 0.6295, total: 0.7764\n",
      "Training loss for epoch 201: pinn: 0.1463, boundary: 0.6041, total: 0.7504\n",
      "Training loss for epoch 202: pinn: 0.1414, boundary: 0.6000, total: 0.7414\n",
      "Training loss for epoch 203: pinn: 0.1405, boundary: 0.5965, total: 0.7370\n",
      "Training loss for epoch 204: pinn: 0.1404, boundary: 0.6123, total: 0.7527\n",
      "Training loss for epoch 205: pinn: 0.1389, boundary: 0.5633, total: 0.7023\n",
      "Training loss for epoch 206: pinn: 0.1388, boundary: 0.5952, total: 0.7340\n",
      "Training loss for epoch 207: pinn: 0.1388, boundary: 0.6069, total: 0.7457\n",
      "Increased fo 2! new alpha:  14.420993610649951\n",
      "Training loss for epoch 208: pinn: 0.1365, boundary: 0.5568, total: 0.6933\n",
      "Training loss for epoch 209: pinn: 0.1406, boundary: 0.5588, total: 0.6995\n",
      "Training loss for epoch 210: pinn: 0.1376, boundary: 0.5566, total: 0.6942\n",
      "Training loss for epoch 211: pinn: 0.1385, boundary: 0.5683, total: 0.7068\n",
      "Training loss for epoch 212: pinn: 0.1375, boundary: 0.5519, total: 0.6893\n",
      "Training loss for epoch 213: pinn: 0.1388, boundary: 0.5438, total: 0.6826\n",
      "Training loss for epoch 214: pinn: 0.1370, boundary: 0.5689, total: 0.7059\n",
      "Training loss for epoch 215: pinn: 0.1367, boundary: 0.6175, total: 0.7542\n",
      "Increased fo 2! new alpha:  15.863092971714948\n",
      "Training loss for epoch 216: pinn: 0.1345, boundary: 0.5811, total: 0.7156\n",
      "Training loss for epoch 217: pinn: 0.1374, boundary: 0.5415, total: 0.6789\n",
      "Training loss for epoch 218: pinn: 0.1359, boundary: 0.5249, total: 0.6608\n",
      "Training loss for epoch 219: pinn: 0.1361, boundary: 0.5339, total: 0.6700\n",
      "Training loss for epoch 220: pinn: 0.1365, boundary: 0.5198, total: 0.6563\n",
      "Training loss for epoch 221: pinn: 0.1332, boundary: 0.5068, total: 0.6400\n",
      "Training loss for epoch 222: pinn: 0.1376, boundary: 0.5390, total: 0.6766\n",
      "Training loss for epoch 223: pinn: 0.1358, boundary: 0.5045, total: 0.6403\n",
      "Training loss for epoch 224: pinn: 0.1367, boundary: 0.5005, total: 0.6372\n",
      "Training loss for epoch 225: pinn: 0.1379, boundary: 0.5180, total: 0.6559\n",
      "Training loss for epoch 226: pinn: 0.1324, boundary: 0.5176, total: 0.6500\n",
      "Training loss for epoch 227: pinn: 0.1373, boundary: 0.5248, total: 0.6622\n",
      "Training loss for epoch 228: pinn: 0.1340, boundary: 0.5190, total: 0.6530\n",
      "Training loss for epoch 229: pinn: 0.1337, boundary: 0.5294, total: 0.6631\n",
      "Training loss for epoch 230: pinn: 0.1348, boundary: 0.5034, total: 0.6383\n",
      "Training loss for epoch 231: pinn: 0.1302, boundary: 0.4897, total: 0.6199\n",
      "Training loss for epoch 232: pinn: 0.1366, boundary: 0.5391, total: 0.6757\n",
      "Training loss for epoch 233: pinn: 0.1372, boundary: 0.5351, total: 0.6723\n",
      "Training loss for epoch 234: pinn: 0.1354, boundary: 0.4966, total: 0.6320\n",
      "Training loss for epoch 235: pinn: 0.1377, boundary: 0.4993, total: 0.6370\n",
      "Training loss for epoch 236: pinn: 0.1354, boundary: 0.5039, total: 0.6393\n",
      "Increased fo 2! new alpha:  17.449402268886445\n",
      "Training loss for epoch 237: pinn: 0.1329, boundary: 0.4823, total: 0.6152\n",
      "Training loss for epoch 238: pinn: 0.1353, boundary: 0.4764, total: 0.6116\n",
      "Training loss for epoch 239: pinn: 0.1328, boundary: 0.5241, total: 0.6569\n",
      "Training loss for epoch 240: pinn: 0.1322, boundary: 0.3770, total: 0.5092\n",
      "Training loss for epoch 241: pinn: 0.1351, boundary: 0.4524, total: 0.5875\n",
      "Training loss for epoch 242: pinn: 0.1347, boundary: 0.4079, total: 0.5426\n",
      "Training loss for epoch 243: pinn: 0.1403, boundary: 0.4894, total: 0.6297\n",
      "Training loss for epoch 244: pinn: 0.1359, boundary: 0.4795, total: 0.6154\n",
      "Training loss for epoch 245: pinn: 0.1336, boundary: 0.4595, total: 0.5931\n",
      "Training loss for epoch 246: pinn: 0.1373, boundary: 0.4472, total: 0.5845\n",
      "Training loss for epoch 247: pinn: 0.1355, boundary: 0.4891, total: 0.6245\n",
      "Training loss for epoch 248: pinn: 0.1390, boundary: 0.4759, total: 0.6149\n",
      "Training loss for epoch 249: pinn: 0.1383, boundary: 0.4918, total: 0.6301\n",
      "Training loss for epoch 250: pinn: 0.1388, boundary: 0.5365, total: 0.6753\n",
      "Increased fo 2! new alpha:  19.19434249577509\n",
      "Training loss for epoch 251: pinn: 0.1402, boundary: 0.5072, total: 0.6474\n",
      "Training loss for epoch 252: pinn: 0.1424, boundary: 0.4882, total: 0.6306\n",
      "Training loss for epoch 253: pinn: 0.1348, boundary: 0.4428, total: 0.5776\n",
      "Training loss for epoch 254: pinn: 0.1388, boundary: 0.4889, total: 0.6277\n",
      "Training loss for epoch 255: pinn: 0.1385, boundary: 0.4747, total: 0.6132\n",
      "Training loss for epoch 256: pinn: 0.1390, boundary: 0.4276, total: 0.5666\n",
      "Training loss for epoch 257: pinn: 0.1396, boundary: 0.4137, total: 0.5533\n",
      "Training loss for epoch 258: pinn: 0.1391, boundary: 0.4838, total: 0.6228\n",
      "Training loss for epoch 259: pinn: 0.1358, boundary: 0.4632, total: 0.5989\n",
      "Training loss for epoch 260: pinn: 0.1312, boundary: 0.4545, total: 0.5858\n",
      "Training loss for epoch 261: pinn: 0.1353, boundary: 0.5091, total: 0.6444\n",
      "Training loss for epoch 262: pinn: 0.1341, boundary: 0.4081, total: 0.5423\n",
      "Training loss for epoch 263: pinn: 0.1357, boundary: 0.4773, total: 0.6130\n",
      "Training loss for epoch 264: pinn: 0.1354, boundary: 0.4567, total: 0.5920\n",
      "Training loss for epoch 265: pinn: 0.1349, boundary: 0.4596, total: 0.5944\n",
      "Training loss for epoch 266: pinn: 0.1352, boundary: 0.4634, total: 0.5985\n",
      "Increased fo 2! new alpha:  21.1137767453526\n",
      "Training loss for epoch 267: pinn: 0.1347, boundary: 0.4395, total: 0.5742\n",
      "Training loss for epoch 268: pinn: 0.1380, boundary: 0.4809, total: 0.6189\n",
      "Training loss for epoch 269: pinn: 0.1336, boundary: 0.4027, total: 0.5363\n",
      "Training loss for epoch 270: pinn: 0.1366, boundary: 0.4628, total: 0.5993\n",
      "Training loss for epoch 271: pinn: 0.1378, boundary: 0.4540, total: 0.5919\n",
      "Training loss for epoch 272: pinn: 0.1370, boundary: 0.4675, total: 0.6045\n",
      "Training loss for epoch 273: pinn: 0.1343, boundary: 0.4061, total: 0.5404\n",
      "Training loss for epoch 274: pinn: 0.1354, boundary: 0.4075, total: 0.5429\n",
      "Training loss for epoch 275: pinn: 0.1343, boundary: 0.4000, total: 0.5343\n",
      "Training loss for epoch 276: pinn: 0.1330, boundary: 0.3529, total: 0.4859\n",
      "Training loss for epoch 277: pinn: 0.1407, boundary: 0.4342, total: 0.5748\n",
      "Training loss for epoch 278: pinn: 0.1352, boundary: 0.4220, total: 0.5572\n",
      "Training loss for epoch 279: pinn: 0.1348, boundary: 0.4167, total: 0.5515\n",
      "Training loss for epoch 280: pinn: 0.1374, boundary: 0.4262, total: 0.5636\n",
      "Training loss for epoch 281: pinn: 0.1371, boundary: 0.4218, total: 0.5589\n",
      "Training loss for epoch 282: pinn: 0.1344, boundary: 0.4346, total: 0.5691\n",
      "Training loss for epoch 283: pinn: 0.1371, boundary: 0.4507, total: 0.5877\n",
      "Increased fo 2! new alpha:  23.22515441988786\n",
      "Training loss for epoch 284: pinn: 0.1344, boundary: 0.4424, total: 0.5769\n",
      "Training loss for epoch 285: pinn: 0.1365, boundary: 0.4041, total: 0.5406\n",
      "Training loss for epoch 286: pinn: 0.1371, boundary: 0.4091, total: 0.5462\n",
      "Training loss for epoch 287: pinn: 0.1330, boundary: 0.4268, total: 0.5597\n",
      "Increased fo 2! new alpha:  25.54766986187665\n",
      "Training loss for epoch 288: pinn: 0.1342, boundary: 0.3850, total: 0.5192\n",
      "Training loss for epoch 289: pinn: 0.1321, boundary: 0.3956, total: 0.5277\n",
      "Training loss for epoch 290: pinn: 0.1347, boundary: 0.4178, total: 0.5525\n",
      "Increased fo 2! new alpha:  28.102436848064315\n",
      "Training loss for epoch 291: pinn: 0.1353, boundary: 0.4104, total: 0.5457\n",
      "Training loss for epoch 292: pinn: 0.1335, boundary: 0.3833, total: 0.5168\n",
      "Training loss for epoch 293: pinn: 0.1318, boundary: 0.3701, total: 0.5020\n",
      "Training loss for epoch 294: pinn: 0.1336, boundary: 0.3779, total: 0.5116\n",
      "Training loss for epoch 295: pinn: 0.1343, boundary: 0.3608, total: 0.4951\n",
      "Training loss for epoch 296: pinn: 0.1336, boundary: 0.3915, total: 0.5251\n",
      "Training loss for epoch 297: pinn: 0.1323, boundary: 0.4099, total: 0.5423\n",
      "Increased fo 2! new alpha:  30.91268053287075\n",
      "Training loss for epoch 298: pinn: 0.1312, boundary: 0.3873, total: 0.5184\n",
      "Training loss for epoch 299: pinn: 0.1337, boundary: 0.3928, total: 0.5265\n",
      "Training loss for epoch 300: pinn: 0.1355, boundary: 0.4002, total: 0.5357\n",
      "Increased fo 2! new alpha:  34.003948586157826\n",
      "Training loss for epoch 301: pinn: 0.1342, boundary: 0.3759, total: 0.5101\n",
      "Training loss for epoch 302: pinn: 0.1324, boundary: 0.3806, total: 0.5130\n",
      "Training loss for epoch 303: pinn: 0.1341, boundary: 0.3795, total: 0.5135\n",
      "Increased fo 2! new alpha:  37.40434344477361\n",
      "Training loss for epoch 304: pinn: 0.1341, boundary: 0.4097, total: 0.5438\n",
      "Increased fo 2! new alpha:  41.14477778925097\n",
      "Training loss for epoch 305: pinn: 0.1312, boundary: 0.4031, total: 0.5342\n",
      "Training loss for epoch 306: pinn: 0.1306, boundary: 0.4349, total: 0.5654\n",
      "Training loss for epoch 307: pinn: 0.1321, boundary: 0.4558, total: 0.5879\n",
      "Increased fo 2! new alpha:  45.25925556817607\n",
      "Training loss for epoch 308: pinn: 0.1316, boundary: 0.4614, total: 0.5930\n",
      "Increased fo 2! new alpha:  49.785181124993684\n",
      "Training loss for epoch 309: pinn: 0.1283, boundary: 0.4798, total: 0.6081\n",
      "Increased fo 2! new alpha:  54.76369923749306\n",
      "Training loss for epoch 310: pinn: 0.1229, boundary: 0.4778, total: 0.6007\n",
      "Training loss for epoch 311: pinn: 0.1259, boundary: 0.4866, total: 0.6125\n",
      "Training loss for epoch 312: pinn: 0.1247, boundary: 0.4938, total: 0.6185\n",
      "Increased fo 2! new alpha:  60.24006916124237\n",
      "Training loss for epoch 313: pinn: 0.1295, boundary: 0.4916, total: 0.6212\n",
      "Increased fo 2! new alpha:  66.26407607736661\n",
      "Training loss for epoch 314: pinn: 0.1252, boundary: 0.5092, total: 0.6344\n",
      "Increased fo 2! new alpha:  72.89048368510328\n",
      "Training loss for epoch 315: pinn: 0.1291, boundary: 0.5049, total: 0.6340\n",
      "Training loss for epoch 316: pinn: 0.1283, boundary: 0.5232, total: 0.6515\n",
      "Training loss for epoch 317: pinn: 0.1265, boundary: 0.5132, total: 0.6396\n",
      "Training loss for epoch 318: pinn: 0.1254, boundary: 0.5011, total: 0.6265\n",
      "Training loss for epoch 319: pinn: 0.1262, boundary: 0.4951, total: 0.6213\n",
      "Training loss for epoch 320: pinn: 0.1261, boundary: 0.5188, total: 0.6448\n",
      "Training loss for epoch 321: pinn: 0.1290, boundary: 0.5186, total: 0.6476\n",
      "Increased fo 2! new alpha:  80.17953205361361\n",
      "Training loss for epoch 322: pinn: 0.1303, boundary: 0.5247, total: 0.6550\n",
      "Increased fo 2! new alpha:  88.19748525897498\n",
      "Training loss for epoch 323: pinn: 0.1269, boundary: 0.5200, total: 0.6470\n",
      "Training loss for epoch 324: pinn: 0.1234, boundary: 0.5272, total: 0.6506\n",
      "Training loss for epoch 325: pinn: 0.1268, boundary: 0.5221, total: 0.6489\n",
      "Training loss for epoch 326: pinn: 0.1264, boundary: 0.5144, total: 0.6408\n",
      "Training loss for epoch 327: pinn: 0.1275, boundary: 0.5253, total: 0.6528\n",
      "Training loss for epoch 328: pinn: 0.1270, boundary: 0.5279, total: 0.6549\n",
      "Increased fo 2! new alpha:  97.01723378487249\n",
      "Training loss for epoch 329: pinn: 0.1304, boundary: 0.5473, total: 0.6776\n",
      "Increased fo 2! new alpha:  106.71895716335975\n",
      "Training loss for epoch 330: pinn: 0.1233, boundary: 0.5436, total: 0.6669\n",
      "Training loss for epoch 331: pinn: 0.1255, boundary: 0.5510, total: 0.6766\n",
      "Training loss for epoch 332: pinn: 0.1276, boundary: 0.5560, total: 0.6836\n",
      "Increased fo 2! new alpha:  117.39085287969573\n",
      "Training loss for epoch 333: pinn: 0.1260, boundary: 0.5397, total: 0.6657\n",
      "Training loss for epoch 334: pinn: 0.1294, boundary: 0.5541, total: 0.6835\n",
      "Training loss for epoch 335: pinn: 0.1270, boundary: 0.5470, total: 0.6740\n",
      "Training loss for epoch 336: pinn: 0.1246, boundary: 0.5552, total: 0.6798\n",
      "Training loss for epoch 337: pinn: 0.1271, boundary: 0.5529, total: 0.6800\n",
      "Increased fo 2! new alpha:  129.1299381676653\n",
      "Training loss for epoch 338: pinn: 0.1276, boundary: 0.5441, total: 0.6717\n",
      "Training loss for epoch 339: pinn: 0.1277, boundary: 0.5436, total: 0.6714\n",
      "Training loss for epoch 340: pinn: 0.1264, boundary: 0.5566, total: 0.6830\n",
      "Training loss for epoch 341: pinn: 0.1289, boundary: 0.5591, total: 0.6880\n",
      "Increased fo 2! new alpha:  142.04293198443185\n",
      "Training loss for epoch 342: pinn: 0.1275, boundary: 0.5521, total: 0.6795\n",
      "Training loss for epoch 343: pinn: 0.1270, boundary: 0.5593, total: 0.6862\n",
      "Training loss for epoch 344: pinn: 0.1275, boundary: 0.5650, total: 0.6925\n",
      "Increased fo 2! new alpha:  156.24722518287504\n",
      "Training loss for epoch 345: pinn: 0.1269, boundary: 0.5600, total: 0.6869\n",
      "Training loss for epoch 346: pinn: 0.1273, boundary: 0.5539, total: 0.6812\n",
      "Training loss for epoch 347: pinn: 0.1268, boundary: 0.5684, total: 0.6953\n",
      "Training loss for epoch 348: pinn: 0.1261, boundary: 0.5662, total: 0.6923\n",
      "Training loss for epoch 349: pinn: 0.1297, boundary: 0.5769, total: 0.7066\n",
      "Training loss for epoch 350: pinn: 0.1242, boundary: 0.5813, total: 0.7055\n",
      "Training loss for epoch 351: pinn: 0.1325, boundary: 0.5761, total: 0.7086\n",
      "Training loss for epoch 352: pinn: 0.1286, boundary: 0.5660, total: 0.6946\n",
      "Training loss for epoch 353: pinn: 0.1290, boundary: 0.5664, total: 0.6954\n",
      "Training loss for epoch 354: pinn: 0.1304, boundary: 0.5602, total: 0.6906\n",
      "Training loss for epoch 355: pinn: 0.1308, boundary: 0.5664, total: 0.6972\n",
      "Training loss for epoch 356: pinn: 0.1301, boundary: 0.5606, total: 0.6907\n",
      "Training loss for epoch 357: pinn: 0.1303, boundary: 0.5675, total: 0.6978\n",
      "Training loss for epoch 358: pinn: 0.1313, boundary: 0.5669, total: 0.6982\n",
      "Increased fo 2! new alpha:  171.87194770116255\n",
      "Training loss for epoch 359: pinn: 0.1334, boundary: 0.5621, total: 0.6955\n",
      "Training loss for epoch 360: pinn: 0.1320, boundary: 0.5711, total: 0.7030\n",
      "Training loss for epoch 361: pinn: 0.1326, boundary: 0.5721, total: 0.7047\n",
      "Increased fo 2! new alpha:  189.05914247127882\n",
      "Training loss for epoch 362: pinn: 0.1325, boundary: 0.5867, total: 0.7192\n",
      "Increased fo 2! new alpha:  207.96505671840671\n",
      "Training loss for epoch 363: pinn: 0.1301, boundary: 0.5749, total: 0.7050\n",
      "Training loss for epoch 364: pinn: 0.1304, boundary: 0.5710, total: 0.7014\n",
      "Training loss for epoch 365: pinn: 0.1316, boundary: 0.5774, total: 0.7091\n",
      "Training loss for epoch 366: pinn: 0.1306, boundary: 0.5808, total: 0.7114\n",
      "Increased fo 2! new alpha:  228.7615623902474\n",
      "Training loss for epoch 367: pinn: 0.1311, boundary: 0.5856, total: 0.7167\n",
      "Increased fo 2! new alpha:  251.63771862927217\n",
      "Training loss for epoch 368: pinn: 0.1323, boundary: 0.5861, total: 0.7184\n",
      "Increased fo 2! new alpha:  276.80149049219943\n",
      "Training loss for epoch 369: pinn: 0.1337, boundary: 0.5843, total: 0.7180\n",
      "Training loss for epoch 370: pinn: 0.1361, boundary: 0.5921, total: 0.7282\n",
      "Training loss for epoch 371: pinn: 0.1325, boundary: 0.5754, total: 0.7079\n",
      "Training loss for epoch 372: pinn: 0.1340, boundary: 0.5792, total: 0.7131\n",
      "Training loss for epoch 373: pinn: 0.1337, boundary: 0.5875, total: 0.7212\n",
      "Increased fo 2! new alpha:  304.4816395414194\n",
      "Training loss for epoch 374: pinn: 0.1341, boundary: 0.5907, total: 0.7248\n",
      "Increased fo 2! new alpha:  334.9298034955614\n",
      "Training loss for epoch 375: pinn: 0.1306, boundary: 0.5774, total: 0.7080\n",
      "Training loss for epoch 376: pinn: 0.1340, boundary: 0.5979, total: 0.7318\n",
      "Training loss for epoch 377: pinn: 0.1327, boundary: 0.5911, total: 0.7237\n",
      "Training loss for epoch 378: pinn: 0.1347, boundary: 0.5826, total: 0.7173\n",
      "Training loss for epoch 379: pinn: 0.1326, boundary: 0.6129, total: 0.7455\n",
      "Training loss for epoch 380: pinn: 0.1315, boundary: 0.5947, total: 0.7262\n",
      "Training loss for epoch 381: pinn: 0.1299, boundary: 0.5990, total: 0.7289\n",
      "Training loss for epoch 382: pinn: 0.1319, boundary: 0.5999, total: 0.7318\n",
      "Increased fo 2! new alpha:  368.4227838451175\n",
      "Training loss for epoch 383: pinn: 0.1369, boundary: 0.6062, total: 0.7431\n",
      "Increased fo 2! new alpha:  405.2650622296293\n",
      "Training loss for epoch 384: pinn: 0.1351, boundary: 0.6215, total: 0.7566\n",
      "Increased fo 2! new alpha:  445.79156845259223\n",
      "Training loss for epoch 385: pinn: 0.1340, boundary: 0.6173, total: 0.7513\n",
      "Training loss for epoch 386: pinn: 0.1335, boundary: 0.6272, total: 0.7607\n",
      "Training loss for epoch 387: pinn: 0.1369, boundary: 0.6228, total: 0.7597\n",
      "Training loss for epoch 388: pinn: 0.1354, boundary: 0.6222, total: 0.7576\n",
      "Training loss for epoch 389: pinn: 0.1348, boundary: 0.6323, total: 0.7671\n",
      "Training loss for epoch 390: pinn: 0.1328, boundary: 0.6321, total: 0.7649\n",
      "Training loss for epoch 391: pinn: 0.1322, boundary: 0.6334, total: 0.7657\n",
      "Training loss for epoch 392: pinn: 0.1338, boundary: 0.6488, total: 0.7826\n",
      "Increased fo 2! new alpha:  490.3707252978515\n",
      "Training loss for epoch 393: pinn: 0.1387, boundary: 0.6465, total: 0.7852\n",
      "Increased fo 2! new alpha:  539.4077978276367\n",
      "Training loss for epoch 394: pinn: 0.1365, boundary: 0.6483, total: 0.7848\n",
      "Training loss for epoch 395: pinn: 0.1394, boundary: 0.6665, total: 0.8059\n",
      "Training loss for epoch 396: pinn: 0.1393, boundary: 0.6609, total: 0.8002\n",
      "Training loss for epoch 397: pinn: 0.1379, boundary: 0.6702, total: 0.8081\n",
      "Training loss for epoch 398: pinn: 0.1368, boundary: 0.6802, total: 0.8170\n",
      "Increased fo 2! new alpha:  593.3485776104004\n",
      "Training loss for epoch 399: pinn: 0.1391, boundary: 0.6706, total: 0.8097\n",
      "Training loss for epoch 400: pinn: 0.1410, boundary: 0.6957, total: 0.8366\n",
      "Training loss for epoch 401: pinn: 0.1398, boundary: 0.6784, total: 0.8183\n",
      "Training loss for epoch 402: pinn: 0.1379, boundary: 0.7012, total: 0.8391\n",
      "Training loss for epoch 403: pinn: 0.1401, boundary: 0.7058, total: 0.8459\n",
      "Increased fo 2! new alpha:  652.6834353714405\n",
      "Training loss for epoch 404: pinn: 0.1402, boundary: 0.6970, total: 0.8371\n",
      "Training loss for epoch 405: pinn: 0.1453, boundary: 0.7098, total: 0.8551\n",
      "Training loss for epoch 406: pinn: 0.1409, boundary: 0.7193, total: 0.8602\n",
      "Increased fo 2! new alpha:  717.9517789085846\n",
      "Training loss for epoch 407: pinn: 0.1401, boundary: 0.7238, total: 0.8639\n",
      "Increased fo 2! new alpha:  789.7469567994432\n",
      "Training loss for epoch 408: pinn: 0.1409, boundary: 0.7426, total: 0.8835\n",
      "Increased fo 2! new alpha:  868.7216524793876\n",
      "Training loss for epoch 409: pinn: 0.1443, boundary: 0.7370, total: 0.8814\n",
      "Training loss for epoch 410: pinn: 0.1408, boundary: 0.7530, total: 0.8939\n",
      "Training loss for epoch 411: pinn: 0.1472, boundary: 0.7485, total: 0.8957\n",
      "Increased fo 2! new alpha:  955.5938177273264\n",
      "Training loss for epoch 412: pinn: 0.1444, boundary: 0.7563, total: 0.9007\n",
      "Increased fo 2! new alpha:  1051.1531995000591\n",
      "Training loss for epoch 413: pinn: 0.1459, boundary: 0.7590, total: 0.9050\n",
      "Increased fo 2! new alpha:  1156.2685194500652\n",
      "Training loss for epoch 414: pinn: 0.1447, boundary: 0.7645, total: 0.9092\n",
      "Increased fo 2! new alpha:  1271.8953713950718\n"
     ]
    }
   ],
   "source": [
    "# Neural network. Note: 2 inputs- (p, r), 1 output- f(r, p)\n",
    "inputs = tf.keras.Input((2))\n",
    "x_ = tf.keras.layers.Dense(100, activation='relu')(inputs)\n",
    "x_ = tf.keras.layers.Dense(100, activation='relu')(x_)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 1 # pinn_loss weight\n",
    "beta = 1 # boundary_loss weight\n",
    "lr = 3e-3\n",
    "lr_decay = 0.9\n",
    "batchsize = 1032\n",
    "boundary_batchsize = 256 \n",
    "epochs = 500\n",
    "save = False\n",
    "load_epoch = -1\n",
    "weight_change = 1.01\n",
    "    \n",
    "# Initialize and fit the PINN\n",
    "pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "            f_boundary=f_boundary[:, 0], size=size)\n",
    "pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, beta=beta, batchsize=batchsize, \n",
    "                                                 boundary_batchsize=boundary_batchsize, epochs=epochs, lr=lr, size=size, \n",
    "                                                 save=save, load_epoch=load_epoch, lr_decay=lr_decay, weight_change=weight_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the outputs to a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PINN outputs\n",
    "with open('./figures/pinn_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(pinn_loss, file)\n",
    "    \n",
    "with open('./figures/boundary_loss.pkl', 'wb') as file:\n",
    "    pkl.dump(boundary_loss, file)\n",
    "    \n",
    "with open('./figures/predictions.pkl', 'wb') as file:\n",
    "    pkl.dump(predictions, file)\n",
    "    \n",
    "with open('./figures/f_boundary.pkl', 'wb') as file:\n",
    "    pkl.dump(f_boundary, file)\n",
    "    \n",
    "with open('./figures/p.pkl', 'wb') as file:\n",
    "    pkl.dump(p, file)\n",
    "    \n",
    "with open('./figures/T.pkl', 'wb') as file:\n",
    "    pkl.dump(T, file)\n",
    "    \n",
    "with open('./figures/r.pkl', 'wb') as file:\n",
    "    pkl.dump(r, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network with Sherpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set-up sherpa parameters, algorithm, and study\n",
    "# parameters = [sherpa.Ordinal(name='beta', range=[5, 10, 15, 20, 30]),\n",
    "#               sherpa.Continuous(name='lr', range=[3e-5, 3e-1], scale='log'),\n",
    "#               sherpa.Ordinal(name='batchsize', range=[256, 512, 1032, 2048]),\n",
    "#               sherpa.Ordinal(name='boundary_batchsize', range=[64, 128, 256]),\n",
    "#               sherpa.Ordinal(name='num_hidden_units', range=[100, 500, 1000]),\n",
    "#               sherpa.Choice(name='activation', range=['relu', 'tanh'])]\n",
    "\n",
    "# algorithm = sherpa.algorithms.RandomSearch(max_num_trials=2)\n",
    "\n",
    "# study = sherpa.Study(parameters=parameters,\n",
    "#                  algorithm=algorithm,\n",
    "#                  lower_is_better=True)\n",
    "\n",
    "# num_iterations = 1\n",
    "\n",
    "# # For each trial in the study, fit the model on the parameters and add the observation to the study\n",
    "# for trial in study:\n",
    "#     # Define neural network\n",
    "#     inputs = tf.keras.Input((2))\n",
    "#     x_ = tf.keras.layers.Dense(trial.parameters, activation=trial.parameters[5])(inputs)\n",
    "#     x_ = tf.keras.layers.Dense(trial.parameters[4], activation=trial.parameters[5])(x_)\n",
    "#     outputs = tf.keras.layers.Dense(1, activation='linear')(x_) \n",
    "\n",
    "#     # Initialize PINN and compile\n",
    "#     pinn = PINN(inputs=inputs, outputs=outputs, lower_bound=lb, upper_bound=ub, p=p[:, 0], r=r[:, 0], \n",
    "#             f_boundary=f_boundary[:, 0], size=size)\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=trial.parameters[1])\n",
    "#     pinn.compile(optimizer=optimizer)\n",
    "    \n",
    "#     # For each iteration, fit the PINN and add the observation to the study\n",
    "#     for iteration in range(num_iterations):\n",
    "#         total_loss, pinn_loss, boundary_loss, predictions = pinn.fit(P_predict=P_star, alpha=alpha, beta=trial.parameters[0], batchsize=trial.parameters[2], \n",
    "#                                                              boundary_batchsize=trial.parameters[3], epochs=100, size=size)\n",
    "#         study.add_observation(trial=trial,\n",
    "#                               iteration=iteration,\n",
    "#                               objective=boundary_loss,\n",
    "#                               context={'boundary_loss': boundary_loss})\n",
    "#     study.finalize(trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
